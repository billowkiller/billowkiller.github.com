<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tags: hadoop | Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/blog/tags/hadoop/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-08-08T12:14:34+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hbase Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/20/hbase/"/>
    <updated>2016-07-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/20/hbase</id>
    <content type="html"><![CDATA[<p>犯懒了，留两篇好文章，基本的东西也都在这文章里面。得空再补补吧。</p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426877.html">深入HBase架构解析（一）</a></p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426950.html">深入HBase架构解析（二）</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/20/hadoop-intro/"/>
    <updated>2015-12-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/20/hadoop-intro</id>
    <content type="html"><![CDATA[<p>看图说话，用一些图表来记录Hadoop。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/22389512.jpg" width="500px" /></p>

<!--more-->

<h3 id="mapreduce">MapReduce</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/71525619.jpg" width="500px" /></p>

<p>schema-on-read表示hadoop更容易处理非结构化或半结构化的数据，因为可以在数据处理的时候进行解析，因此在加载数据的时候更加轻量化。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/99564088.jpg" width="500px" /></p>

<h3 id="hdfs">HDFS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/60582298.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/98566811.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/1953976.jpg" width="500px" /></p>

<p>NameNode维护整个文件系统的文件目录树，文件目录的元信息和文件的数据块索引。这些信息以 FSImage 和 EditLog 方式存储在本地文件系统中。 FSImage 是某一时刻的镜像，后续修改都是放在 EditLog 中。 Second NameNode 会间隔将 FSImage 和 EditLog 进行合并后放在 NameNode 上。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/94295030.jpg" width="500px" /></p>

<h3 id="yarn">Yarn</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/80769446.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/31902359.jpg" width="400px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37369292.jpg" width="500px" /></p>

<ol>
  <li>
    <p>Fair Scheduler
 Facebook开发的适合共享环境的调度器，支持多用户多分组管理，每个分组可以配置资源量，也可限制每个用户和每个分组中的并发运行作业数量；每个用户的作业有优先级，优先级越高分配的资源越多。</p>
  </li>
  <li>
    <p>Capacity Scheduler
 Yahoo开发的适合共享环境的调度器，支持多用户多队列管理，每个队列可以配置资源量，也可限制每个用户和每个队列的并发运行作业数量，也可限制每个作业使用的内存量；每个用户的作业有优先级，在单个队列中，作业按照先来先服务（实际上是先按照优先级，优先级相同的再按照作业提交时间）的原则进行调度。</p>
  </li>
</ol>

<p><strong>Fair Scheduler vs Capacity Scheduler</strong></p>

<ul>
  <li>
    <p>相同点</p>

    <ul>
      <li>均支持多用户多队列，即：适用于多用户共享集群的应用环境</li>
      <li>单个队列均支持优先级和FIFO调度方式</li>
      <li>均支持资源共享，即某个queue中的资源有剩余时，可共享给其他缺资源的queue</li>
    </ul>
  </li>
  <li>
    <p>不同点</p>

    <ul>
      <li>
        <p>核心调度策略不同。 计算能力调度器的调度策略是，先选择资源利用率低的queue，然后在queue中同时考虑FIFO和memory constraint因素；而公平调度器仅考虑公平，而公平是通过作业缺额体现的，调度器每次选择缺额最大的job（queue的资源量，job优先级等仅用于计算作业缺额）。</p>
      </li>
      <li>
        <p>内存约束。计算能力调度器调度job时会考虑作业的内存限制，为了满足某些特殊job的特殊内存需求，可能会为该job分配多个slot；而公平调度器对这种特殊的job无能为力，只能杀掉这种task。</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/75633723.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/56706884.jpg" width="450px" /></p>

<h3 id="hadoop-io">HADOOP IO</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/15159126.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/90632065.jpg" width="500px" /></p>

<p>其他流行的兼容Hadoop的序列化框架还有：Avro、Thrift、google protobuf</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/11927837.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/70087429.jpg" width="500px" /></p>

<h3 id="hadoop-jobs">HADOOP JOBS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/38335644.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37931472.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/10857296.jpg" width="500px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning/"/>
    <updated>2015-12-01T09:50:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning</id>
    <content type="html"><![CDATA[<p>Hadoop的调优涉及到整个MapReduce的各个过程，并且要对每个参数的意义和Counter信息有一定的了解。也就是说，根据Counter的信息推测哪些MapReduce阶段可能存在性能瓶颈，并且根据这个瓶颈理解对应的Hadoop 框架中的处理逻辑，进而可以调整相关参数的大小或程序的行为。</p>

<p>从大面上来看，MapReduce的瓶颈可能存在以下图中的各个部分。</p>

<p><img src="http://ww2.sinaimg.cn/large/74311666jw1eyjx8zpfz2j20rq0cegna.jpg" width="500px" /></p>

<!--more-->

<p>在开始调优之旅前，先来个开胃小菜。进行调优的过程中，我们首先要知道整个job的运行情况。</p>

<p>Hadoop 2中的JobHistory是能够提供作业运行时各个参数指标的展示工具，可以通过这个UI界面查看所有的Counter。另一方面如果不方便通过界面的方式查看，则可以利用Hadoop自带的命令行工具查看，方法是<code>hadoop job -history &lt;history file&gt;</code>, 这个history file的位置通常是在mapreduce.jobhistory.done-dir目录下，可以用如下方式查找<code>hadoop fs -lsr &lt;done-dir&gt; | grep job_1398974791337_0037</code>。</p>

<h2 id="map-optimizations">Map Optimizations</h2>

<p>在Map端的优化通常会涉及到输入的数据和它的处理过程，以及你的应用程序代码。Mapper需要读取作业的输入，输入文件的不同也会影响到作业运行的效率，例如文件是否是splittable，数据的本地性和input split的数量等等。</p>

<h3 id="data-locality">Data Locality</h3>

<p>在分布式计算中有条著名的准则<strong>Pushing compute to the data</strong>，map的task应该尽可能的被安排在数据存放的节点上。可以用Counter来判断作业是否符合这条准则:</p>

<ul>
  <li>HDFS_BYTES_READ：这个值应当不大于input file的block size</li>
  <li>DATA_LOCAL_MAPS：这个值应当为1</li>
  <li>RACK_LOCAL_MAPS：这个值应当为0</li>
</ul>

<p>以下几种情况可能会发生non-local read:</p>

<ul>
  <li>不能分割的大文件，这样mapper就必须从不同的节点中读取blocks。</li>
  <li>文件格式支持split，但是用的input format不对。典型的情况是LZOP格式，需要先建立索引后再进行读取。</li>
  <li>Yarn的调度器不能在某个节点上产生map container，通常是由于集群under load。</li>
</ul>

<p>解决方法是：</p>

<ul>
  <li>尽量保持unsplittable文件的大小接近一个block的大小。</li>
  <li>设置yarn的<code>scheduler.capacity.node-locality-delay</code>，引入跳过的调度次数，来增加map task分配到数据节点上的概率。</li>
  <li>使用Twitter提供的LZO Input Format来处理lzop数据，或者使用bzip2格式的文件。</li>
</ul>

<h3 id="map-number-overwhelm">Map Number Overwhelm</h3>

<p>当输入有很多的input split的时候，每个input split都需要一个mapper来执行，而每个mapper都是一个单独的进程。这样会给调度器和集群带来极大的压力。原因通常有两个：</p>

<ul>
  <li>input data由很多的小文件组成，Hadoop会为每个小文件生成一个mapper，最终时间会大量消耗在启动进程上。</li>
  <li>每个文件并不是很小（和block size相当），但总体的数据量很大，横跨上千个HDFS的blocks。这样每个block也会分配给单独的mapper。</li>
</ul>

<p>如果是第一种情况，可以先聚合这个小文件，或者使用类似avro的文件格式来存储。或者直接用<code>CombineFileInputFormat</code>来处理以上这两种情况，它可以在一个mapper里处理多个HDFS block的数据。<code>CombineFileInputFormat</code>会首先检查input files的所有blocks，简历每个block到data nodes的映射关系，接着将同一个节点上的blocks聚合到一个input split中以保持data locality。它有三个配置项来调整：</p>

<ul>
  <li>mapreduce.input.fileinputformat.split.minsize.per.node</li>
  <li>mapreduce.input.fileinputformat.split.minsize.per.rac</li>
  <li>mapreduce.input.fileinputformat.split.maxsize</li>
</ul>

<p>以上的默认配置会造成每个节点上尽量形成一个最大的input split，影响作业的并行性，可以用以上几个配置来调整。<code>CombineFileInputFormat</code>还包括两个具体的类：</p>

<ul>
  <li><code>CombineTextInputFormat</code></li>
  <li><code>CombineSequenceFileInputFormata</code></li>
</ul>

<h3 id="input-split-computation">Input Split Computation</h3>

<p>如果提交作业的client是在集群局域网之外，那么input split的计算可能带来高成本。</p>

<p>当输入的数据源是HDFS时，client需要做如下事情，包括file listing, file status retrieving，input files数量比较多的时候，整个过程带来数据传输的延迟是比较可观的。</p>

<p>可以通过设置<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>将input split的计算交给AppMaster处理，这是在集群内部进行的。</p>

<h2 id="shuffle-optimizations">Shuffle Optimizations</h2>

<h3 id="using-the-combiner">Using the Combiner</h3>

<p>combiner可以有效的减少mapper和reducer之间通信的数据量。</p>

<h3 id="using-binary-comparators">Using Binary Comparators</h3>

<p>MapReduce在做sorting或者merging的时候，使用<code>RawComparator</code>比较map output key。内置的<code>Writable</code> classes（<code>Text</code>、<code>IntWritable</code>）有byte-level的比较器，无需将二进制的数据重新组装成实际的对象，所以能够快速进行序列化对象的比较。</p>

<p>用户可以在自己构造的<code>Writable</code>对象里面实现<code>WritableComparable</code>接口，这处理起来会比较容易，但是另一方面要注意MapReduce中map output data是以byte的形式存储的，这会导致在shuffle和sort的阶段需要从byte到object的转化才可以完成对象的比较。</p>

<p>可以看到在Hadoop的内置<code>Writable</code>对象不仅实现了<code>WritableComparable</code>接口，还自定义继承自<code>WritableComparator</code>的比较器。<code>WritableComparator</code>有什么作用呢，可以看下它的一些方法申明。</p>

<pre><code>public class WritableComparator implements RawComparator {
    public int compare(byte[] b1, int s1, int l1,
                       byte[] b2, int s2, int l2); 
}
</code></pre>

<p>可以看到这是byte-level的Comparator，<code>Writable</code>对象正是覆盖了这里面的compare方法。在内置的<code>Writable</code>对象中都实现了<code>WritableComparator</code>，所以无需担心内置对象的效率。但是自己所构造的对象也可以实现<code>WritableComparator</code>方法来提高效率。</p>

<p>例如一个拥有firstName和lastName的Person对象：</p>

<pre><code>private String firstName;
private String lastName;

@Override
public void write(DataOutput out) throws IOException {
    out.writeUTF(lastName);
    out.writeUTF(firstName);
}
</code></pre>

<p><img src="http://i5.tietuku.com/e5eba32886b5773d.png" width="600px" /></p>

<pre><code>public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,
                     int l2) {
    int lastNameResult = compare(b1, s1, b2, s2);
    if (lastNameResult != 0) {
        return lastNameResult;
    }
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compare(b1, s1 + b1l1 + 2, b2, s2 + b2l1 + 2);
}

public static int compare(byte[] b1, int s1, byte[] b2, int s2) {
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compareBytes(b1, s1 + 2, b1l1, b2, s2 + 2, b2l1);
}

public static int readUnsignedShort(byte[] b, int offset) {
    int ch1 = b[offset];
    int ch2 = b[offset + 1];
    return (ch1 &lt;&lt; 8) + (ch2);
}
</code></pre>

<h3 id="tunning-the-shuffle-internals">Tunning the shuffle internals</h3>

<p>在mapper中，output record首先被存储在一个内存buffer中，当这个buffer增长到一定大小的时候，数据被spill到磁盘中的一个文件。整个过程持续到mapper完成所有的output record生成。过程如下：</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>在整个阶段中，I/O相关的splling和merging是最耗时的，所以理想状况应该是所有的output数据都可以装入buffer中，这样只有一个文件被spill到磁盘。这对所有的作业来说自然是不大可能，但是如果mapper可以通过filter或者project的方法减少input data，那么可以好好调整下<code>mapreduce.task.io.sort.mb</code>的大小，因为这个数据直接关系buffer的大小。可以通过检查下面的Counters来调整map端的shuffle：</p>

<ul>
  <li>MAP_OUTPUT_BYTES  用这个数据来估计是否可以调整<code>mapreduce.task.io.sort.mb</code>来装入map的output record</li>
  <li>SPILLED_RECORDS/MAP_OUTPUT_RECORDS  这两个数据的理想情况是一致的，表示只有一个spill发生。</li>
  <li>FILE_BYTES_READ/FILE_BYTES_WRITTEN  比较这两个数据和MAP_OUTPUT_BYTES可以理解在splling和merging阶段发生的读写副作用</li>
</ul>

<p>在reduce方面，map的output通过每个节点上运行的shuffle service进程被发送到对应的reducer。在reducer中，map output是被写入到一个内存buffer中，在数据接收的过程中buffer中的数据被排好序，并在到达一定数据量的时候写入磁盘。同时有一个后台进程负责不断merge这个小的spllied file到merged files中，当所有的fetcher获取了所有的outputs，会有一个最终的merging发生，这时候数据也就从merged files到reducer了。也就是如下图的这个过程：</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<p>通map端的调优一样，reduce端也是尽量将数据存入内存中，减少splling和merging发生的次数，但是这个过程并不如map端一样明显，因为数据是在边接收边merging的。默认情况下，无论数据是否可以装入内存中，splling总会发生，所以可以调整<code>mapreduce.reduce.merge.memtomem.enabled</code>为true启动memory-to-memory的merge。map端的Counter同样适用于reduce。</p>

<p>以下的参数可以用来调整Hadoop的shuffle行为：</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Default</th>
      <th>Map or Reduce</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mapreduce.task.io.sort.mb</td>
      <td>100 (MB)</td>
      <td>Map</td>
      <td>The total amount of buffer memory in megabytes to use when buffering map outputs. This should be approximately 70% of the map task’s heap size.</td>
    </tr>
    <tr>
      <td>mapreduce.map.sort.spill.percent</td>
      <td>0.8</td>
      <td>Map</td>
      <td>Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than 0.5.</td>
    </tr>
    <tr>
      <td>mapreduce.task.io.sort.factor</td>
      <td>10</td>
      <td>Map and Reduce</td>
      <td>The number of streams to merge at once while sorting files. This determines the number of open file handles. Larger clusters with 1,000 or more nodes can bump this up to 100.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.parallelcopies</td>
      <td>5</td>
      <td>Reduce</td>
      <td>The default number of parallel transfers run on the reduce side during the copy (shuffle) phase. Larger clusters with 1,000 or more nodes can bump this up to 20.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.input.buffer.percent</td>
      <td>0.7</td>
      <td>Reduce</td>
      <td>The percentage of memory to be allocated from the maximum heap size to store map outputs during the shuffle.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.merge.percent</td>
      <td>0.66</td>
      <td>Reduce</td>
      <td>The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.merge.memtomem.enabled</td>
      <td>false</td>
      <td>Reduce</td>
      <td>If all the map outputs for each reducer can be stored in memory, then set this property to true.</td>
    </tr>
  </tbody>
</table>

<p>Shuffle的原则是使用filter和project减少数据量，使用combiner以及压缩map的output，尽可能的减少mapper和reducer质检传递的数据，减低IO带来的开销。这样之后再利用上述提到的参数来调整Shuffle的过程。</p>

<h2 id="recuder-optimizations">Recuder Optimizations</h2>

<h3 id="the-number-of-reducers">The Number of Reducers</h3>

<p>大多数情况下map端的并行度是由框架根据input files和input format来自动决定的，但在Reduce端，reducer的数量是由用户自行决定的。太少的reducers意味着没能充分利用集群的资源，太多的reducer则会让调度器疲于奔命，如果没有太多的资源供所有reducer运行则会拖累reducer的执行效率。在一些使用场景中，不能避免的需要使用少量的reducer来运行作业，例如数据写入DB系统中。另外一些场景中需要确认数据是否会发生倾斜，以及如何partition的，数据量是否会让reducer发生OOM的情况。</p>

<h2 id="general-tunning-tips">General tunning tips</h2>

<ul>
  <li>压缩</li>
  <li>使用类似于Avro或Parquet的数据格式存储数据，带来的好处是空间利用率，序列化和反序列化更加有效。
    <ul>
      <li>在Hadoop中，text并不是一个有效的数据格式，空间利用率低，解析成本高，特别是用到正则的时候。</li>
      <li>尽可能考虑使用二进制的文件存储格式。</li>
    </ul>
  </li>
</ul>

<h2 id="tunning-tools">Tunning tools</h2>

<h3 id="stack-dumps">stack dumps</h3>

<p>ssh到task运行的机器上，执行下面的命令：</p>

<pre><code>ps aux | grep container_1393242034820_0001_01_000002
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
</code></pre>

<p><code>SIGQUIT</code>信号的发送应该要间隔几秒，当JVM收到这个信号的时候会执行stack dump，这样可以了解到程序在这段时间内的运行情况。最后可以在task 的output file中查看dump出来的栈信息。</p>

<h3 id="profiling-map-and-reduce-task">Profiling Map and Reduce Task</h3>

<p>可以使用HPROF结合一些Mapreduce job method来进行Profiling。HPROF是JVM内置的java profiling工具，Hadoop内置了对HPROF的支持。可以在driver中加入如下的代码：</p>

<pre><code>job.setProfileEnabled(true);
job.setProfileParams(
    "-agentlib:hprof=depth=8,cpu=samples,heap=sites,force=n," +
        "thread=y,verbose=n,file=%s");
job.setProfileTaskRange(true, "0,1,5-10");
job.setProfileTaskRange(false, "");
</code></pre>

<p>在<code>setProfileParams</code>方法中设置的参数会在每个container中建立一个名为profile.out的文件，这个文件可以很容易被解析。可以通过ssh到目标机器查看或者通过JobHistory UI界面查看。</p>

<p>profile.out包括一些stack traces，还包括内存和CPU时间的信息。以下是一个profile.out文件：</p>

<p><img src="http://i5.tietuku.com/ba1dcbebf426b1a8.png" width="600px" /></p>

<p>可以看出来第一个问题是在<code>String.split</code>这个方法的使用上，它采用正则表达式来分割字符串，这个是相当耗时的一个举措，可以用Apache Commons Lang library的<code>StringUtils.split</code>来替换。另外一个是发生在Text的构造上，可以只构造一个Text实例，采用<code>set</code>方法进行设置，这样会更加有效率。</p>

<p>需要注意，使用HPROF会给程序的执行带来额外的负担，需要持续的收集profiling的信息，所以在正常的运行过程中不应该加上。</p>

<h2 id="conclusion">Conclusion</h2>

<p>一些在工作中实际用到的调优方法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-30/1330399.jpg" width="900px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Secondary Sorting]]></title>
    <link href="http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting/"/>
    <updated>2015-11-22T17:27:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting</id>
    <content type="html"><![CDATA[<p>Hadoop MapReduce的神奇之处发生在mapper和reducer之间，将所有相同key的map输出记录聚集在一块，使得用户可以方便的处理聚合在一起的数据。Hadoop内部使用了partition、sort和merge（shuffle的一部分），在每个reducer中流式地得到排序后的key和value集合。在MapReduce Sorting中有个特别的部分是secondary sort，也就是对value进行排序。</p>

<!--more-->

<p>Secondary sort在两种情况下特别有用：</p>

<ul>
  <li>需要某一部分的数据比其他数据更快的到达reducer。</li>
  <li>希望job的输出按照两个key进行排序。</li>
</ul>

<p>实现Secondary sort需要对MapReduce中的数据流和处理有一定的了解，下图展示了对reducer中出现的数据有影响的三个部分。</p>

<p><img src="http://i5.tietuku.com/7ad3ad872415c4b6.png" width="600px" /></p>

<p><code>partitioner</code>决定哪个reducer接收该mapper数据记录；<code>sorting RawComparator</code>用于在各自的分片中排序输出的结果，map和reduce阶段都有它，其中map阶段的sorting是对reduce阶段sorting的一个优化，让reducer的sorting更高效；最后，<code>grouping RawComparator</code>用于决定reducer处理排序后记录的边界，发生在reducer从本地磁盘读取数据的时候，也就是说，你可以用这个方法决定数据记录是如何聚集起来调用一个reduce方法的。MapReduce默认把这个三个方法作用于map方法输出的key上。</p>

<p>要实现Secondary sorting，我们需要重写partitioner、sort comparator和grouping comparator。</p>

<p>下面，通过对人名的排序来说明如何使用Secondary sorting。使用primary sort排序last name，secondary sort排序first name。</p>

<p>我们需要构建一个由map方法输出的Composite key，这个key由两部分组成：</p>

<ul>
  <li>Natural Key</li>
  <li>Secondary Key</li>
</ul>

<p><img src="http://i5.tietuku.com/25eedf0319e92775.png" width="430px" /></p>

<p>``` java
public class Person implements WritableComparable<person> {</person></p>

<p>private String firstName;
  private String lastName;</p>

<p>@Override
  public void readFields(DataInput in) throws IOException {
    this.firstName = in.readUTF();
    this.lastName = in.readUTF();
  }</p>

<p>@Override
  public void write(DataOutput out) throws IOException {
    out.writeUTF(firstName);
    out.writeUTF(lastName);
  }
…
```</p>

<p>下图说明hadoop框架配置中用于设置partitioning、sorting和grouping类的名字和方法。</p>

<p><img src="http://i5.tietuku.com/520e7242cd6ecc43.png" width="530px" /></p>

<h3 id="partitioner">Partitioner</h3>

<p>默认的partitioner使用对key进行hash后取reducer个数的模。但是默认的partitioner使用整个key，会导致相同的natural key发往不同的reducer。所以需要实现自己的partitioner。</p>

<p>``` java
public class PersonNamePartitioner extends
    Partitioner&lt;Person, Text&gt; {</p>

<p>@Override
  public int getPartition(Person key, Text value, int numPartitions) {
    return Math.abs(key.getLastName().hashCode() * 127) %
        numPartitions;
  }
} 
```</p>

<h3 id="sorting">Sorting</h3>

<p>``` java
public class PersonComparator extends WritableComparator {
  protected PersonComparator() {
    super(Person.class, true);
  }</p>

<p>@Override
  public int compare(WritableComparable w1, WritableComparable w2) {</p>

<pre><code>Person p1 = (Person) w1;
Person p2 = (Person) w2;


int cmp = p1.getLastName().compareTo(p2.getLastName());
if (cmp != 0) {
  return cmp;
}

return p1.getFirstName().compareTo(p2.getFirstName());   } } ```
</code></pre>

<h3 id="grouping">grouping</h3>

<p>grouping阶段所有的数据记录已经是secondary sort了，grouping comparator需要将相同的last name聚合在一起。</p>

<p>``` java
public class PersonNameComparator extends WritableComparator {</p>

<p>protected PersonNameComparator() {
    super(Person.class, true);
  }</p>

<p>@Override
  public int compare(WritableComparable o1, WritableComparable o2) {</p>

<pre><code>Person p1 = (Person) o1;
Person p2 = (Person) o2;

return p1.getLastName().compareTo(p2.getLastName());
</code></pre>

<p>}
}
```</p>

<h3 id="mapreduce">MapReduce</h3>

<p>最后在driver中，需要设置上文提到的三个类：</p>

<p>``` java
job.setPartitionerClass(PersonNamePartitioner.class);
job.setSortComparatorClass(PersonComparator.class);
job.setGroupingComparatorClass(PersonNameComparator.class);</p>

<p>public static class Map extends Mapper&lt;Text, Text, Person, Text&gt; {
  private Person outputKey = new Person();</p>

<p>@Override
  protected void map(Text lastName, Text firstName, Context context)
      throws IOException, InterruptedException {
    outputKey.set(lastName.toString(), firstName.toString());
    context.write(outputKey, firstName);
  }
}</p>

<p>public static class Reduce extends Reducer&lt;Person, Text, Text, Text&gt; {</p>

<p>Text lastName = new Text();
  @Override
  public void reduce(Person key, Iterable<text> values,
                     Context context)
      throws IOException, InterruptedException {
    lastName.set(key.getLastName());
    for (Text firstName : values) {
      context.write(lastName, firstName);
    }
  }
}   
```</text></p>

<p>Secondary sort涉及到的自定义的partitioner、sorter和grouper，还是比较复杂的。可以考虑<a href="http://htuple.org">htuple</a>对简单类型进行secondary sort。</p>

]]></content>
  </entry>
  
</feed>
