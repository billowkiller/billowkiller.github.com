<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tags: apriori | Billowkiller's Blog]]></title>
  <link href="http://billowkiller.github.io/blog/tags/apriori/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-06-01T23:20:53+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apriori and FP-Growth]]></title>
    <link href="http://billowkiller.github.io/blog/2016/03/06/apriori/"/>
    <updated>2016-03-06T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/03/06/apriori</id>
    <content type="html"><![CDATA[<p>如果说监督学习的形式化表达是 $Pr(Y \vert X)$, 找到最佳的参数最小化每个 $x$ 的epxected error： $argmin_{\theta}\ E_{Y \vert X} L(Y, \theta)$。那么非监督学习的形式化表达就是 $Pr(X)$，目标是在没有 $Y$ 引导的情况下，推测出 $Pr(X)$ 的潜在属性。本文要提到的apriori算法也是一种非监督学习算法，wiki的定义为</p>

<blockquote>
  <p>The Apriori Algorithm is an influential algorithm for mining frequent itemsets for boolean association rules.</p>
</blockquote>

<p>著名的啤酒和尿布例子就是指这个算法。它是属于关联分析中的一种，也就是从大规模数据集中寻找物品间的隐含关系。</p>

<!--more-->

<p>在apriori的定义中出现两个词，frequent itemsets 和 association rules，也就是频繁项集和关联规则，这里解释下频繁项集是经常出现在一块的物品的集合，关联规暗示两种物品之间可能存在很强的关系。下面给个简单的例子：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/5038926.jpg" width="300px" /></p>

<p>集合{葡萄酒，尿布, 豆奶} 就是频繁项集的一个例子，尿布➞葡萄酒 则是关联规则，意味着如果有人买了尿布，那么他很可能也会买葡萄酒。</p>

<h2 id="definition">Definition</h2>

<p>频繁是如何定义的呢？有许多的概念可以表示，主要有</p>

<ul>
  <li><strong>Support</strong>，这个表示数据集包含该项集的记录比例， $S(A) = T(A) / total\ transactions$</li>
  <li><strong>Confidence</strong>，$C(A \to B) = T(A \to B) / T(A)$</li>
  <li><strong>Lift</strong>, $L(A \to B) = C(A \to B)/ T(B)$</li>
</ul>

<p>如果有，Computer ➞ antivirus_software , 其中 support=2%, confidence=60%。
表示的意思是所有的商品交易中有2%的顾客同时买了电脑和杀毒软件，并且购买电脑的顾客中有60%也购买了杀毒软件。</p>

<p>下面我们给出关联规则形式化的定义：</p>

<p>假设 $I = \lbrace i_1, i_2…i_n \rbrace $ 表示items，$i_n$ 是binary attribute，表示商品买或不买。$T = \lbrace t_1, t_2…t_n \rbrace $ 是交易的集合，成为 database。每个交易都有唯一的标示，并且为 $I$ 的一个子集。规则就是 $X \to Y,\ where\ X,Y \subseteq I, X \cap Y = \varnothing$，这里 $X$ 称为 antecedent， $Y$ 称为 consequent.</p>

<h2 id="apriori-algorithm">Apriori Algorithm</h2>

<p>关联规则的生成过程可以分为两步：</p>

<ol>
  <li>首先根据最小support在database中找出所有的频繁项集</li>
  <li>根据所得的频繁项集和最小confidence约束生成规则</li>
</ol>

<p>在database中找到所有的频繁项集是比较困难的，因为需要找到所有可能的 $2^n-1$ 项集（排除空集）。虽然项集的个数是根据 $I$ 的大小呈指数增长，但是可以通过support的downward-closure特性进行有效的搜索。downward-closure表示对于一个频繁项，它的子集也必须是频繁的；对于一个非频繁的集合，它的超集必定也是非频繁的。下面给出伪代码：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/1439703.jpg" width="600px" /></p>

<p>迭代过程可以看成两个步骤：</p>

<ul>
  <li>Join Step: $C_k$ is generated by joining $L_{k-1}$ width itself</li>
  <li>Prune Step: Any (k-1)-itemset that is not frequent cnanot be a subset of a frequent k-itemset</li>
</ul>

<p>下面给出一个示例可以参考：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/4948435.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/92538392.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/62108557.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/26861819.jpg" width="500px" /></p>

<p>这里解释下3-itemset的生成，对于 $\lbrace I1,I2,I3 \rbrace$ 这儿例子，因为是属于L2的笛卡尔积，所以在L2中需要包含有 $\lbrace I1,I2 \rbrace, \lbrace I2, I3 \rbrace, \lbrace I1, I3 \rbrace$。所有不满足的都不能构成 $C_3$。</p>

<p>对于下一步 Generating 4-itemset Frequent Pattern，我们得到是一个空集，所以被剪枝了。最后一步是根据频繁项集生成关联规则。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/33075969.jpg" width="500px" /></p>

<p>对于大的数据量来说，生成频繁项集比较耗时，可以采用下面的方法提高效率。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-6/23943688.jpg" width="500px" /></p>

<h2 id="fp-growth">FP-Growth</h2>

<p>Apriori通过不断的构造候选集、筛选候选集挖掘出频繁项集，需要多次扫描原始数据，当原始数据较大时，磁盘I/O次数太多，效率比较低下。FP-Growth算法则只需扫描原始数据两遍，通过FP-tree数据结构对原始数据进行压缩，效率较高。</p>

<p>搜索引擎中的提示词项就可以用FP-Growth得到，通过输入项找到它的频繁项集。</p>

<p>FP-Growth算法主要分为两个步骤：FP-tree构建、递归挖掘FP-tree。FP-tree构建通过两次数据扫描，将原始数据中的事务压缩到一个FP-tree树，该FP-tree类似于前缀树，相同前缀的路径可以共用，从而达到压缩数据，减少数据库扫描的目的。接着通过FP-tree找出每个item的条件模式基、条件FP-tree，递归的挖掘条件FP-tree得到所有的频繁项集。递归挖掘FP-tree利用分治的思想将任务分解为更小的任务，并且可以避免频繁项集Candidate的生成。</p>

<p>还是以上面的数据库为示例，我们看下产生频繁项集的过程。剩下的挖掘关联规则则和Apriori一样。</p>

<ul>
  <li>
    <p>第一次扫描和Apriori一样，得到1-itemsets和它们的support counts。频繁集是按照support的带下倒序排列。结果为 $L = \lbrace I2:7,I1:6,I3:6,I4:2,I5:2 \rbrace$。</p>
  </li>
  <li>
    <p>第二次扫描的时候构建FP-tree。</p>

    <ul>
      <li>对每个transaction，过滤不频繁集合，剩下的频繁项集按 $L$ 顺序排序</li>
      <li>把每个transaction的1-itemsets插入到FP-tree中，相同前缀的路径可以共用</li>
      <li>同时增加一个header table，把FP-tree中相同item连接起来，也是降序排序</li>
    </ul>
  </li>
</ul>

<p>结果如图所示：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-7/45883923.jpg" width="500px" /></p>

<p>接下来进行频繁项的挖掘。</p>

<p>步骤为：</p>

<ol>
  <li>从header table的最下面的item开始，构造每个item的条件模式基（conditional pattern base）
    <ul>
      <li>顺着header table中item的链表，找出所有包含该item的前缀路径，这些前缀路径就是该item的条件模式基（CPB）</li>
      <li>所有这些CPB的频繁度（计数）为该路径上item的频繁度（计数）</li>
    </ul>
  </li>
  <li>构造条件FP-tree（conditional FP-tree）
    <ul>
      <li>累加每个CPB上的item的频繁度（计数），过滤低于阈值的item，构建FP-tree</li>
    </ul>
  </li>
  <li>FP-Growh：递归的挖掘每个条件FP-tree，累加后缀频繁项集，直到找到FP-tree为空或者FP-tree只有一条路径（只有一条路径情况下，所有路径上item的组合都是频繁项集）</li>
</ol>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-7/57749816.jpg" width="500px" /></p>

<p>接着上面的例子：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-7/37306135.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-7/29869117.jpg" width="500px" /></p>

<p>FP-Growth算法计算频繁项集的效率要高于Apriori，原因有以下几点：</p>

<ul>
  <li>无需Candidate的生成和检查</li>
  <li>使用压缩的数据结构</li>
  <li>减少重复的数据库扫描，事实上只需要两次</li>
  <li>基本的操作就是计数和构建FP-Tree</li>
</ul>

<h2 id="reference">Reference</h2>

<p><a href="http://120.52.72.49/software.ucv.ro/c3pr90ntcsf0/~cmihaescu/ro/teaching/AIR/docs/Lab8-Apriori.pdf">http://120.52.72.49/software.ucv.ro/c3pr90ntcsf0/~cmihaescu/ro/teaching/AIR/docs/Lab8-Apriori.pdf</a>
<a href="http://120.52.72.51/www3.cs.stonybrook.edu/c3pr90ntcsf0/~cse634/lecture_notes/07apriori.pdf">http://120.52.72.51/www3.cs.stonybrook.edu/c3pr90ntcsf0/~cse634/lecture_notes/07apriori.pdf</a></p>

]]></content>
  </entry>
  
</feed>
