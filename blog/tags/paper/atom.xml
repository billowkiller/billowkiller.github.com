<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tags: paper | Billowkiller's Blog]]></title>
  <link href="http://billowkiller.github.io/blog/tags/paper/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2013-05-15T09:18:01-04:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[Wutao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[google论文：MapReduce]]></title>
    <link href="http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-mapreduce/"/>
    <updated>2013-05-14T01:09:00-04:00</updated>
    <id>http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-mapreduce</id>
    <content type="html"><![CDATA[<p>论文：<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/mapreduce-osdi04.pdf">英文版</a>，<a href="http://blademaster.ixiezi.com/2010/03/27/google-mapreduce%E4%B8%AD%E6%96%87%E7%89%88/">中文版</a></p>

<hr />

<h2>1. 导论</h2>

<h3>1.1 定义</h3>

<p>先给个定义：
MapReduce是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户首先创建一个Map函数处理一个基于key/value
pair的数据集合，输出中间的基于key/value
pair的数据集合；然后再创建一个Reduce函数用来合并所有的具有相同中间key值的中间value值。</p>

<p>使用这个抽象模型，我们只要表述我们想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面。设计这个抽象模型的灵感来自Lisp和许多其他函数式语言的Map和Reduce的原语。</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr2_zps9c617225.png" alt="big picture of MapReduce"/></p>

<!--more-->


<h3>1.2 概述</h3>

<ul>
<li><p>Programmers must specify:</p>

<ul>
<li>map (k, v) → &lt;k’, v’>*</li>
<li>reduce (k’, v’) → &lt;k’, v’>*

<ul>
<li>All values with the same key are reduced together</li>
</ul>
</li>
</ul>
</li>
<li><p>Optionally, also:</p>

<ul>
<li><p>partition (k’, number of partitions) → partition for k’</p>

<ul>
<li>Often a simple hash of the key, e.g., hash(k’) mod n</li>
<li>Divides up key space for parallel reduce operations</li>
</ul>
</li>
<li><p>combine (k’, v’) → &lt;k’, v’>*</p>

<ul>
<li>Mini-reducers that run in memory after the map phase</li>
<li>Used as an optimization to reduce network traffic</li>
</ul>
</li>
</ul>
</li>
<li><p>The execution framework handles everything else…</p>

<ul>
<li>Scheduling: assigns workers to map and reduce tasks</li>
<li>“Data distribution”: moves processes to data</li>
<li>Synchronization: gathers, sorts, and shuffles intermediate data</li>
<li>Errors and faults: detects worker failures and restarts</li>
</ul>
</li>
<li><p>Limited control over data and execution flow</p>

<ul>
<li>All algorithms must expressed in m, r, c, p</li>
</ul>
</li>
<li><p>You don’t know:</p>

<ul>
<li>Where mappers and reducers run</li>
<li>When a mapper or reducer begins or finishes</li>
<li>Which input a particular mapper is processing</li>
<li>Which intermediate key a particular reducer is processing</li>
</ul>
</li>
</ul>


<h2>2. 实现</h2>

<h3>2.1 流程</h3>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr1_zps85dad9ca.png" alt="Execution Overview" height="500px"></p>

<p>上图展示了我们的MapReduce实现中操作的全部流程。</p>

<ol>
<li>用户程序首先调用的MapReduce库将输入文件分成M个数据片段，每个数据片段的大小一般从16MB到64MB。然后用户程序在机群中创建大量的程序副本。</li>
<li>这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是worker程序，由master分配任务。有M个Map任务和R个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。</li>
<li>被分配了map任务的worker程序读取相关的输入数据片段，从输入的数据片段中解析出key/value
pair，然后把key/value
pair传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value
pair，并缓存在内存中。</li>
<li>缓存中的key/value
pair通过分区函数分成R个区域，之后周期性的写入到本地磁盘上。缓存的key/value
pair在本地磁盘上的存储位置将被回传给master，由master负责把这些存储位置再传送给Reduce
worker。</li>
<li>当Reduce
worker程序接收到master程序发来的数据存储位置信息后，使用RPC从Map
worker所在主机的磁盘上读取这些缓存数据。当Reduce
worker读取了所有的中间数据后，通过对key进行排序后使得具有相同key值的数据聚合在一起。由于许多不同的key值会映射到相同的Reduce任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li>
<li>Reduce
worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce
worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。</li>
<li>当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对MapReduce调用才返回。</li>
</ol>


<h3>2.2 map和Reduce的同步</h3>

<ul>
<li><p>Cleverly-constructed data structures</p>

<ul>
<li>Bring partial results together</li>
</ul>
</li>
<li><p>Sort order of intermediate keys</p>

<ul>
<li>Control order in which reducers process keys</li>
</ul>
</li>
<li><p>Partitioner</p>

<ul>
<li>Control which reducer processes which keys</li>
</ul>
</li>
<li><p>Preserving state in mappers and reducers</p>

<ul>
<li>Capture dependencies across multiple keys and values</li>
</ul>
</li>
</ul>


<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr3_zps5ead0c7b.png" alt="map和Reduce的同步" height="300px"></p>

<h3>2.3 本地聚合</h3>

<ul>
<li><p>Ideal scaling characteristics:</p>

<ul>
<li>Twice the data, twice the running time</li>
<li>Twice the resources, half the running time</li>
</ul>
</li>
<li><p>Why can’t we achieve this?</p>

<ul>
<li>Synchronization requires communication</li>
<li>Communication kills performance</li>
</ul>
</li>
<li><p>Thus… avoid communication!</p>

<ul>
<li>Reduce intermediate data via local aggregation</li>
<li>Combiners can help</li>
</ul>
</li>
</ul>


<h3>2.4 Shuffle and Sort</h3>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr4_zps7ee59e35.png" alt="Shuffle and Sort" height="400px"/></p>

<h3>2.5 Master</h3>

<p>Master持有一些数据结构，它存储每一个Map和Reduce任务的状态（空闲、工作中或完成)，以及Worker机器(非空闲任务的机器)的标识。</p>

<p>Master就像一个数据管道，中间文件存储区域的位置信息通过这个管道从Map传递到Reduce。因此，对于每个已经完成的Map任务，master存储了Map任务产生的R个中间文件存储区域的大小和位置。当Map任务完成时，Master接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。</p>

<p>master周期性的ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息，master将把这个worker标记为失效。所有由这个失效的worker完成的Map任务被重设为初始的空闲状态，之后这些任务就可以被安排给其他的worker。同样的，worker失效时正在运行的Map或Reduce任务也将被重新置为空闲状态，等待重新调度。</p>

<p>master周期性的将数据写入磁盘，即检查点（checkpoint）。如果这个master任务失效了，可以从最后一个检查点开始启动另一个master进程。然而，由于只有一个master进程，master失效后再恢复是比较麻烦的，因此我们现在的实现是如果master失效，就中止MapReduce运算。客户可以检查到这个状态，并且可以根据需要重新执行MapReduce操作。</p>

<h2>3. 性能优化</h2>

<h3>3.1 straggler</h3>

<p>影响一个MapReduce的总执行时间最通常的因素是straggler(落伍者)：在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过预期。</p>

<p>当一个MapReduce操作接近完成的时候，master调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，我们都把这个任务标记成为已经完成。我们调优了这个机制，通常只会占用比正常操作多几个百分点的计算资源。我们发现采用这样的机制对于减少超大MapReduce操作的总处理时间效果显著。</p>

<h3>3.2 分区函数(partitioning function)</h3>

<p>我们在中间key上使用分区函数来对数据进行分区，之后再输入到后续任务执行进程。一个缺省的分区函数是使用hash方法(比如，hash(key)
mod
R)进行分区。hash方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对key值进行的分区将非常有用。</p>

<p>使用“hash(Hostname(urlkey)) mod
R”作为分区函数就可以把所有来自同一个主机的URLs保存在同一个输出文件中。</p>

<h3>3.3 顺序保证</h3>

<p>在给定的分区中，中间key/value
pair数据的处理顺序是按照key值增量顺序处理的。</p>

<h3>3.4 Combiner函数</h3>

<p>用户指定一个可选的combiner函数，combiner函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</p>

<p>一般情况下，Combiner和Reduce函数是一样的。Combiner函数和Reduce函数之间唯一的区别是MapReduce库怎样控制函数的输出。</p>

<h3>3.5 跳过损坏的记录</h3>

<ul>
<li>Map/Reduce functions sometimes fail for particular inputs

<ul>
<li><p>Best solution is to debug &amp; fix</p>

<ul>
<li>Not always possible \~ third-party source libraries</li>
</ul>
</li>
<li><p>On segmentation fault:</p>

<ul>
<li>Send UDP packet to master from signal handler</li>
<li>Include sequence number of record being processed</li>
</ul>
</li>
<li><p>If master sees two failures for same record:</p>

<ul>
<li>Next worker is told to skip the record</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>4. 要点和例子</h2>

<h3>4.1 Points need to be emphasized</h3>

<ul>
<li>No reduce can begin until map is complete</li>
<li>Master must communicate locations of intermediate files</li>
<li>Tasks scheduled based on location of data</li>
<li>If map worker fails any time before reduce finishes, task must be
completely rerun</li>
<li>MapReduce library does most of the hard work for us!</li>
</ul>


<h3>4.2 例子</h3>

<ol>
<li>分布式的Grep：Map函数输出匹配某个模式的一行，Reduce函数是一个恒等函数，即把中间数据复制到输出。</li>
<li>计算URL访问频率：Map函数处理日志中web页面请求的记录，然后输出(URL,1)。Reduce函数把相同URL的value值都累加起来，产生(URL,记录总数)结果。</li>
<li>倒转网络链接图：Map函数在源页面（source）中搜索所有的链接目标（target）并输出为(target,source)。Reduce函数把给定链接目标（target）的链接组合成一个列表，输出(target,list(source))。</li>
<li>每个主机的检索词向量：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。Map函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的URL。Reduce函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的(主机名,检索词向量)。</li>
<li>倒排索引：Map函数分析每个文档输出一个(词,文档号)的列表，Reduce函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出(词,list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。</li>
<li>分布式排序：Map函数从每个记录提取key，输出(key,record)。Reduce函数不改变任何的值。这个运算依赖分区机制(在4.1描述)和排序属性(在4.2描述)。</li>
</ol>


<h2>5. Hadoop</h2>

<p><strong>术语对照</strong></p>

<p> <table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>翻译</strong></span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>Hadoop</strong><strong>术语</strong></span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>Google</strong><strong>术语</strong></span></p>


<p></td>
<td width="277"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>相关解释</strong></span></p>


<p></td>
</tr>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">作业</span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Job</span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Job</span></p>


<p></td>
<td width="277"></p>

<p class="TableContents"><span style="font-size: 14px;">用户的每一个计算请求，就称为一个作业。</span></p>


<p></td>
</tr>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">作业服务器</span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">JobTracker</span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Master</span></p>


<p></td>
<td width="277"></p>

<p class="TableContents"><span style="font-size: 14px;">用户提交作业的服务器，同时，它还负责各个作业任务的分配，管理所有的任务服务器。</span></p>


<p></td>
</tr>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">任务服务器</span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">TaskTracker</span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Worker</span></p>


<p></td>
<td width="277"></p>

<p class="TableContents"><span style="font-size: 14px;">任劳任怨的工蜂，负责执行具体的任务。</span></p>


<p></td>
</tr>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">任务</span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Task</span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Task</span></p>


<p></td>
<td width="277"></p>

<p class="TableContents"><span style="font-size: 14px;">每一个作业，都需要拆分开了，交由多个服务器来完成，拆分出来的执行单位，就称为任务。</span></p>


<p></td>
</tr>
<tr>
<td width="109"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">备份任务</span></p>


<p></td>
<td width="136"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Speculative Task</span></p>


<p></td>
<td width="142"></p>

<p class="TableContents" align="center"><span style="font-size: 14px;">Buckup Task</span></p>


<p></td>
<td width="277"></p>

<p class="TableContents"><span style="font-size: 14px;">每一个任务，都有可能执行失败或者缓慢，为了降低为此付出的代价，系统会未雨绸缪的实现在另外的任务服务器上执行同样一个任务，这就是备份任务。</span></p>


<p></td>
</tr>
</tbody>
</table>
具体可以看博文<a href="http://www.cnblogs.com/duguguiyu/archive/2009/02/28/1400278.html">http://www.cnblogs.com/duguguiyu/archive/2009/02/28/1400278.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[google论文：GFS]]></title>
    <link href="http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-gfs/"/>
    <updated>2013-05-14T01:09:00-04:00</updated>
    <id>http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-gfs</id>
    <content type="html"><![CDATA[<p>论文：<a href="http://www.cs.rochester.edu/meetings/sosp2003/papers/p125-ghemawat.pdf">英文版</a>，<a href="http://blademaster.ixiezi.com/2010/03/27/the-google-file-system%E4%B8%AD%E6%96%87%E7%89%88/">中文版</a></p>

<hr />

<h2>1. 导论</h2>

<p> 先给个定义：GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，但可以提供容错功能。它可以给大量的用户提供总体性能较高的服务。</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/GoogleFileSystemGFS.svg/500px-GoogleFileSystemGFS.svg.png" alt="Google File System. Designed for system-to-system interaction, and not for user-to-system interaction. The chunk servers replicate the data automatically"/></p>

<p><strong>Assumptions in Google File System (GFS)</strong></p>

<ul>
<li><p>GFS should be built with commodity hardware</p>

<ul>
<li>Inexpensive disks and machines</li>
</ul>
</li>
<li><p>GSF stores a modest number of large files</p>

<ul>
<li><p>GSF stores a modest number of large files</p>

<ul>
<li>e.g. Big-table, Map-Reduce records</li>
</ul>
</li>
<li><p>Do not optimize for small files</p></li>
</ul>
</li>
<li><p>Workloads</p>

<ul>
<li>Large streaming reads (1MB or more) and small random reads (a
few KBs)</li>
<li>Sequential appends to files by hundreds of data producers

<ul>
<li>Utilizing the fact that files are seldom modified again</li>
</ul>
</li>
</ul>
</li>
<li><p>High sustained bandwidth is more important than latency</p>

<ul>
<li>Response time for individual read and write is not critical</li>
</ul>
</li>
</ul>


<!--more-->


<p><strong>Prerequisite</strong></p>

<ol>
<li>组件失效被认为是常态事件，而不是意外事件。</li>
<li>以通常的标准衡量，我们的文件非常巨大。</li>
<li>绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。</li>
<li><p>应用程序和文件系统API的协同设计提高了整个系统的灵活性。e.g.</p>

<ul>
<li> 放松了在GFS一致性模型的要求</li>
<li>引入了原子性的记录追加操作</li>
<li>三个冗余的数据可以不是位一致，但是要求校验和验证</li>
</ul>
</li>
<li><p>系统的工作负载</p>

<ul>
<li><p>读操作</p>

<ul>
<li>大规模的流式读取</li>
<li>小规模的随机读取</li>
</ul>
</li>
<li><p>写操作</p>

<ul>
<li>许多大规模的、顺序的、数据追加方式的写操作</li>
</ul>
</li>
</ul>
</li>
<li><p>系统必须高效的、行为定义明确的实现多客户端并行追加数据到同一个文件里</p>

<ul>
<li>使用最小的同步开销来实现的原子的多路追加数据操作是必不可少的</li>
<li>文件可以在稍后读取，或者是消费者在追加的操作的同时读取文件</li>
</ul>
</li>
<li><p>高性能的稳定网络带宽远比低延迟重要</p>

<ul>
<li>高速率的、大批量的处理数据</li>
<li>极少有程序对单一的读写操作有严格的响应时间要求</li>
</ul>
</li>
</ol>


<h2><strong>2. 架构</strong></h2>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/ache_zps14a9a2bc.png" alt="GFS Achitecture" height="300px"></p>

<ul>
<li>Files are divided into chunks</li>
<li>Fixed-size chunks (64MB)</li>
<li>Replicated over chunkservers, called replicas</li>
<li>Unique 64-bit chunk handles</li>
<li><p>Chunks as Linux files</p></li>
<li><p>Single master</p></li>
<li><p>Multiple chunkservers</p>

<ul>
<li>Grouped into Racks</li>
<li>Connected through switches</li>
</ul>
</li>
<li><p>Multiple clients</p></li>
<li>Master/chunkserver coordination

<ul>
<li>HeartBeat messages</li>
</ul>
</li>
</ul>


<p><strong><em>注意逻辑的Master节点和物理的Master服务器的区别</em></strong></p>

<h3>Master节点</h3>

<p>Master节点管理所有的文件系统元数据。这些元数据包括名字空间、访问控制信息、文件和Chunk的映射信息、以及当前Chunk的位置信息。Master节点还管理着系统范围内的活动，比如，Chunk租用管理、失效Chunk的回收、以及Chunk在Chunk服务器之间的迁移。</p>

<p>单一的Master节点可以通过全局的信息精确定位Chunk的位置以及进行复制决策。另外，我们必须减少对Master节点的读写，避免Master节点成为系统的瓶颈。客户端并不通过Master节点读写文件数据。反之，客户端向Master节点询问它应该联系的Chunk服务器。客户端将这些元数据信息缓存一段时间，后续的操作将直接和Chunk服务器进行数据读写操作。</p>

<h3>Master服务器</h3>

<p>Master服务器存储3种主要类型的元数据，包括：文件和Chunk的命名空间、文件和Chunk的对应关系、每个Chunk副本的存放地点。所有的元数据都保存在Master服务器的内存中。前两种类型的元数据（命名空间、文件和Chunk的对应关系）同时也会以记录变更日志的方式记录在操作系统的系统日志文件中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程Master服务器上。采用保存变更日志的方式，我们能够简单可靠的更新Master服务器的状态，并且不用担心Master服务器崩溃导致数据不一致的风险。Master服务器不会持久保存Chunk位置信息。Master服务器在启动时，或者有新的Chunk服务器加入时，向各个Chunk服务器轮询它们所存储的Chunk的信息。</p>

<p>Master服务器并不保存持久化保存哪个Chunk服务器存有指定Chunk的副本的信息。Master服务器只是在启动的时候轮询Chunk服务器以获取这些信息。Master服务器能够保证它持有的信息始终是最新的，因为它控制了所有的Chunk位置的分配，而且通过周期性的心跳信息监控Chunk服务器的状态。</p>

<ul>
<li>简化了在有Chunk服务器加入集群、离开集群、更名、失效、以及重启的时候，Master服务器和Chunk服务器数据同步的问题。</li>
<li>只有Chunk服务器才能最终确定一个Chunk是否在它的硬盘上。Master服务器无需维护一个这些信息的全局视图</li>
</ul>


<h3>操作日志</h3>

<p>操作日志包含了关键的元数据变更历史记录。这对GFS非常重要。这不仅仅是因为操作日志是元数据唯一的持久化存储记录，它也作为判断同步操作顺序的逻辑时间基线。文件和Chunk，连同它们的版本，都由它们创建的逻辑时间唯一的、永久的标识。</p>

<p>必须确保日志文件的完整，确保只有在元数据的变化被持久化后，日志才对客户端是可见的。把日志复制到多台远程机器，并且只有把相应的日志记录写入到本地以及远程机器的硬盘后，才会响应客户端的操作请求。</p>

<p>为了缩短Master启动的时间，我们必须使日志足够小。Master服务器在日志增长到一定量时对系统状态做一次Checkpoint，将所有的状态数据写入一个Checkpoint文件。在灾难恢复的时候，Master服务器就通过从磁盘上读取这个Checkpoint文件，以及重演Checkpoint之后的有限个日志文件就能够恢复系统。</p>

<p>由于创建一个Checkpoint文件需要一定的时间，所以Master服务器的内部状态被组织为一种格式，这种格式要确保在Checkpoint过程中不会阻塞正在进行的修改操作。</p>

<h3><strong>一致性模型</strong></h3>

<ul>
<li>Relaxed consistency model</li>
<li><p>Two types of mutations</p>

<ul>
<li><p>Writes</p>

<ul>
<li>Cause data to be written at an application-specified file
offset</li>
</ul>
</li>
<li><p>Record appends</p>

<ul>
<li>Operations that append data to a file</li>
<li>Cause data to be appended atomically at least once</li>
<li>Offset chosen by GFS, not by the client</li>
</ul>
</li>
</ul>
</li>
<li><p>States of a file region after a mutation</p>

<ul>
<li><p>Consistent</p>

<ul>
<li>All clients see the same data, regardless which replicas
they read from</li>
</ul>
</li>
<li><p>Defined</p>

<ul>
<li>consistent + all clients see what the mutation writes in its
entirety</li>
</ul>
</li>
<li><p>Undefined</p>

<ul>
<li>consistent +but it may not reflect what any one mutation has
written</li>
</ul>
</li>
<li><p>Inconsistent</p>

<ul>
<li>Clients see different data at different times</li>
<li>The client retries the operation</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>经过了一系列的成功的修改操作之后，GFS确保被修改的文件region是已定义的，并且包含最后一次修改操作写入的数据。GFS通过以下措施确保上述行为：（a）
对Chunk的所有副本的修改操作顺序一致，（b）使用Chunk的版本号来检测副本是否因为它所在的Chunk服务器宕机而错过了修改操作而导致其失效。失效的副本不会再进行任何修改操作，Master服务器也不再返回这个Chunk副本的位置信息给客户端。它们会被垃圾收集系统尽快回收。</p>

<p>使用GFS的应用程序可以利用一些简单技术实现这个宽松的一致性模型，这些技术也用来实现一些其它的目标功能，包括：</p>

<ul>
<li>尽量采用追加写入而不是覆盖</li>
<li><p>Checkpoint</p>

<ul>
<li>to verify how much data has been successfully written</li>
</ul>
</li>
<li><p>自验证的写入操作</p>

<ul>
<li>Checksums to detect and remove <em>padding</em></li>
</ul>
</li>
<li><p>自标识的记录。</p>

<ul>
<li>Unique Identifiers to identify and discard <em>duplicates</em></li>
</ul>
</li>
</ul>


<h2><strong>3. 系统交互</strong></h2>

<ul>
<li>Master uses leases to maintain a consistent mutation order among
replicas</li>
<li>Primary is the chunkserver who is granted a chunk lease</li>
<li>All others containing replicas are secondaries</li>
<li>Primary defines a mutation order between mutations</li>
<li><p>All secondaries follows this order</p></li>
<li><p>数据流和控制流分开</p></li>
<li><p>数据以管道的方式，顺序的沿着一个精心选择的Chunk服务器链推送</p>

<ul>
<li>Data transfer is pipelined over TCP connections</li>
<li>Each machine forwards the data to the “closest” machine</li>
<li>全双工的交换网络</li>
</ul>
</li>
<li><p><strong>Benefits</strong>：Avoid bottle necks and minimize latency</p></li>
</ul>


<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/c_zps5923b611.png" alt="Write control and Data Flow"></p>

<ol>
<li>客户机向Master节点询问哪一个Chunk服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk持有租约，Master节点就选择其中一个副本建立一个租约。</li>
<li>Master节点将主Chunk的标识符以及其它副本（又称为secondary副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便后续的操作。只有在主Chunk不可用，或者主Chunk回复信息表明它已不再持有租约的时候，客户机才需要重新跟Master节点联系。</li>
<li>客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk服务器接收到数据并保存在它的内部LRU缓存中，一直到数据被使用或者过期交换出去。由于数据流的网络传输负载非常高，通过分离数据流和控制流，我们可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会哪个Chunk服务器保存了主Chunk。</li>
<li>当所有的副本都确认接收到了数据，客户机发送写请求到主Chunk服务器。这个请求标识了早前推送到所有副本的数据。主Chunk为接收到的所有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中。</li>
<li>主Chunk把写请求传递到所有的二级副本。每个二级副本依照主Chunk分配的序列号以相同的顺序执行这些操作。</li>
<li>所有的二级副本回复主Chunk，它们已经完成了操作。</li>
<li>主Chunk服务器回复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主Chunk和一些二级副本执行成功。（如果操作在主Chunk上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的region处于不一致的状态。我们的客户机代码通过重复执行失败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤（3）到步骤（7）做几次尝试。</li>
</ol>


<h3>记录追加的原子性</h3>

<ul>
<li>The client specifies only the data (not file offset)</li>
<li><p>Similar to writes</p>

<ul>
<li>Mutation order is determined by the primary</li>
<li>All secondaries use the same mutation order</li>
</ul>
</li>
<li><p>GFS appends data to the file at least once atomically</p>

<ul>
<li>The chunk is padded if appending the record exceeds the maximum
size &mdash;> <em>padding</em></li>
<li>If a record append fails at any replica, the client retries the
operation &mdash;> <em>record duplicates</em></li>
<li>File region may be defined but interspersed with <em>inconsistent</em></li>
</ul>
</li>
</ul>


<h3>快照</h3>

<ul>
<li><p>Goals</p>

<ul>
<li>To quickly create branch copies of huge data sets</li>
<li>To easily checkpoint the current state</li>
</ul>
</li>
<li><p>Copy-on-write technique</p>

<ul>
<li>Metadata for the source file or directory tree is duplicated</li>
<li>Reference count for chunks are incremented</li>
<li>Chunks are copied later at the first write</li>
</ul>
</li>
</ul>


<h3><strong>Master Operation</strong></h3>

<ul>
<li>Namespaces are represented as a lookup table mapping full pathnames
to metadata</li>
<li>Use locks over regions of the namespace to ensure proper
serialization</li>
<li><p>Each master operation acquires a set of locks before it runs</p></li>
<li><p>GFS has no directory (i-node) structure</p>

<ul>
<li>Simply uses directory-like file names: /foo, /foo/bar

<ul>
<li>Thus listing files in a directory is slow</li>
</ul>
</li>
</ul>
</li>
<li><p>Concurrent Access</p>

<ul>
<li>Read lock on a parent path, write lock on the leaf file name

<ul>
<li>protect delete, rename and snapshot of in-use files</li>
</ul>
</li>
</ul>
</li>
<li><p>Rebalancing</p>

<ul>
<li>Places new replicas on chunk servers with below-average disk
space utilizations</li>
</ul>
</li>
<li><p>Re-replication</p>

<ul>
<li><p>When the number of replicas falls below 3 (or user-specified
threshold)</p>

<ul>
<li>The master assigns the highest priority to copy (clone) such
chunks</li>
</ul>
</li>
<li><p>Spread replicas of a chunk across racks</p></li>
</ul>
</li>
</ul>


<p><strong>Example of Locking Mechanism</strong></p>

<p>Preventing /home/user/foo from being created while /home/user is being
snapshotted to /save/user</p>

<ul>
<li><p>Snapshot operation</p>

<ul>
<li>Read locks on /home and /save</li>
<li>Write locks on /home/user and /save/user</li>
</ul>
</li>
<li><p>File creation</p>

<ul>
<li>read locks on /home and /home/user</li>
<li>write locks on /home/user/foo</li>
</ul>
</li>
<li><p>Conflict locks on /home/user</p></li>
</ul>


<h2>4. 其他细节</h2>

<h3><strong>垃圾回收</strong></h3>

<ul>
<li><p>Deleted files</p>

<ul>
<li>Deletion operation is logged</li>
<li>File is renamed to a hidden name(deferred deletion)， then may be
removed later or get recovered</li>
<li>The master regularly scans and removes hidden files, existed
more than three days

<ul>
<li>HeartBeat messages inform chunk servers of deleted chunks</li>
</ul>
</li>
</ul>
</li>
<li><p>Orphaned chunks (unreachable chunks)</p>

<ul>
<li>Identified and removed during a regular scan of the chunk
namespace</li>
</ul>
</li>
<li><p>Stale replicas</p>

<ul>
<li>Chunk version numbering

<ul>
<li>increases when the master grants a new lease of the chunk</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>Replica Operations</h3>

<ul>
<li><p>Creation</p>

<ul>
<li>Disk space utilization</li>
<li>Number of recent creations on each chunkserver</li>
<li>Spread across many racks</li>
</ul>
</li>
<li><p>Re-replication</p>

<ul>
<li>Prioritized: How far it is from its replication goal…</li>
<li>The highest priority chunk is cloned first by copying the chunk
data directly from an existing replica</li>
</ul>
</li>
<li><p>Rebalancing</p>

<ul>
<li>Periodically</li>
</ul>
</li>
</ul>


<h3>Fault Tolerance</h3>

<ul>
<li><p>Fast Recovery</p>

<ul>
<li>The master and the chunk server are designed to restore their
state in seconds no matter how they terminated.</li>
<li>Servers are routinely shut down just by killing the process</li>
</ul>
</li>
<li><p>Master Replications</p>

<ul>
<li>Master has the maps from file names to chunks</li>
<li>One (primary) master manages chunk mutations

<ul>
<li>Several shadow masters are provided for read-only accesses

<ul>
<li>Snoop operation logs and apply these operations exactly
as the primary does</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Data Integrity</p>

<ul>
<li><p>Corruption of stored data</p>

<ul>
<li>High temperature of storage devices causes such errors</li>
</ul>
</li>
<li><p>Checksums for each 64KB in a chunk</p>

<ul>
<li>chunk servers verifies the checksum of data before sending
it to the client or other chunk servers</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[google论文: big table]]></title>
    <link href="http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-big-table/"/>
    <updated>2013-05-14T01:09:00-04:00</updated>
    <id>http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-big-table</id>
    <content type="html"><![CDATA[<p>论文：<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/bigtable-osdi06.pdf">英文版</a>，
<a href="http://dblab.xmu.edu.cn/sites/default/files/20120508_172346_207.pdf">中文版</a></p>

<hr />

<h3>1. 导论</h3>

<p>BigTable is a compressed, high performance, and proprietary data storage
system built on Google File System, Chubby Lock Service, SSTable
(log-structured storage like LevelDB) and a few other Google
technologies.（这句是wikipedia摘抄的定义）</p>

<p> 构成：</p>

<ul>
<li>Scheduler</li>
<li><a href="http://www.cnblogs.com/billowkiller/archive/2013/04/10/3012085.html">GFS</a></li>
<li>Chubby Lock service</li>
<li>Sawzall</li>
<li><a href="http://www.cnblogs.com/billowkiller/archive/2013/04/15/3022035.html">MapReduce</a></li>
<li>SSTable</li>
<li>some compression algorithms</li>
</ul>


<p>先通俗几个名词：</p>

<!--more-->


<p>Google File System</p>

<p>　　• Large-scale distributed “file system”</p>

<p>　　• Master: responsible for metadata</p>

<p>　　• Chunk servers: responsible for reading and writing large chunks of
data</p>

<p>　　• Chunks replicated on 3 machines, master responsible for ensuring
replicas exist</p>

<p>Chubby</p>

<p>　　• {lock/file/name} service</p>

<p>　　• Coarse-grained locks, can store small amount of data in a lock</p>

<p>　　• 5 replicas, need a majority vote to be active</p>

<p>SSTable</p>

<p>　　• Immutable, sorted file of key-value pairs</p>

<p>　　• Chunks of data plus an index</p>

<p>　　　　– Index is of block ranges, not values</p>

<p>　　　　– Index loaded into memory when SSTable is opened</p>

<p>　　　　– Lookup is a single disk seek</p>

<p>　　• Alternatively, client can load SSTable into mem</p>

<p><img src="http://images.cnitblog.com/blog/434023/201303/30210501-5a1c2d5da5104b979ea6d2567f37edfb.png" alt="SSTable" height="150px"/></p>

<p>Tablet</p>

<p>　　• Contains some range of rows of the table</p>

<p>　　• Unit of distribution &amp; load balance</p>

<p>　　• Built out of multiple SSTables</p>

<p><img src="http://images.cnitblog.com/blog/434023/201303/30210659-c4b2010c6dee47f6a8f5c2c799ed1114.png" alt="Tablet" height="200px"/></p>

<p>Table</p>

<p>　　• Multiple tablets make up the table</p>

<p>　　• SSTables can be shared</p>

<p>　　• Tablets do not overlap, SSTables can overlap</p>

<p><img src="http://images.cnitblog.com/blog/434023/201303/30210828-a38c1e041a6d4add86c387cda95a24d2.png" alt="Table" height="200px"/></p>

<h3>2. 数据模型</h3>

<p>BigTable不能支持完整的关系型数据模型，只提供一个简单的数据模型，可以支持针对数据部署和格式的动态控制，允许用户区推理底层存储所展现的数据的位置属性。这句话有些玄乎，其实就是nosql的典型存储方式，不管你需要存储的数据有多大，是什么样的格式——图片，音乐或者其他，bigtable都把它们当做字符串存储在GFS中。</p>

<p>这个数据模型世纪上市一个稀疏的、分布的、永久的多位排序图。采用row
key，column key，timestamp对图进行索引。每个值都是未经解释的字节数组。</p>

<p>(row:string, column string, time:int64)→string</p>

<p>一个表中的行键，是任意的字符串(当前尺寸是64KB)，
对每一个行键所包含数据的读或写都是一个原子操作，而不管这个行中所包含的的列的数量多少。在行键上根据字典顺序对数据进行维护。对一个表而言，行区间是动态划分的，因为列的大小数量不确定。每一个行区间是一个Tablet，是负载均衡和数据分发的基本单位。</p>

<p>列键被分组为一个称为“Column
family”的集合，是基本的访问控制单元。存储在一个列家族当中的所有数据通常都属于同一个数据类型。因为通常是对同一个列家族中的数据一起压缩。存储上也是面向列进行的。列键的命名方式为Column family:qualifier。Family
is heavyweight, qualifier
lightweight，即为，列家族很少变化，基本上是固定的，但是修饰符可以是任意字符串。</p>

<p>这里给出一个实际的例子。假设我们想要拷贝一个可能被很多项目都是用的、很大的网页集合以及相关的信息，让我们把这个特定的表称为Webtable。在Webtable当中，我们使用URL作为行键，网页的不同方面作为列键，并把网页的内容存储在contents:column中，如图所示。通过对URL地址进行反转，属于同一个领域的网页都会被分组到连续的行中。例如，在键com.google.maps/index.html下面存储maps.google.com/index.html中包含的数据。把来自同一个领域的数据彼此临近存储，使得一些领域分析更加高效。</p>

<p><img src="http://dblab.xmu.edu.cn/sites/default/files/images/datamodel.jpg" alt="Google Bigtable，厦门大学，厦门大学计算机系，数据库实验室，林子雨" height="200px"/></p>

<ul>
<li>存储了网页数据的Webtable的一个片段。行名称是反转的URL，contents列家族包含了网页内容，anchor列家族包含了任何引用这个页面的anchor文本。CNN的主页被Sports
Illustrated和MY-look主页同时引用，因此，我们的行包含了名称为”anchor:cnnsi.com”和”anchor:my.look.ca”的列。每个anchor单元格都只有一个版本，contents列有三个版本，分别对应于时间戳t3,t5和t6。</li>
</ul>


<h3>3. 实现</h3>

<p>BigTable实现包括三个主要的功能组件：</p>

<p>(1) 库函数：链接到每个客户端，</p>

<p>(2) 一个主服务器，</p>

<p>(3) 许多Tablet服务器。</p>

<p>主服务器负责把Tablet分配到Tablet服务器，探测Tablet服务器的增加和过期，进行Table服务器的负载均衡，以及GFS文件系统中的垃圾收集。除此以外，它还处理模式变化，比如表和列家族创建。</p>

<p>每个Tablet服务器管理一个Tablet集合，通常，在每个Tablet服务器上，会放置10到1000个Tablet。Tablet服务器处理针对那些已经加载的Tablet而提出的读写请求，并且会对过大的Tablet进行划分。</p>

<p>客户端并不是直接从主服务器读取数据，而是直接从Tablet服务器上读取数据。因为BigTable客户端并不依赖于主服务器来获得Tablet的位置信息，所以，大多数客户端从来不和主服务器通信。从而使得在实际应用中，主服务器负载很小。</p>

<p><img src="http://images.cnitblog.com/blog/434023/201303/30212406-a7b6fa2ced51443eb5d8660694dfae2d.png" alt="big table system structure" height="400px"/></p>

<h3>4. tablet查找</h3>

<p>因为tablet是从一个服务器到另外一个服务器移动的，那么怎么找到一个特定的row在哪个服务器上存储呢？</p>

<p>使用了一个类似于 B+树的三层架构，来存储Tablet位置信息。</p>

<p><img src="http://dblab.xmu.edu.cn/sites/default/files/images/Fig4(1).jpg" alt="big table system structure" height="300px" alt="Google Bigtable，厦门大学，厦门大学计算机系，数据库实验室，林子雨"/></p>

<p>METADATA表中存储了二级信息，包括一个日志，它记载了和每个tablet有关的所有事件，比如，一个服务器什么时候开始提供这个tablet服务。这些信息对于性能分析和程序调试是非常有用的。</p>

<p>客户端函数库会缓存Tablet位置信息。如果客户端不知道一个Tablet的位置信息，或者它发现，它所缓存的Tablet位置信息部正确，那么，它就会在Tablet位置层次结构中依次向上寻找。如果客户端缓存是空的，那么定位算法就需要进行三次轮询，其中就包括一次从Chubby中读取信息。如果客户端的缓存是过期的，定位算法就要进行六次轮询，因为，只有在访问无效的时候才会发现缓存中某个entry是过期的（这里假设METADATA
Tablets不会频繁移动）。虽然，Tablets位置信息是保存在缓存中，从而不需要访问GFS，但是，仍然通过让客户端库函数预抓取tablet位置信息，来进一步减少代价，具体方法是：每次读取METADATA表时，都要读取至少两条以上的Tablet位置信息。</p>

<p>每个Tablet只能被分配到一个tablet服务器。主服务器跟踪tablet服务器的情况，掌握当前tablet被分配到tablet服务器的情况，其中包括哪个tablet还没有被分配。当一个tablet没有被分配，并且一个具有足够空间可以容纳该tablet的tablet服务器是可用时，主服务器就把当前这个tablet分配给这个tablet服务器，主服务器会向tablet服务器发送一个tablet负载请求。</p>

<p> BigTable使用Chubby来跟踪tablet服务器。当一个Tablet服务器启动的时候，它创建并且获得一个独占的排他锁，这个锁会锁住一个特定的Chubby目录中的一个唯一命名的文件。主服务器监视这个目录（服务器目录），来发现tablet服务器。如果一个tablet服务器停止服务，它就会丢失这个锁，比如，由于网络故障，导致这个tablet服务器丢失了这个Chubby会话。（Chubby提供了一个完善的机制，来允许一个tablet服务器检查自己是否已经丢失了这个独占排他锁）。如果丢失了锁，那么，只要目录中的这个文件还存在，那么一个tablet服务器就会努力去获得这个锁。如果文件不再存在，那么，这个tablet服务器就不再能够对外提供服务，因此，它就自杀。一旦一个tablet服务器终止了服务（比如，簇管理系统把这个tablet服务器从簇中移除），它就会努力释放锁，这样，主服务器就可以更快地重新分配这个tablet。</p>

<p>主服务器需要探测，什么时候tablet服务器不再提供tablet服务，并且要负责尽快对这些tablet进行重新分配。为了探测什么时候tablet服务器不再提供tablet服务，主服务器会周期性地询问每个tablet服务器，了解他们的锁的状态。如果一个tablet服务器报告，它已经丢失了锁；或者，在最近的几次尝试中，主服务器都无法与tablet服务器取得联系，主服务器就会努力获得一个针对这个服务器文件的独占排他锁。如果主服务器可以获得这个锁，那么，Chubby就是可用的，相应地，这个tablet服务器或者已经死亡，或者有些故障导致它无法到达Chubby。因此，主服务器就从Chubby中删除这个tablet服务器的文件，从而确保这个tablet服务器不再能够提供服务。一旦一个服务器文件被删除，主服务器就可以把所有以前分配给该服务器的tablet，都移动到“待分配”tablet集合。为了保证一个BigTable簇不会轻易受到主服务器和Chubby之间的网络故障的影响，如果一个主服务器的Chubby会话过期了，这个主服务器就会自杀。但是，正如上所述，主服务器失效，不会改变tablet到table的分配。</p>

<h3>5. 读写方式</h3>

<p><img src="http://dblab.xmu.edu.cn/sites/default/files/images/Fig5.jpg" height="300px" alt="Google Bigtable，厦门大学，厦门大学计算机系，数据库实验室，林子雨"/></p>

<p> 一个tablet的持久化存储是存在GFS当中，如图5所示。更新被提交到一个提交日志，日志中记录了redo记录。在这些更新当中，最近提交的更新被存放到内存当中的一个被称为memtable的排序缓冲区，比较老的更新被存储在一系列SSTable中。为了恢复一个tablet，tablet服务器从METADATA表当中读取这个tablet的元数据。这个元数据包含了SSTable列表，其中，每个SSTable都包括一个tablet和一个重做点（redo
point）的集合，这些redo
point是一些指针，它们指向那些可能包含tablet所需数据的重做日志。服务器把SSTable索引读入内存，并且重构memtable，方法是，执行重做点以后的所有已经提交的更新。</p>

<p>当一个写操作到达tablet服务器，服务器首先检查它是否是良好定义的，并且发送者是否被授权执行该操作。执行授权检查时，会从一个Chubby文件中读取具有访问权限的写入者的列表，这个Chubby文件通常总能够在Chubby客户端缓存中找到。一个有效的变化，会被写到提交日志中。分组提交是为了改进许多小更新[13,16]操作的吞吐量。在写操作已经被提交以后，它的内容就会被插入到memtable。</p>

<p>当一个读操作到达Tablet服务器，与写操作类似，服务器也会首先检查它是否是良好定义和得到授权的。一个有效地读操作是在以下二者的合并的基础上执行的，即一系列SSTable和memtable。由于SSTable和memtable是字典排序的数据结构，合并视图的执行是非常高效的。</p>

<p><img src="http://images.cnitblog.com/blog/434023/201303/30213930-99817ed4d1154148869e43523bcc6f4b.png" height="300px" alt="tablet分解"/></p>

<p>当tablet发生合并或分解操作时，正在到达的读写操作仍然可以继续进行。</p>

<h3></h3>

<h3>6. 压缩</h3>

<p>压缩分为三种：</p>

<p>　　•　　Minor compaction – convert a full memtable into an SSTable, and
start a new memtable</p>

<p>　　　　　　– Reduce memory usage</p>

<p>　　　　　　– Reduce log traffic on restart</p>

<p>　　•　　Merging compaction</p>

<p>　　　　　　– Reduce number of SSTables</p>

<p>　　　　　　– Good place to apply policy “keep only N versions”</p>

<p>　　•　　Major compaction</p>

<p>　　　　　　– Merging compaction that results in only one SSTable</p>

<p>　　　　　　– No deletion records, only live data</p>

<h3>7. 完善措施</h3>

<ul>
<li><strong>locality group</strong></li>
</ul>


<p>　　客户端可以把多个列家族一起分组到一个locality
group中。我们会为每个tablet中的每个locality
group大都创建一个单独的SSTable。把那些通常不被一起访问的列家族分割到不同的locality
group，可以实现更高效的读。</p>

<p>　　除此以外，一些有用的参数，可以针对每个locality
group来设定。例如，一个locality
group可以设置成存放在内存中。常驻内存的locality
group的SSTable，采用被动加载的方式被加载tablet服务器的内存，即只有应用请求SSTable中的数据，而这些数据　　　又不在内存中时，才把SSTable加载到内存。一旦加载，属于这些locality
group的列家族，就可以被应用直接访问，而不需要读取磁盘。这个特性对于那些被频繁访问的小量数据来说是非常有用的。</p>

<ul>
<li><strong>Compression</strong></li>
</ul>


<p>　　　客户端可以决定是否对相应于某个locality
group的SSTable进行压缩，如果压缩，应该采用什么格式。用户自定义的压缩格式可以被应用到每个SSTable块中（块的尺寸可以采用与locality
group相关的参数来进行控制）。虽然对每个块进行单独压缩会损失一些空间，但是，我们可以从另一个方面受益，当解压缩时，只需要对小部分数据进行解压，而不需要解压全部数据。许多客户端都使用“两段自定义压缩模式”。第一遍使用Bentley
and
McIlroy[6]模式，它对一个大窗口内的长公共字符串进行压缩。第二遍使用一个快速的压缩算法，这个压缩算法在一个16KB数据量的窗口内寻找重复数据。</p>

<ul>
<li><strong>Bloom filters</strong></li>
</ul>


<p>　　一个读操作必须从构成一个tablet的当前状态的所有SSTable中读取数据。如果这些SSTable不在内存中，我们就不得不需要很多磁盘访问。我们通过下面的方式来减少磁盘访问，即允许客户端来确定，为某个特定locality
group中的SSTable创建Bloom filter。一个Bloom
filter允许我们询问，一个SSTabble是否包含属于指定的“行-列队”的特定的数据。对于某个特定的应用，一个用来存储Bloom
filter的很少量的tablet服务器内存空间，都可以极大减少读操作的磁盘访问次数。我们使用Bloom
filter也意味着，许多针对目前不存在的行或列的查询，根本就不需要访问磁盘。</p>
]]></content>
  </entry>
  
</feed>
