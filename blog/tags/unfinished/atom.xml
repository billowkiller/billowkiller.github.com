<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tags: unfinished | Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/blog/tags/unfinished/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-07-05T12:13:33+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recommender System in a Nutshell]]></title>
    <link href="http://billowkiller.github.io/blog/2015/04/10/recsys-introduction/"/>
    <updated>2015-04-10T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/04/10/recsys-introduction</id>
    <content type="html"><![CDATA[<p>推荐系统（Recommender System An Introduction）读书笔记。</p>

<!--more-->

<h2 id="section">基于内容的推荐系统</h2>

<p>基于内容的推荐：基于两类信息，<code>物品特征的描述</code>和描述用户兴趣的<code>用户记录</code>（比如喜爱的物品特点）。</p>

<p>一般工作原理是评估用户还没有看到的物品与当前用户过去喜欢的物品的相似度。那么如何表示物品的内容以及相似度？</p>

<p>描述物品目录最简单的方法就是维护每个物品特征的详细列表，<strong>属性集、特征集或物品记录</strong>。另外可以使用<code>关键词列表</code>表示文档内容，好处是能够从文档内容本身自动生成列表。</p>

<p>为了防止<code>通用词</code>和<code>长文档</code>带来的推荐结果偏置，一般会使用TF-IDF计算关键词的权重。给出下列公式：</p>

<script type="math/tex; mode=display"> TF(i, j) = \frac{freq(i,j)}{maxOthers(i,j)},\ IDF(i) = log\frac{N}{n(i)} </script>

<script type="math/tex; mode=display"> TF-IDF(i, j) = TF(i,j) \cdot IDF(i) </script>

<p>$TF(i,j)$ 表示文档 $j$ 中关键词 $i$ 的<code>归一化词频值</code>，$freq(i,j)$ 表示 $i$ 在 $j$ 中出现的绝对频率； $maxOthers(i,j)$ 表示 $j$ 中其他关键词的最大词频。 $N$ 为所有可推荐文档数量，$n(i)$ 为关键词 $i$ 出现过文档的数量。</p>

<p>归一化是针对长文档，IDF 是针对通用词</p>

<p>从文本中抽取关键词并赋予权值的做法局限性为</p>

<ul>
  <li>向量通常大而稀疏，通过以下几种技术改进：
    <ul>
      <li>停用词和词干还原</li>
      <li>特征选择，仅用信息量最大的一些词来减少文档描述规模，期望删除噪声。</li>
      <li>选用短语</li>
    </ul>
  </li>
  <li>没有考虑到文档上下文</li>
</ul>

<p>User-CF为“推荐相似用户喜欢的物品”，基于内容的推荐可以描述成“推荐与用户过去喜欢的物品相似的物品”。通常的基于内容相似度检索技术为：</p>

<ul>
  <li>KNN
    <ul>
      <li>用余弦相似度评估两个文档的向量是否相似；</li>
      <li>可以考虑用户的短期兴趣和长期兴趣，适合于个性化的新闻推荐；</li>
      <li>易于实现，能够快速适应新近变化，只需要相对少的评分数据；</li>
      <li>预测精确度较低。</li>
    </ul>
  </li>
  <li>相关项反馈——Rocchio方法</li>
</ul>

<p>另外可以将基于内容的推荐看成分类问题。</p>

<p>典型的是朴素贝叶斯方法，基本上有两种对文档及其特征建模的方法：<strong>多项式模型和伯努利模型</strong>，二者都忽略词的<code>位序</code>问题。多元伯努利模型中，文档被处理成一个二进制向量，描述某个词是否包含在文档中；多项式模型中，考虑词出现在文档中的次数，分类结果会比莫努力模型好些。</p>

<p>在多项式模型中，词 $v_i$ 出现在 $c$ 类文档的条件概率为：</p>

<table>
  <tbody>
    <tr>
      <td>$$ P(v_i</td>
      <td>C=c) = \frac{CountTerms(v_i, docs(c))+1}{AllTerms(docs(c))+\vert V \vert}$$</td>
    </tr>
  </tbody>
</table>

<p>上式采用<code>拉普拉斯平滑</code>以防止条件概率为 0，$\vert V \vert$ 为所有文档中不同词的数量。</p>

<p>朴素贝叶斯分类器能够达到很高的精确度，并且其组成部分能够在获得新数据时很容易更新，且学习时间复杂度随样本数量线性增加。</p>

<p>上文提到过<code>特征选择</code>问题，原始的大而稀疏向量会导致性能和内存需求问题，并且容易导致过拟合。典型的可以采用 $\chi^2$ 检验或 Fisher 判别指标。</p>

<p>$\chi^2$ 检验是检测两个时间是否不相干的标准统计方法。在特征选择中，根据训练数据分析某种分类结果是否与某个具体词的出现有联系。基于$\chi^2$ 检验来选择特征，首先按词的$\chi^2$ 值大小降序排列；其次需要确定用于分类器的理想特征数目。</p>

<p>基于内容的推荐系统有许多局限：</p>

<ul>
  <li><strong>浅层内容分析</strong>。</li>
  <li>推荐结果缺乏新颖性，倾向于给出相同的推荐。</li>
  <li>冷启动问题，需要来自用户的初始评分集合。</li>
</ul>

<h2 id="section-1">协同过滤</h2>

<p>协同过滤推荐方法的主要思想：利用已有用户群过去的行为或意见预测当前用户最可能喜欢那些东西或对那些东西感兴趣。纯粹的协同方法输入只有用户-物品评分矩阵，输出可以为：</p>

<ol>
  <li>当前用户对物品的评分</li>
  <li>TopN推荐物品列表</li>
</ol>

<h3 id="section-2">基于用户的最近邻推荐</h3>

<p>这是一种早期方法，对当前用户没有见过的物品 $p$，利用近邻对物品的评价计算预测值。潜在的假设为：</p>

<ol>
  <li>如果用户过去有相似的偏好，未来也会有相似的偏好</li>
  <li>用户偏好不会随着时间而改变</li>
</ol>

<p>确定相似用户集，通常用的方法是Pearson相关系数。给定评分矩阵R，$\bar{r}_a$ 代表用户a的平均评分，用户a和用户b的相似度 $sim(a,b)$ 表示</p>

<script type="math/tex; mode=display"> sim(a,b) = \frac{\sum_{p \in P}(r_{a,p}-\bar{r}_a)(r_{b,p}-\bar{r}_b)}{\sqrt{\sum_{p \in P}(r_{a,p}-\bar{r}_a)^2}\sqrt{\sum_{p \in P}(r_{b,p}-\bar{r}_b)^2}}</script>

<p>Pearson方法考虑到用户评分标准并不相同的事实，可以发现评分值之间存在的线性相关性。但是对于广受大众欢迎的物品相似度会更高，可以类似TF-IDF，引入反用户频率(IUF)计算。用户a对物品p的预测值如下：</p>

<script type="math/tex; mode=display">pred(a,p) = \bar{r}_a + \frac{\sum_{b \in N}sim(a,b)(r_{b,p}-\bar{r}_b)}{\sum_{b \in N}sim(a,b)}</script>

<p>预测的时候可以降低近邻规模减少计算复杂度，可以将用户相似度定义一个具体的最小阈值，或者将规模大小限制为一个固定值，只考虑K个最近邻。阈值会影响可预测物品的覆盖率，但是K值不会影响，它却会带来bias-variance tradeoff。</p>

<p>对于基于用户的推荐系统，Pearson相关系数比其他方法更胜一筹，但是对于基于物品的推荐技术，余弦相似度会比Pearson相关度量表现更好。</p>

<p>基于物品的算法主要思想是利用物品相似度，而不是用户相似度。</p>

<p>在基于物品的推荐中，通常使用改进版的余弦相似度，在原有的基础上减去评分的平均值，得到的结果类似于Pearson方法，取值在-1到+1之间，公式如下：</p>

<script type="math/tex; mode=display"> sim(a,b) = \frac{\sum_{u \in U}(r_{u,a}-\bar{r}_u)(r_{u,b}-\bar{r}_u)}{\sqrt{\sum_{u \in U}(r_{u,a}-\bar{r}_u)^2}\sqrt{\sum_{u \in U}(r_{u,b}-\bar{r}_u)^2}}</script>

<p>预测公式为加权评分综合：</p>

<script type="math/tex; mode=display">pred(u,p) = \frac{\sum_{i \in ratedItems(u)}sim(i,p) \cdot r_{u,i}}{\sum_{i \in ratedItems(u)}sim(i,p)}</script>

<p>基于物品的推荐可以离线构建一个物品相似度矩阵加速线上预测。在线上，通过确定与p最相似的物品，并计算u对这些领巾物品评分的加权综合得到u对p的预测评分。</p>

<p>原则上这种方法对基于用户的推荐也适用，但是实际情况中，两个用户评分重叠情况非常少见，这就意味着一些其他的评分值可能影响到用户间的相似度。</p>

<p>在协同过滤中，会遇到数据稀疏和冷启动问题。这种挑战就是用相对较少的有效评分得到准确的预测。直接做法就是利用用户的附加信息，比如性别、年龄等帮助分类用户信息，这就涉及到利用矩阵的外部信息，也就是混合系统。</p>

<p>另外处理这些问题的方法还包括，基于图的方法，主要思想是利用假定用户品味的传递性，并由此增强额外信息矩阵；缺省投票；利用相似用户给出相似物品的评分。</p>

<p>冷启动是稀疏问题的一个特例，问题包括：如何处理新用户；如何处理为评分或购买的新物品。这两个问题都是通过混合方法解决，可以在推荐之前要求用户给出最低限度数量的评分。</p>

<p>上述的协同推荐技术是Memory-based CF Algorithm。另外一种是Model-based。包括</p>

<ul>
  <li>矩阵因子分解</li>
  <li>关联规则挖掘</li>
  <li>基于概率分析的推荐方法（预测问题看成分类问题）</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Process]]></title>
    <link href="http://billowkiller.github.io/blog/2014/05/09/random-process/"/>
    <updated>2014-05-09T16:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/05/09/random-process</id>
    <content type="html"><![CDATA[<p>本文用来记录一些随机过程。随机过程是处理包含时间以及数据序列的概率模型。可用于一下数据序列的建模：</p>

<ul>
  <li>每天的股票价格数据序列</li>
  <li>交通网络中的每个点的交通负荷数据序列</li>
  <li>雷达对飞机的定位数据序列</li>
</ul>

<p>序列中的每个数据都视为一个随机变量，所以随机序列就是一串有限或无限的随机变量序列，但是更加强调：</p>

<ul>
  <li>过程中产生的数据序列之间的<strong>相关</strong>关系。比如股票的未来价格与历史价格关系。</li>
  <li>对整个过程中<strong>长期均值</strong>感兴趣。比如有多大比例的时间机器出于闲置。</li>
  <li>需要刻画某些<strong>边界事件</strong>的似然或频率。在给定时间内，线路同时出于忙碌的概率，计算机网络中缓冲器数据溢出的频率。</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-16/62147816.jpg" width="500px" /></p>

<p>下面讨论几个随机过程。</p>

<!--more-->

<h2 id="section">伯努利过程和泊松过程</h2>

<p>这两个过程重点研究的是<strong>相邻到达时间是互相独立的随机变量</strong>的模型。区别在于：</p>

<ul>
  <li>伯努利（Bernoulli）过程考虑到达时间是<code>离散</code>的情形，相邻时间服从<code>几何分布</code>。</li>
  <li>泊松（Poisson）过程考虑到达时间是<code>连续</code>的情形，相邻时间服从<code>指数分布</code>。</li>
</ul>

<h3 id="section-1">伯努利过程</h3>

<p>通常将伯努利过程视为独立投币序列，正面朝上的概率是 $p,\ 0&lt;p&lt;1$。这里把硬币朝上视为“到达”，连续投币产生伯努利过程。下面给出伯努利过程相关的随机变量和性质：</p>

<ul>
  <li>服从参数为 $n$ 和 $p$ 的二项分布。这是 $n$ 次相继独立的实验成功的总次数 $S$ 的分布，有</li>
</ul>

<script type="math/tex; mode=display"> P_S(k) =  \binom{k}{n} p^k (1-p)^{n-k},\ k = 0,1,..n </script>

<script type="math/tex; mode=display">E(S) = np,\ var(S)=np(1-p)</script>

<ul>
  <li>服从参数为 $p$ 的集合分布。这是相互独立重复的伯努利试验首次成功时刻T的分布，有</li>
</ul>

<script type="math/tex; mode=display"> P_T(t) = p(1-p)^{t-1},\ t=1,2,...</script>

<script type="math/tex; mode=display"> E(T) = \frac{1}{p},\ var(T) = \frac{1-p}{p^2}</script>

<p>伯努利过程有<strong>独立性</strong>和<strong>无记忆性</strong>。假设我们观测过程 $n$ 步，但是还没有成功。那么余下的 $T-n$ 有什么结论呢？</p>

<script type="math/tex; mode=display"> P(T-n=t \vert T>n) = (1-p)^{t-1}p = P(T=t),\ t=1,2,...</script>

<p>可以看到未来试验次数仍然是相同的集合分布。</p>

<p>当 $n$ 充分大，而 $p$ 很小，均值 $np$ 始终。数学上，我们可以让 $n$ 增大，但是同时缩小 $p$，这样可以保值 $np$ 是一个固定值 $\lambda$，从极值意义上看，二项分布的分布列可以简化为泊松分布。</p>

<h3 id="section-2">泊松分布</h3>

<p>跟伯努利过程相比，泊松过程是连续时间轴上的到达时间。</p>

<p>伯努利过程无法刻画相同时间段内，发生两次或者多次到达事件，虽然可以通过将时间段选的非常小，但是无法选择一个准确的小时间段。这时候考虑时间段长度趋近于零的情况，即连续时间模型。</p>

<p>泊松过程相关的随机变量和性质：</p>

<ul>
  <li>服从参数为 $\lambda \tau$ 的<strong>泊松分布</strong>。这是泊松过程的强度为 $\lambda$，在时间长度为 $\tau$ 的区间内到达的总次数 $N_\tau$ 的分布，有</li>
</ul>

<script type="math/tex; mode=display"> P_{N_\tau}(k) = P(k,\tau) = e^{-\lambda \tau} \frac{(\lambda \tau)^k}{k!},\ k=0,1,...</script>

<script type="math/tex; mode=display"> E(N_\tau) = \lambda \tau,\ var(N_\tau) = \lambda \tau</script>

<ul>
  <li>服从参数为 $\lambda$ 的<strong>指数分布</strong>，这是首次达到的时间 $T$ 的分布，有</li>
</ul>

<script type="math/tex; mode=display">f_T(t) = \lambda e^{-\lambda t},\ t \ge 0,\ E(T)=\frac{1}{\lambda},\ var(T)=\frac{1}{\lambda^2}</script>

<p>上式可以将一个固定长度为 $\tau$ 的时间区间分成一个个小区间，将小区间看成伯努利过程来近似。</p>

<h3 id="section-3">马尔可夫链</h3>

<p>马尔可夫链的定义是一种随机过程，它描述了一种状态序列，其每个状态值取决于前面的状态。用以下公式表示马尔可夫性质：</p>

<script type="math/tex; mode=display">p(\theta^{(t+1)} \vert \theta^{(1)},\theta^{(2)},..\theta^{(t)}) = p(\theta^{(t+1)} \vert \theta^{(t)}) </script>

<p>状态之间的转换通过 transition kernel，这是一个 $k \times k$ 矩阵，$k$ 表示状态数。一个 $3 \times 3$ 的转换矩阵 $P$ 如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/92862152.jpg" alt="" /></p>

<p>马尔可夫链有一个平稳分布，stationary distribution $\pi$，定义为：$\pi = \pi P$。无论起始状态是什么样，马尔可夫链通常会收敛到 $\pi$ 上。</p>

<p>通常会根据所选的起始状态，有这不同的收敛时间。在利用马尔可夫链进行抽样时，在收敛之前的一段时间，比如前 $n-1$ 次迭代，各个状态的边际分布还不能认为是稳定分布，所以在进行估计的时候，应该把前面的这$n-1$ 个迭代值去掉。这个过程称为 <code>burn-in</code>。这样会使我们的抽样更加接近平稳分布，减少对起始位置的依赖。</p>

]]></content>
  </entry>
  
</feed>
