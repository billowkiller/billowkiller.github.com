<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[tags: paper，google | Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/blog/tags/paper,google/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-11-29T09:57:06+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mesa: Google's near real-time OLAP warehouse]]></title>
    <link href="http://billowkiller.github.io/blog/2016/08/06/mesa/"/>
    <updated>2016-08-06T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/08/06/mesa</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Ashish et al. VLDB 2014</p>
</blockquote>

<p>谷歌Mesa是一个服务于谷歌广告业务的近实时分析型数据仓库，主要用于广告主的报表、内部审计和预测服务。作为底层的存储系统，Mesa能处理PB级的数据，每秒百万的行更新，并且需要应对每日上亿的查询请求，因此它需要满足的设计目标是：</p>

<blockquote>
  <p>near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes.</p>
</blockquote>

<!--more-->

<p>据Google描述，随着他们的广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。为了应付如此持续增长并且重要数据的处理、存储和查询需求，Mesa的具体需求包括：</p>

<ul>
  <li>
    <p><code>原子更新</code>。某一单个的用户行为可能会引起多个关系数据级别的更新，从而影响定义在某个指标集上（例如：点击和成本）跨某个维度集（例如：广告客户和国家）的数千张一致性视图。所以系统状态不会在查询时处于一个只有部分更新生效的状态。</p>
  </li>
  <li>
    <p><code>一致性和正确性</code>。出于业务和法律的原因，该系统必须返回一致和正确的数据。即使某个查询牵涉到多个数据中心，我们仍然需要提供强一致性和可重复的查询结果。</p>
  </li>
  <li>
    <p><code>可用性</code>。系统不允许出现单点故障。不会出现由于计划中或非计划中的维护或故障所造成的停机，即使出现影响整个数据中心或地域性的断电也不能造成停机。</p>
  </li>
  <li>
    <p><code>近实时的更新吞吐率</code>。系统必须支持大约每秒几百万行规模的持续更新，包括添加新数据行和对现有数据行的增量更新。这些更新必须在几分钟内对跨不同视图和数据中心的查询可见。</p>
  </li>
  <li>
    <p><code>查询性能</code>。系统必须对那些对时间延迟敏感的用户提供支持，按照超低延迟的要求为他们提供实时的客户报表，而分批提取用户需要非常高的吞吐率。总的来说，系统必须支持将99%的点查询的延迟控制在数百毫秒之内，并且整体查询控制在每天获取万亿行的吞吐量。</p>
  </li>
  <li>
    <p><code>可伸缩性</code>。系统规模必须可以随着数据规模和查询总量的增长而伸展。举个例子，它必须支持万亿行规模和PB级的数据。但是即使上述参数再出现显著增长，更新和查询的性能必须仍然得以保持。</p>
  </li>
  <li>
    <p><code>在线的数据和元数据转换</code>。为了支持新功能的启用或对现有数据粒度的变更，客户端经常需要对数据模式进行转换或对现有数据的值进行修改。这些变更必须对正常的查询和更新操作没有干扰。</p>
  </li>
</ul>

<p>所有Google现有的大数据技术都无一能满足所有以上的需求。BigTable无法提供<code>原子性和强一致性</code>。而Megastore、Spanner和F1虽然为跨地域复制的数据提供了<code>强一致性</code>的访问，但是他们无法支持Mesa客户端所有需要的<code>峰值更新吞吐率</code>。既有的ROLAP或者MOLAP方法也不能在提供<code>近实时查询</code>同时支持分钟级别的<code>数据更新和聚合</code>。但是Mesa在其不同的基础设施中充分利用了现有的Google技术组件。它使用了BigTable来存储所有<code>持久化的元数据</code>，使用了Colossus (Google的分布式文件系统)来存储数据文件。此外，Mesa还利用了MapReduce来处理连续的数据。</p>

<p>以下有几个技术要点：</p>

<ul>
  <li>为了存储的可伸缩性和可用性，数据是水平分片，并且重复的。</li>
  <li>为了得到一致性和update时候的查询可用性，利用了多版本控制。</li>
  <li>为了更新可伸缩行，数据的更新是批量的和周期性的，并且赋值一个新的版本。</li>
  <li>为了多数据中心数据更新的一致性，Mesa利用基于Paxos的分布式同步协议。</li>
</ul>

<p>Mesa的contributions包括以下一些内容：</p>

<ul>
  <li>高吞吐量的PB级别的数据仓库，同时维持ACID属性以提供事务的处理能力</li>
  <li>新的版本管理方式，利用批量更新，得到低延迟和高吞吐量</li>
  <li>应用数据是通过一个独立和冗余的程序进行异步复制，但是关键的元数据是同步复制。这样可以减少管理副本的同步消耗，并且得到数据更新的高吞吐量</li>
  <li>在线的schema change，并不会影响现有程序的正确性和性能</li>
  <li>软件错误或硬件错误带来的数据损毁的容忍性</li>
</ul>

<h3 id="section">数据模型</h3>

<p>在Mesa的数据是多维度的，定义了细粒度的一个事实表。在这个事实表中分为两种属性，一个是 Key， 一个是 Value。Key是多维的，并且具有层级，例如 date 可以组织成 day, month, year。单个事实表在这些层级之间的聚合可以被物化，这样就支持数据分析的上卷和下钻。Value 也就是一些数据 Metrics。</p>

<p>数据是组织成表的，每个表有 schema 用于指定表结构。schema 里面包含了 Key 空间和 Value 空间，指明他们的类型和聚合方式；另外还包含 aggregation function $F: V \times V \to V$。一般来说聚合方法有 SUM、MAX、MIN和REPLACE；schema 还指明了table的一个或多个索引方法。</p>

<p>数据按对应的索引序排序分割成限定大小的文件，每个文件内部再按行分割成行组row blocks进行压缩存储，每个行组内部的数据在实际存储时，按列式存储的layout进行转换压缩，提高压缩率。而索引文件由行键取固定长度的前缀组成，映射到对应数据行在行组内部的存储偏移量。因为是一个前缀，所以索引可能不能精确定位一行的位置，而是定位这一行在行组内的偏移范围，在通过二分查找的方式在行组内部定位到具体的行。</p>

<h3 id="section-1">查询和更新</h3>

<p>Mesa中存储的数据是多版本的，更新时版本号是向前叠加的，只有处理完当前的版本才会更新下一个版本。这使得当新的更新正在处理时，Mesa可以向用户提供前置状态的<code>一致性数据</code>，也就是 update 的<code>原子性</code>。这种严格的版本顺序还可以保证数据的正确性。</p>

<p>通常，每隔几分钟，上游系统就会执行一次数据更新的批处理，结果为产生数据提交文件。独立的各个<code>无状态</code>的数据提交者实例，负责对跨（Mesa运行所在的）全部数据中心的更新操作进行协调。提交者为每个更新批处理分配一个新的版本号，并基于<code>Paxos一致算法</code>向版本数据库发布全部与该更新关联的元数据。当一个更新满足提交的条件时，意味着一个给定的更新已经被全球范围内的大量Mesa实例进行了合并，提交者会将该次更新的版本号声明为新的提交版本号，并将该值存储在版本数据库里。</p>

<p>查询通常都是根据提交版本号来分发的，因为查询通常都是根据提交版本号来分发的，所以Mesa不需要在更新和查询之间进行任何的锁操作。更新都是由Mesa实例在批处理中进行异步实施的。这些属性使得Mesa获得了非常高的查询和更新吞吐率，同时也对数据一致性提供了保障。</p>

<h3 id="section-2">版本管理</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-8-7/25645530.jpg" width="500px" /></p>

<p>为了支持查询的高效性，如上图，Mesa中的版本分为Base，Cumulative和Singleton，含义在图中也有了清晰的表达，主要的目的是为了<code>打平查询时计算的消耗，将数据聚合前置</code>。Singleton就是上文中的上游产生的数据，每个几分钟产生一个。Base，Cumulative分别是由Base Compaction和Cumulative Compaction产生。二者都会按照一定的规则进行，显然后者的频率会远高于前者。</p>

<p>singleton中因为row都是有序，所以两个singleton归并的时间是线性的。多个文件的归并可以使用最小堆算法，达到 nlogn 的时间。如何找到需要版本号的多个文件，可以使用最短路径算法。</p>

<p>归并方法与HBase的Memory Store -&gt; flush -&gt; Minor compaction -&gt; major compaction的整体流程概念也很相似。但是与HBase／Bigtable的区别在于，批量的写操作，Mesa是直接通过外部系统提交批量数据来实现，HBase则是借助于Memory Store／Flush机制来缓冲，将随机写累积成批量写，Mesa因此需要外部系统配合，不过，也减少了服务自身的复杂性。底层文件合并，目的都是为了提升检索效率，但是HBase更多的是做简单（或者说通用）的数据归并动作，将无效数据（比如Delte掉的数据，超过历史版本数量的数据）剔除，减少文件数量等，本质上不改变数据的形态，性能的收益是从检索本身的开销上获得的，而对于Mesa来说，其工作的重点是历史数据的聚合（剔除一行的动作是通过负值Value来实现），本质上是将细粒度数据转化为粗粒度数据的过程（当然，和前面说的，实际上，取决于聚合函数的定义，可能可以达到其它目的），性能的改善更多的是从数据的形态变化上获得的。</p>

<p>Mesa中Version版本的概念也与BigTable／HBase相比较，形式类似，但是整体目的用途，还是不同的。HBase系统中版本，尽管理论上可以认为是行／列以外的又一个存储维度，但实际系统的设计导向，基本就是作为区分数据历史版本使用，有不少其它系统（比如Percolator／Trafodion）借助于HBase的Cell版本功能，构建起MVCC的机制来实现例如跨行跨表原子操作，OLTP等特性。但是在Mesa中，除了构建原子操作，从查询方式（返回所有0-n版本聚合后的数据）和预聚合合并Delta文件等系统整体设计思路的角度来看，其版本的功能指向，更接近时间序列的概念，用来罗列相同Key下的可聚合的细粒度数据，当然，实际应用可能性取决于聚合函数的定义（比如假设有一个聚合函数是LastVersion，那就接近HBase的版本概念了）。</p>

<h3 id="section-3">其他工程优化</h3>

<p>delta 剪枝：类似于key range，bloomfilter等，可以快速数据块中是否含有需要的信息。</p>

<p>Scan to seek： 如果查询条件的过滤字段不是索引组合键的第一个字段，对过滤字段的左侧字段进行枚举检索，尽量减少需要进行范围检索的工作</p>

<p>Resume key：海量数据的返回是分批返回的，返回结果中附带一个Resume key用作标识当前进度，便于后续查询可以在此基础上继续进行</p>

<p>Mesa系统内部的日常数据维护工作可能涉及到大量读写工作，采用了MR作业来分布式的进行，而要保证MR作业的时间可控性，正确的进行分区，避免数据倾斜会是一个需要妥善解决的问题，Mesa采用数据采样的方式，对每个Delta文件同步存储一个采样文件，通过预读采样文件，决定MR任务分区的键值</p>

<p>对于表结构Schema的在线变更，Mesa提供两种方式，一是使用新的schema全量拷贝数据到新的版本上，二是在特定的表结构变更场景中（比如增加字段的这种表结构变更操作），在查询的时候动态判断schema版本，对返回数据进行转换，（增加字段的变更为例，Mesa会为老数据补上默认值），这个过程只需要维持一段特定的时间，因为MESA内的数据经过一段时间会进行Delta增量合并操作，在合并过程中Mesa再对涉及到的历史数据进行格式转换操作。</p>

<p>数据校验采用线上和线下两种方式。线上测试在每个update和query操作中进行，针对每个数据文件和index文件，检查checksum，并且查看row key的排序和范围。线下测试会进行全局的checksum校验，这个checksum依赖row的顺序，对于不同粒度（table，version，index）会有不同的checksum粒度。</p>

<h3 id="lessons-learned">Lessons Learned</h3>

<ul>
  <li>分布式并行化，非中心化的思想横贯其中。</li>
  <li>注意模块化和抽象化，以及分层的设计理念</li>
  <li>减少对应用层的假设，需要设计的更加通用化，对现在和未来应用的假设越少越好</li>
</ul>

]]></content>
  </entry>
  
</feed>
