<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-12-14T10:12:24+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Distributed System Design]]></title>
    <link href="http://billowkiller.github.io/blog/2016/10/27/distribute-design/"/>
    <updated>2016-10-27T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/10/27/distribute-design</id>
    <content type="html"><![CDATA[<p>分布式系统原理的脑图。</p>

<p><code>TODO:</code> 先mark，以后再添加内容。（怎么搞脑图的展现，每次更新都截图上传到图床，太麻烦了）</p>

<!--more-->

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-1/84585064.jpg" width="900px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper Roles]]></title>
    <link href="http://billowkiller.github.io/blog/2016/10/23/zk/"/>
    <updated>2016-10-23T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/10/23/zk</id>
    <content type="html"><![CDATA[<p>看到一篇不错的zookeeper应用介绍[http://blog.csdn.net/ldds<em>520/article/details/51679071](http://blog.csdn.net/ldds</em>520/article/details/51679071)，记录下。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-30/15982256.jpg" width="600px" /></p>

<!--more-->

<table cellspacing="0" cellpadding="0" style="margin:1em 0px 2em; padding:0px; width:607px; border-collapse:collapse; border-spacing:0px; color:rgb(51,51,51); font-family:'Segoe UI',Calibri,'Myriad Pro',Myriad,'Trebuchet MS',Helvetica,Arial,sans-serif; font-size:13px; border-style:solid">
<tbody>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>数据发布与订阅（配置中心）</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。</li><li style="margin:0px; padding:0px">分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。</li><li style="margin:0px; padding:0px">分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。</li><li style="margin:0px; padding:0px">系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>注意</strong>：在上面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>负载均衡</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
这里说的负载均衡是指软负载均衡。在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。<strong></strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的<a target="_blank" href="http://metaq.taobao.org/" style="color:rgb(45,138,199); text-decoration:none; outline:none">metaq</a>都是通过zookeeper来做到生产者、消费者的负载均衡。这里以metaq为例如讲下：<br />
<strong>生产者负载均衡</strong>：metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>消费负载均衡：</strong></p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。MetaQ的消费策略是：</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">每个分区针对同一个group只挂载一个消费者。</li><li style="margin:0px; padding:0px">如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。</li><li style="margin:0px; padding:0px">如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>命名服务(Naming Service)</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。<strong></strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，<a target="_blank" href="http://code.alibabatech.com/wiki/display/dubbo/Home" style="color:rgb(45,138,199); text-decoration:none; outline:none">点击这里</a>查看Dubbo开源项目。在Dubbo实现中：
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>服务提供者</strong>在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>服务消费者</strong>启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>注意</strong>，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式通知/协调</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。</li><li style="margin:0px; padding:0px">另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。</li><li style="margin:0px; padding:0px">另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>集群管理与Master选举</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题：</li></ul>
<ol style="margin:0px 0px 10px 15px; padding:0px; list-style-type:disc; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">集群中机器有变动的时候，牵连修改的东西比较多。</li><li style="margin:0px; padding:0px">有一定的延时。</li></ol>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：</p>
<ol style="margin:0px 0px 10px 15px; padding:0px; list-style-type:disc; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。</li><li style="margin:0px; padding:0px">创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。</li></ol>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">Master选举则是zookeeper中最为经典的应用场景了。</li></ul>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
另外，这种场景演化一下，就是动态Master选举。这就要用到?EPHEMERAL_SEQUENTIAL类型节点的特性了。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
上文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,?/currentMaster/{sessionId}-2 ,?/currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。</li><li style="margin:0px; padding:0px">在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。在Hbase实现中，会在ZK上存储一些ROOT表的地址和HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了HMaster的单点问题</li></ul>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式锁</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是<strong>保持独占</strong>，另一个是<strong>控制时序</strong>。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。</li><li style="margin:0px; padding:0px">控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。</li></ul>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式队列</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当
 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。</p>
<div><br />
</div>
</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paxos and Raft]]></title>
    <link href="http://billowkiller.github.io/blog/2016/09/23/paxos-raft/"/>
    <updated>2016-09-23T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/09/23/paxos-raft</id>
    <content type="html"><![CDATA[
<h2 id="distributed-consensus">1. Distributed Consensus</h2>

<p>分布式一致性是指在一组process里对一个value达成的一致。这个value可以是任何操作，例如“修改某个变量的值”，“设置某个节点为primary”等等。为什么需要分布式一致性呢，因为在分布式系统里，组件是不可靠的，无论是网络问题还是组件自身的问题，我们希望在这些不可靠组件构成的系统中得到一个可靠的系统。</p>

<p>一致性协议就是为了解决这些不可靠组件如何对外提供一致的value。具体说来可以认为多个process可以提供不同的value，一致性协议能迫使这些组件互相协作得到一个一致的结论，并且允许有少量的process出现失败或重启。</p>

<p>一致性协议可以应用的场景包括节点中状态机的复制，分布式的键值存储，分布式序号生成等等，任意节点都可以接收消息，但是对外提供的确是一致的value。</p>

<!--more-->

<p>在分布式系统中，基础复制方法有以下几种：</p>

<ul>
  <li>主从异步复制（磁盘在复制前损毁，则造成数据丢失）</li>
  <li>主从同步复制（一个失联节点会造成整个系统不可用）</li>
  <li>主从版同步复制（可能任何从库都不完整，需要多数派读写）</li>
  <li>多数派读写</li>
</ul>

<h2 id="paxos">2. Paxos</h2>

<p>Paxos算法是Lamport于1990年提出的一种基于消息传递的一致性算法，一开始并未引起人们注意，直到06年google的chubby锁服务使用paxos作为chubby cell中的一致性算法，paxos的人气从此一路狂飙。</p>

<p>Paxos有两个原则：</p>

<ul>
  <li>安全原则—保证不能做错的事
    <ul>
      <li>只能有一个值被批准，不能出现第二个值把第一个覆盖的情况</li>
      <li>每个节点只能学习到已经被批准的值，不能学习没有被批准的值</li>
    </ul>
  </li>
  <li>存活原则—只要有多数服务器存活并且彼此间可以通信最终都要做到的事
    <ul>
      <li>最终会批准某个被提议的值</li>
      <li>一个值被批准了，其他服务器最终会学习到这个值 </li>
    </ul>
  </li>
</ul>

<p>Paxos有两个角色：</p>

<ul>
  <li>Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。</li>
  <li>Acceptor：提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值。</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-20/80935469.jpg" width="700px" /></p>

<h3 id="paxos-1">2.1 Paxos过程</h3>

<p>下面介绍Paxos协议过程，以及解决的一些问题。</p>

<p><strong>1、第一阶段 Prepare</strong></p>

<p><strong>P1a：Proposer 发送 Prepare</strong></p>

<p>Proposer 生成全局唯一且递增的提案 ID（Proposalid，以高位时间戳 + 低位机器 IP 可以保证唯一性和递增性），向 Paxos 集群的所有机器发送 PrepareRequest，这里无需携带提案内容，只携带 Proposalid 即可。</p>

<p><strong>P1b：Acceptor 应答 Prepare</strong></p>

<p>Acceptor 收到 PrepareRequest 后，做出“两个承诺，一个应答”。</p>

<p>两个承诺：</p>

<ul>
  <li>第一，不再应答 Proposalid <code>小于等于</code>（注意：这里是 &lt;= ）当前请求的 PrepareRequest；</li>
  <li>第二，不再应答 Proposalid <code>小于</code>（注意：这里是 &lt; ）当前请求的 AcceptRequest</li>
</ul>

<p>一个应答：</p>

<ul>
  <li>返回自己已经 Accept 过的提案中 ProposalID 最大的那个提案的内容，如果没有则返回空值;</li>
</ul>

<p><strong>注意：这“两个承诺”中，蕴含两个要点：</strong></p>

<ul>
  <li>就是应答当前请求前，也要按照“两个承诺”检查是否会违背之前处理 PrepareRequest 时做出的承诺；</li>
  <li>应答前要在本地持久化当前 Propsalid。</li>
</ul>

<p><strong>2、第二阶段 Accept</strong></p>

<p><strong>P2a：Proposer 发送 Accept</strong></p>

<p>“提案生成规则”：Proposer 收集到多数派应答的 PrepareResponse 后，从中选择proposalid最大的提案内容，作为要发起 Accept 的提案，如果这个提案为空值，则可以自己随意决定提案内容。然后携带上当前 Proposalid，向 Paxos 集群的所有机器发送 AccpetRequest。</p>

<p><strong>P2b：Acceptor 应答 Accept</strong></p>

<p>Accpetor 收到 AccpetRequest 后，检查不违背自己之前作出的“两个承诺”情况下，持久化当前 Proposalid 和提案内容。最后 Proposer 收集到多数派应答的 AcceptResponse 后，形成决议。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-20/9823122.jpg" width="600px" /></p>

<h3 id="section">2.2 核心思想</h3>

<ul>
  <li>Optimistic concurrency control. Hold a <code>preemptible lock</code> first, try updating, restart on denial.</li>
  <li>Quorum as a logical unit of acceptor for Choose operation. A value is chosen iff it’s accepted by a quorum, which implies from the proposer’s perspective the Choose operation is <code>atomic</code>, it’s all or nothing, it’s either accepted by a quorum or it isn’t.</li>
</ul>

<h3 id="section-1">2.3 协议推导</h3>

<p>Paxos 协议利用了 Quorum 机制，选择的 W=R=N/2+1。简单而言，协议就是 Proposer 更新 Acceptor 的过程，一旦某个 Proposer 成功更新了超过半数的 Acceptor，则更新成功。Learner 按 Quorum 去读取 Acceptor，一旦某个 value在超过半数的 Acceptor 上被成功读取，则说明这是一个被批准的 value。协议通过引入轮次，使得高轮次的议抢占低轮次的提议来避免死锁。</p>

<p>协议的关键点是如何满足“在一次Paxos算法实例过程中只批准一个Value”，称这个约束为“约束条件 1”，推到过程就是不断推到出“约束条件 1”的充分不必要条件。</p>

<ul>
  <li>“约束条件 2” =&gt; “约束条件 1”：一旦一个 value 获得超过半数的 Acceptor 批准,之后 Paxos 协议实例只能批准这个 value。</li>
  <li>“约束条件 3” =&gt; “约束条件 2”：一旦一个 value 获得超过半数的 Acceptor 批准,之后任何 Acceptor 只能批准这个 value。</li>
  <li>“约束条件 4” &lt;=&gt; “约束条件 3”: 一旦一个 value v 获得超过半数的 Acceptor 批准,之后 Proposer  议的 value 只能是 v。</li>
  <li>“约束条件 5” =&gt; “约束条件 4”：Proposer 提议一个 value v 前,要么之前没有任何一个 value 被批准,要么存在一个大小为 N/2+1 的 Acceptor 集合,这个集合内的各个 Acceptor 批准过的轮数最大的 value 是 v。</li>
</ul>

<p>可以用反证法证明“约束条件 5”.</p>

<h3 id="section-2">2.4 问题</h3>

<ol>
  <li>
    <p>这里可以看到paxos使用了多acceptor，为什么？</p>

    <p>为了解决Acceptor crash的问题，必须要用到一种多数选择的方法。</p>
  </li>
  <li>
    <p>如何保证批准的提案无法改变？</p>

    <p>假设Proposer以更高的序号发提案，但已经批准的提案必然被一半以上的Acceptor接受，那么Acceptor返回应答必然包含这个已经批准的提案，所以此时Proposer发起的Accept消息必然是（高序号，已经批准的提案）这么一个组合。所以一旦一个提案被批准，以后永远只能批准这个提案。</p>
  </li>
  <li>
    <p>paxos中是否存在死锁和活锁？</p>

    <p>虽然paxos协议过程类似于”占坑“，需要value抢占超过半数的”坑“，但是高轮数的提案可以抢占低轮数的提案，所以可以避免死锁的发生。但是这种设计可能导致”活锁“，即Proposer互相不断以更高的轮数提出提案，使得每轮paxos过程都无法完成。一种解决方案是Proposer重新提案之前等待随机时间，让上一个提案有时间进行第二阶段的accept。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/70209540.jpg" width="500px" /></p>
  </li>
  <li>
    <p>如果某个提案之后又另外一个提案跟进会发生什么情况？</p>

    <p>如果前一个提案已经被多数派接受，那新的提案会看到和使用前一个提案作为它的value；如果前一个提案没有被多数派接受，但新提案发现这个提案，那新提案也会使用它作为value；如果前一个提案没有被多数派接受，并且新提案没有发现，则新提案会使用自己的value，旧的提案不会被block。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/38064842.jpg" width="500px" /></p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/41211772.jpg" width="500px" /></p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/4176188.jpg" width="500px" /></p>
  </li>
  <li>
    <p>server如何知道提案的value？</p>

    <p>只有Proposer自己知道自己选择的提案，别的server如果想要知道必须发起一个paxos instance。</p>
  </li>
  <li>
    <p>在 P2a 阶段，为什么Proposer发起的提案需要使用旧的value和更高的proposalID？</p>

    <p>是受“在一次paxos instance中只批准一个value”的约束。即使读取了N/2+1的acceptor状态，由于没有读取所有acceptor，Proposer也无法判断value是否批准了。这时候选择proposalID更高的旧value，可以保证，要么此时Paxos还没有批准任何一个value，要么只能是旧的value。</p>
  </li>
  <li>
    <p>如果提案被多数派接受后，一个acceptor宕机了，导致多数派不成立，会有什么问题？ </p>

    <p>不会有问题，因为新的Proposer选择的quorum一定包括接受上一次提案的那台机器(接受的机器数至少是N/2)，这时候会使用更高的proposalID提交旧的value。</p>
  </li>
</ol>

<h3 id="zab">2.5 Zab</h3>

<p>Zookeeper 使用一种修改后的 Paxos 协议，称为 Zab。</p>

<p>在 zookeeper 中，始终分为两种场景：</p>

<ul>
  <li>Leader activation：leader 选举，数据同步</li>
  <li>Active messaging：leader 接收 client 更新请求，同步到各个 follower</li>
</ul>

<p>在两种场景中，zk都依赖于一个全局版本号：zxid。zxid 由 (epoch, count) 组成，<code>epoch</code> 是选举编号，每次提议进行leader选举时 epoch 都会增加，<code>count</code> 是 leader 为每个更新操作决定给的序号。从全局看，<strong>一个 zxid 代表了一个更新操作的全局序号（版本号）</strong>。</p>

<p>每个 zookeeper 节点都有各自最后 commit 的 zxid，表示这个 zookeeper 节点上最近成功执行的更新操作，也代表了这个节点的数据版本。在 Leader activation 阶段，<strong>每个 zk 节点都以自己的 zxid 作为 proposalID 发起 paxos instance，设置自己为leader (value)</strong>。每个节点既是 Proposer 也是 Acceptor。通过 Paxos 协议，某个超过 quorum 半数的节点中持有最大的 zxid 节点会成为新的leader。实际上 proposalID 会是 (zxid，nodeid)，这是当 zxid 相同时，zk 会选择节点编号较大的成为 leader。成为新的 leader 需要与 follower 进行同步，数据同步过程可能会涉及删除 follower 上的最后一条脏数据。</p>

<p>当与至少半数节点完成数据同步后，leader 更新 epoch，在各个 follower 上以 (epoch + 1, 0) 为 zxid 写一条没有数据的更新操作。这个更新操作称为 <code>NEW_LEADER</code> 消息，是为了在各个节点上更新 leader 信息，当收到超过半数的 follower 对 NEW_LEADER 的确认后，leader 发起对 NEW_LEADER 的 COMMIT 操作，并进入 Active messaging 阶段。</p>

<p>进入 active messaging 状态的 leader 会接收从客户端发来的更新操作，为每个更新操作生成递增的 count，组成递增的 zxid。Leader 将更新操作以 zxid 的顺序发送给各个 follower (包括leader本身, 一个 leader 同时也是 follower，当收到超过半数的 follower 的确认后，Leader 发送针对该更新操作的 COMMIT 消息给各个 follower。这个更新操作的过程很类似<code>两阶段提交</code>，只是 leader 永远不会对更新操作做 abort 操作。</p>

<p>如果 leader 不能更新超过半数的 follower，此时可以发起新的 leader 选举。最后一条更新操作处于“中间状态”，其是否生效取决于选举出的新 leader 是否有该条更新操作。</p>

<p>Zookeeper 通过 zxid 将两个场景阶段较好的结合起来，且能保证全局的强一致性。</p>

<ul>
  <li>由于同一时刻只有一个 zookeeper 节点能获得超过半数的 follower，所以同一时刻最多只存在唯一的 leader；</li>
  <li>每个 leader 利用 TCP 带来的消息 FIFO 特点以 zxid 顺序更新各个 follower，只有成功完成前一个更新操作的才会进行下一个更新操作；</li>
  <li>在同一个 leader 任期内，数据在全局满足 quorum 约束的强一致，即读超过半数的节点一定可以读到最新已提交的数据；</li>
  <li>每个成功的更新操作都至少被超过半数的节点确认，使得新选举的 leader 一定可以包括最新的已成功提交的数据。</li>
</ul>

<h2 id="raft">4. Raft</h2>

<p>Paxos 相比 Raft 比较复杂和难以理解。角色扮演和流程比 Raft 都要啰嗦。比如 Agreement 这个流程，在 Paxos 里边：Client 发起请求举荐 Proposer 成为 Leader，Proposer 然后向全局 Acceptors 寻求确认，Acceptors 全部同意 Proposer 后，Proposer 的 Leader 地位得已承认，Acceptors 还得再向Learners 进行全局广播来同步。鉴于 paxos 的难以理解，standford 开发另外一个一致性算法 Raft。</p>

<p>一般来说一致性算法有两种：</p>

<ul>
  <li>节点同质，leader-less</li>
  <li>节点异构，leader-based</li>
</ul>

<p>容易理解上面的这两种方法，Paxos 是节点同质的，Raft 采用节点异构，那对比与同质的有什么好处？</p>

<ul>
  <li>问题得到分解（normal operation、leader change），可以类比 zab。</li>
  <li>normal operation 得到简化，没有冲突</li>
  <li>比 leader-less 来的更高效，相当于一次 prepare 阶段可以有多次 accept 阶段。</li>
</ul>

<p>那下面就来说说 Raft 是怎么做的，为什么会比 paxos 容易理解。</p>

<h3 id="server-states">4.1 Server states</h3>

<p>Server 被分为 Leader、Follower、Candidate。这个类似于分布式副本管理中的 primary 和 secondary。Leader 负责与 client 交互并发送同步 RPC 命令给 Follower；Follower 是完全被动的接收和反馈 RPC 命令；Candidate 用来选举 Leader。状态图如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/40396396.jpg" width="600px" /></p>

<p>Raft 中 Follower 长时间没有接受到心跳就会转为 Candidate 状态，收到多数投票应答之后可以转为 Leader，Leader 会定期向其他节点发送心跳。当 Leader 和 Candidate 接收到更高版本的消息后，转为 Follower。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/64890697.jpg" width="700px" /></p>

<h3 id="leader-election">4.2 Leader Election</h3>

<p>Raft 中时间被分为 Terms 这么一个概念，在 Terms 中完成 Leader Election 和 Normal operation。每个 term 最多只有一个 leader，term 允许选举失败，导致 term 没有 leader（未达到多数投票应答而超时）。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/22131457.jpg" width="600px" /></p>

<p>Raft 的选主过程中，每个 Candidate 节点先将本地的 Current Term 加一，然后向其他节点发送RequestVote 请求，其他节点根据本地数据版本、长度和之前选主的结果判断应答成功与否。具体处理规则如下：</p>

<ol>
  <li>如果 now – lastLeaderUpdateTimestamp &lt; elect_timeout，忽略请求</li>
  <li>如果 req.term &lt; currentTerm，忽略请求。</li>
  <li>如果 req.term &gt; currentTerm，设置 req.term 到 currentTerm 中，如果是 Leader 和Candidate 转为 Follower。</li>
  <li>如果 req.term == currentTerm，并且本地 voteFor 记录为空或者是与 vote 请求中 term 和CandidateId 一致，req.lastLogIndex  &gt; lastLogIndex，即 Candidate 数据新于本地则同意选主请求。</li>
  <li>如果 req.term == currentTerm，如果本地 voteFor 记录非空或者是与 vote 请求中 term 一致 CandidateId 不一致，则拒绝选主请求。</li>
  <li>如果 lastLogTerm &gt; req.lastLogTerm，本地最后一条 Log 的 Term 大于请求中的 lastLogTerm，说明 candidate 上数据比本地旧，拒绝选主请求
 上面的选主请求处理，符合Paxos的“少数服从多数，后者认同前者”的原则。按照上面的规则，选举出来的 Leader，一定是多数节点中 Log 数据最新的节点。</li>
</ol>

<p>下面来分析一下<strong>选主的时间和活锁问题</strong>。</p>

<p>设定 Follower 检测 Leader Lease 超时为 HeartbeatTimeout，Leader 定期发送心跳的时间间隔将小于 HeartbeatTimeout，避免 Leader Lease 超时，通常设置为小于 HeartbeatTimeout/2。当选举出现冲突，即存在两个或多个节点同时进行选主，且都没有拿到多数节点的应答，就需要重新进行选举，这就是常见的选主活锁问题。类似于 Paxos，Raft 中引入随机超时时间机制，有效规避活锁问题。</p>

<p>注意上面的 Log 新旧的比较，是基于 <code>lastLogTerm</code> 和 <code>lastLogIndex</code> 进行比较，而不是基于 <code>currentTerm</code> 和 <code>lastLogIndex</code> 进行比较。<strong>currentTerm 只是用于忽略老的Term的vote请求，或者提升自己的currentTerm，并不参与Log新旧的决策</strong>。</p>

<p>考虑一个非对称网络划分的节点，在一段时间内会不断的进行vote，并增加currentTerm，这样会导致网络恢复之后，Leader会接收到AppendEntriesResponse中的term比currentTerm大，Leader就会重置currentTerm并进行StepDown，这样Leader就对齐自己的Term到划分节点的Term，重新开始选主，最终会在上一次多数集合中选举出一个term&gt;=划分节点Term的Leader。</p>

<p>Raft中收到任何term高于currentTerm的请求都会进行StepDown，前提是必须经过前期必要的检查，就像VoteRequest处理中时间戳的检查。</p>

<h3 id="log-recovery">4.3 Log Recovery</h3>

<p>Log Recovery 就是要保证一定已经 Committed 的数据不会丢失，未 Committed 的数据转变为 Committed，但不会因为修复过程中断又重启而影响节点之间一致性。</p>

<blockquote>
  <p>Committed的定义: 持久的，最终会被状态机执行的Log Entry。</p>
</blockquote>

<p>Log Recovery 这里分为 <code>Leader-Alive</code> 和 <code>Leader-Change</code>。</p>

<ul>
  <li>Leader-Alive 修复主要是解决某些 Follower 节点重启加入集群，或者是新增 Follower节点加入集群。
    <ul>
      <li>Leader需要向Follower节点传输漏掉的Log Entry，如果Follower需要的Log Entry已经在Leader上Log Compaction清除掉了，Leader需要将上一个Snapshot和其后的Log Entry传输给Follower节点。</li>
      <li>Leader-Alive模式下，只要Leader将某一条Log Entry复制到多数节点上，Log Entry就转变为Committed。</li>
    </ul>
  </li>
  <li>Leader-Change修复主要是在保证Leader切换前后数据的一致性。
    <ul>
      <li>通过上面Raft的选主可以看出，每次选举出来的Leader一定包含已经committed的数据（<strong>抽屉原理</strong>：选举出来的Leader是多数中数据最新的，一定包含已经在多数节点上commit的数据），新的Leader将会覆盖其他节点上不一致的数据。</li>
      <li>虽然新选举出来的Leader一定包括上一个Term的Leader已经Committed的Log Entry，但是<em>可能也包含上一个Term的Leader未Committed的Log Entry</em>。这部分Log Entry需要转变为Committed，相对比较麻烦，需要考虑Leader多次切换且未完成Log Recovery。解决方案如下：</li>
    </ul>

    <p>Raft中增加了一个约束：<strong>对于之前Term的未Committed数据，修复到多数节点，且在新的Term下至少有一条新的Log Entry被复制或修复到多数节点之后，才能认为之前未Committed的Log Entry转为Committed</strong>。下图就是一个Leader-Change场景下的Log Recovery过程：</p>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/63638971.jpg" width="600px" /></p>

<ol>
  <li>S1是Term2的Leader，将LogEntry部分复制到S1和S2的2号位置，然后Crash。</li>
  <li>S5被S3、S4和S5选为Term3的Leader，并只写入一条LogEntry到本地，然后Crash。</li>
  <li>S1被S1、S2和S3选为Term4的Leader，并将2号位置的数据修复到S3，达到多数；并在本地写入一条Log Entry，然后Crash。</li>
  <li>
    <p>这个时候2号位置的Log Entry虽然已经被复制到多数节点上，但是并不是Committed。这时候可能会产生两种情况：</p>

    <ul>
      <li>S5被S3、S4和S5选为Term5的Leader，将本地2号位置Term3写入的数据复制到其他节点，覆盖S1、S2、S3上Term2写入的数据</li>
      <li>S1被S1、S2、S3选为Term5的Leader，将3号位置Term4写入的数据复制到S2、S3，使得2号位置Term2写入的数据变为Committed</li>
    </ul>
  </li>
</ol>

<p>选出Leader之后，Leader运行过程中会进行副本的修复，这个时候只要多数副本数据完整就可以正常工作。Leader为每个Follower维护一个nextId，标示下一个要发送的logIndex。过程如图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/9644050.jpg" width="600px" /></p>

<p>上图中Follower a与Leader数据都是一致的，只是有数据缺失，直接通知Leader从logIndex=5开始进行重传，只需一次回溯。Follower b与Leader有不一致性的数据，需要回溯7次才能找到需要进行重传的位置.</p>

<h3 id="membership-management">4.4	Membership Management</h3>

<p>分布式系统运行过程中节点总是会存在故障报修，需要支持节点的动态增删。节点增删过程不能影响当前数据的复制，并能够自动对新节点进行数据修复，如果删除节点涉及Leader，还需要触发<em>自动选主</em>。直接增加节点可能会导致出现新老节点结合出现两个多数集合，造成冲突。</p>

<p>下图是3个节点的集群扩展到5个节点的集群，直接扩展可能会造成Server1和Server2构成老的多数集合，Server3、Server4和Server5构成新的多数集合，两者不相交从而可能导致决议冲突。
 <img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-12-7/36189950.jpg" width="600px" /></p>

<p> Raft采用的策略相对简单一些，每次只增删一个节点，这样就不会出现两个多数集合，不会造成决议冲突的情况。按照如下规则进行处理：</p>

<ol>
  <li>Leader收到AddPeer/RemovePeer的时候就进行处理，而不是等到committed，这样马上就可以使用新的peer set进行复制AddPeer/RemovePeer请求。 </li>
  <li>Leader启动的时候就发送AddPeer请求，防止上一轮AddPeer没有完成commit。 </li>
  <li>Leader在删除自身节点的时候，会在RemovePeer被Committed之后，进行关闭。</li>
</ol>

<p>因为节点动态调整跟Leader选举是两个并行的过程，可以实现安全的动态节点增删。另外节点需要一些宽松的检查来保证选主和AppendEntries的多数集合：</p>

<ul>
  <li>节点可以接受不是来自于自己Leader的AppendEntries请求</li>
  <li>节点可以为不属于自己节点列表中的Candidate投票</li>
</ul>

<p>为了避免同时有两个节点变更正在进行，在有未committed的change正在进行的时候，不允许进行节点变更。节点变更有一个问题，对一个只有两个节点的Cluster，发起RemovePeer。这个时候一个节点挂掉，另外一个节点没有收到RemovePeer请求，这样系统将停止工作。因此强烈建议集群节点数&gt;=3个。</p>

<h2 id="multi-paxos">3. Multi Paxos</h2>

<p>终于到了最后的 <code>Multi Paxos</code>，这篇文章断断续续写了也快3个月了。在实践中验证的分布式一致性算法包括，Paxos、ZAB、Multi Paxos，其实还有 <code>Viewstamped</code>，不过再这篇文章中将不会再补充，因为用的实在是太少了。Multi Paxos也一样，还没有有影响力的开源实现，据我说知，阿里的 OceanBase 和 Google 的不少产品都有用到。下图是一个总结，可以看到开源的软件大多数都使用 Zookeeper 作为满足一致性要求。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-12-13/96999766-file_1481634011735_c025.png" width="600px" /></p>

<p>下面就介绍 Multi Paxos。我们向来分析下 Paxos 的缺点。</p>

<ul>
  <li>Paxos 中每选中一个 value，都需要经过两轮 RPC (prepare, accept)。</li>
  <li>当并发的 Proposer 数量增加的时候，冲突和重启数也会增加。</li>
</ul>

<p>那么参照 Raft，我们可以用 leader-based 的方法来解决。这里的 multi 也就代表一次 prepare 可以进行多次 accept，从而提高性能。</p>

<h2 id="discussion">5. Discussion</h2>

<h2 id="ref">Ref</h2>

<p><a href="https://docs.google.com/presentation/d/1y2lbLzSmZdd3OVzXpxAvmvWt4Hhn_WljQiU4x30Cst4/edit?usp=sharing">A Beginner’s Guide to Paxos</a></p>

<p><a href="https://ramcloud.stanford.edu/~ongaro/userstudy/">Raft user sutdy</a></p>

<p><a href="https://raft.github.io/">The Raft Consensus</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hbase Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/20/hbase/"/>
    <updated>2016-07-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/20/hbase</id>
    <content type="html"><![CDATA[<p>犯懒了，留两篇好文章，基本的东西也都在这文章里面。得空再补补吧。</p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426877.html">深入HBase架构解析（一）</a></p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426950.html">深入HBase架构解析（二）</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/04/06/kafka/"/>
    <updated>2016-04-06T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/04/06/kafka</id>
    <content type="html"><![CDATA[<p>Kafka最早是由LinkedIn开发的一个分布式发布-订阅消息系统，现在已经是Apache的一个开源项目。它具有以下的一些特点：</p>

<ul>
  <li>作为分布式系统，很容易 scale out</li>
  <li>消息以时间复杂度为O(1)的方式持久化到磁盘，支持离线消费和实时消费</li>
  <li>发布和订阅都支持高吞吐量，单机支持每秒100K条以上消息的传输</li>
  <li>支持多个订阅端，并且可以在异常情况下自动对这些消费者进行负载均衡</li>
</ul>

<p>消息系统的好处包括：</p>

<ul>
  <li>解耦生产者和消费者</li>
  <li>持久化直到消息已经被完全处理</li>
  <li>扩展性</li>
  <li>灵活性 &amp; 峰值处理能力</li>
  <li>可恢复性，系统的一部分组件失效时，不会影响到整个系统。</li>
  <li>顺序保证</li>
  <li>缓冲，有助于控制和优化数据流经过系统的速度。</li>
  <li>异步通信</li>
</ul>

<!--more-->

<h2 id="section">名词解释</h2>

<ul>
  <li>
    <p>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</p>
  </li>
  <li>
    <p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>
  </li>
  <li>
    <p>Partition：Parition是物理上的概念，每个Topic包含一个或多个Partition.</p>
  </li>
  <li>
    <p>Producer：负责发布消息到Kafka broker</p>
  </li>
  <li>
    <p>Consumer：消息消费者，向Kafka broker读取消息的客户端。</p>
  </li>
  <li>
    <p>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>
  </li>
</ul>

<h2 id="section-1">架构图</h2>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310020.png" width="500px" /></p>

<p>如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310025.png" width="500px" /></p>

<p>上图示意消费者和生产者的工作方式。同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>

<p><img src="http://sookocheff.com/img/kafka/kafka-in-a-nutshell/log-anatomy.png" width="500px" /></p>

<p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。每条消息都被append到某个Partition中，具体存储到哪一个Partition是根据Partition机制。如果Partition机制比较合理，不同的消息可以并行写入不同broker的不同Partition里，能极大的提高了吞吐率。另因为磁盘限制，Kafka提供两种策略删除旧数据：一是基于时间，二是基于Partition文件大小。</p>

<h3 id="kafka-delivery-guarantee">Kafka delivery guarantee</h3>

<p>Producer向broker发送消息时，默认情况下一条消息从Producer到broker是确保了 <code>At least once</code>，但如果设置Producer为异步发送则实现 <code>At most once</code>。<code>Exactly once</code>还未实现（生成一种消息主键，幂等重试）。</p>

<p>Consumer在从broker读取消息后，可以选择<code>autocommit</code>或<code>手动commit</code>。区别在于一个是读完消息先commit再处理消息，一个是读完消息先处理再commit。前者实现 <code>At most once</code>，后者实现 <code>At least once</code>，但如果消息的处理有幂等性，则可以理解为<code>Exactly once</code>。如果要做到严格<code>Exactly once</code>，则让offset和操作输入存在同一个地方，<strong>保证数据的输出和offset的更新要么都完成，要么都不完成</strong>，参考Spark Kafka的<code>DirectAPI</code>的实现以及<code>Druid</code>。</p>

<p>Kafka允许的<code>Exactly once</code>语义其实和它的特性有很大关系：
* 每条消息有序和不可变的放在partition中，同时分配一个递增的offset。这样通过&lt;partition, offset&gt;就可以确定一条消息，并且它的前辈和后辈都是不变的。
* 消费者拉数据，从而由消费者自身进行流控。
* 消费者可以根据&lt;partition, offset&gt;寻找消息，允许message rewind，并且根据消息的metadata可以保证消息接收的不重不丢。</p>

<h2 id="high-available">High Available</h2>

<p>Kafka的HA包括Data Replication和Leader Election两方面。</p>

<h3 id="data-replication">Data Replication</h3>

<p><img src="http://cdn4.infoqstatic.com/statics_s2_20160405-0343u1/resource/articles/kafka-analysis-part-2/zh/resources/0416000.png" width="500px" /></p>

<p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。Kafka分配Replica的算法如下：</p>

<ol>
  <li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>
  <li>将第i个Partition分配到第（i mod n）个Broker上</li>
  <li>将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</li>
</ol>

<p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR(in-sync replica)中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW(high watermark)并且向Producer发送ACK。HW会从leader持续发送到follower并被保存到每个broker的磁盘中。</p>

<p>对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p>

<p>Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。</p>

<h3 id="leader-election">Leader Election</h3>

<p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。对比Majority Vote则需要2f+1个Replica。</p>

<p>Kafka中，如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除，包括两种情况：长时间未向leader发送fetch request，消息lag超过阈值。
为了防止ISR里面的慢节点，Producer选择是否被commit阻塞。</p>

<p>选举时候，Kafka会在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。这种方式改善每个follower都使用zk watch的方法进行选举的问题：</p>

<ul>
  <li>brain split</li>
  <li>herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</li>
  <li>ZooKeeper负载过重 </li>
</ul>

<h2 id="kafka-">Kafka 网络模型</h2>

<p>Kafka使用的网络模型是典型的reactor模式，一个acceptor处理新来的连接请求，分配给N个processor处理，每个processor都有selector从socket中读取数据，生成request对象放到requestChannel中。requestChannel包含一个requestQueue和一个responseQueues，requestQueue是一个blocking queue，它的大小为<code>queued.max.requests</code>；responseQueues 包含N个blocking queue，对应每个processor。Kafka会有M个Handler threads用于处理responseQueues中的request，并且生成response放到对应的response队列中，处理过程如下：KafkaRequestHandler循环从RequestChannel中取Request并交给kafka.server.KafkaApis处理具体的业务逻辑。。</p>

<p>这里面涉及的数目在配置文件中都有体现，上述的每个acceptor包括processor在Kafka中称为NIO socket server，数量在<code>listeners</code>中定义，例如<code>PLAINTEXT://myhost:9092, SSL://:9091 </code>。N 取值 <code>background.threads</code>，M 取值 <code>num.io.threads</code>。</p>

<p>在 NIO socket server 中会给每个processor的responseQueue都注册一个ResponseListener，一旦有Response产生就会通知对应的processor发送Response到客户端。</p>

<h2 id="kafka-clients-operations">Kafka Clients’ Operations</h2>

<p>这一章节介绍一个Kafka client对于Kafka Resources可能的操作类型。</p>

<p>Operation包括以下几种：Read, Write, Create, Delete, Alter, Describe, ClusterAction。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/80246194.jpg" width="450px" /></p>

<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-4+-+Command+line+and+centralized+administrative+operations">KIP-4</a></p>

<h3 id="createdelete">Create/Delete</h3>

<p>Create/Delete就是创建和删除Topics。</p>

<p>具体过程如下：</p>

<ol>
  <li>broker发送<code>TopicMetadataRequest</code>到controller。</li>
  <li>Controller在ZooKeeper的/brokers/topics节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。</li>
  <li>对于删除Topic操作，Topic工具会将该Topic名字存于/admin/delete_topics。若delete.topic.enable为true，则Controller注册在/admin/delete_topics上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest，若为false则Controller不会在/admin/delete_topics上注册Watch，也就不会对该事件作出反应。</li>
  <li>对于创建Topic操作，Controller从/brokers/ids读取当前所有可用的Broker列表，对于set_p中的每一个Partition：
    <ul>
      <li>从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）</li>
      <li>将新的Leader和ISR写入/brokers/topics/[topic]/partitions/[partition]</li>
    </ul>
  </li>
  <li>直接通过RPC向相关的Broker发送LeaderAndISRRequest。</li>
</ol>

<p>创建Topic顺序图如下所示。</p>

<p><img src="http://cdn4.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-3/zh/resources/0606003.png" width="500px" /></p>

<p>有两点说明：</p>

<ul>
  <li>对于<code>auto.create.topics.enable=false</code>的Kafka，如果对未存在的topic进行produce，则会导致producer <code>org.apache.kafka.common.errors.TimeoutException</code>错误。</li>
  <li>除了使用broker进行创建Topic，还可以通过Kafka的AdminUtils直接指定zk、topic、partitions，replicationFactor，把相关信息写入zk来创建Topic。</li>
</ul>

<h3 id="alterdescribe">Alter/Describe</h3>

<p>alter/describe 是对配置修改或查看的操作，包括Topic和Client两个部分。改变方法也是通过Kafka的AdminUtils和ConfigCommand，指定zk、topic或要修改的properties。</p>

<h3 id="clusteraction">ClusterAction</h3>

<p>包括LeaderAndIsrRequest，StopReplicaRequest，UpdateMetadataRequest，ControlledShutdownRequest</p>

<h3 id="read">Read</h3>

<p>consumer 订阅Topic数据。consumer根据consumer group的分配算法如下</p>

<pre><code>rebalance process for consumer C_i in group G
For each topic T that C_i subscribes to {
 remove partitions owned by C_i from the ownership registry
 read the broker and the consumer registries from Zookeeper
 compute P_T = partitions available in all brokers under topic T
 compute C_T = all consumers in G that subscribe to topic T
 sort P_T and C_T
 let j be the index position of C_i in C_T and let N = |P_T|/|C_T|
 assign partitions from j*N to (j+1)*N - 1 in P_T to consumer C_i
 for each assigned partition p {
 set the owner of p to C_i in the ownership registry
 let O_p = the offset of partition p stored in the offset registry
 invoke a thread to pull data in partition p from offset O_p
 }
}
</code></pre>

<p>当订阅topic的时候，kafka会注册一个<code>ConsumerRebalanceListener</code>，当发生以下任何一个事件的时候，会引发上述的rebalance算法：</p>

<ul>
  <li>Number of partitions change for any of the subscribed list of topics</li>
  <li>Topic is created or deleted</li>
  <li>An existing member of the consumer group dies</li>
  <li>A new member is added to an existing consumer group via the join API</li>
</ul>

<p>Kafka的Consumer API可以透明地处理上述情况，另外还包括server的fail，partition的聚合。</p>

<p>读取数据发生在<code>poll</code>函数中，每次调用时，consumer都会使用上次消费的offset作为这次的起始offset，并且顺序的读取记录。当然上次消费的offset也可以由<code>seek</code>函数直接指定。</p>

<p>读取数据在Kafka内部是一个FetchRequest，处理的过程为
* authorize
* FetchResponse
* recordAndMaybeThrottle(quota控制)</p>

<h3 id="write">Write</h3>

<p>producer 发布数据的动作。produce的api接口可以选择同步或异步，默认情况下<code>send</code>接口是异步的，它将数据放到缓冲区后就立即返回，等到数据到达一定带下后再发送出去，节约IO开销。可以直接在send()后调用get()，这样可以达到同步的效果。另外一个重要的参数是acks，用来设置produce 请求完成的标准，也就是数据要写到几个broker中才算是完成。</p>

<p>发布数据在Kafka内部是一个ProducerRequest，处理过程为：</p>

<ul>
  <li>authorize</li>
  <li>produceResponse</li>
  <li>recordAndMaybeThrottle(quota控制)</li>
</ul>

<h3 id="partition-and-key">Partition and Key</h3>

<p>这里说下partition和key的关系。注意到，在kafka的api中会有一个key的概念，而实际上kafka只是一个消息的订阅和发布系统，和key应该扯不上一点关系，那么这个key有什么用呢，不用key会有什么影响。</p>

<p>看下没有指定key或者key为null的时候kafka是怎么处理的。首先kafka会随机选择一个partition，然后在一个默认的时间（10min）内所有的数据都会写到这个partition内。这会造成数据不均衡的分布在各个partition中。这时可以通过减少metadata refresh interval 缩短这个默认时间来减轻数据不均衡的现象。</p>

<p>不过更实际的还是指定一个key，因为kafka默认的是使用hashing-based partitioner，可能还会造成数据不均衡。这时候就需要使用自定义的partition，并指定<code>partitioner.class</code>。</p>

<h2 id="authorize">Authorize</h2>

<h3 id="authorizer">Authorizer接口</h3>

<p>Kafka带有Authorizer接口，这个是所有实现授权的插件都必须要实现的接口。启动的时候会读<code>authorizer.class</code>配置，<code>authorizer.class</code>就是实现授权的具体类。</p>

<p>Kafka的授权逻辑是<code>Principal P is [Allowed/Denied] Operation O From Host H On Resource R</code>，P是用户，O是上文提到的各种Operation，Host就是client地址，R是Kafka资源，包括cluster、topic、consumer-group。</p>

<p>Kafka本身自带一个<code>SimpleAclAuthorizer</code>，用它来实现一些简单资源的访问，例如</p>

<blockquote>
  <p>Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from IP 198.51.100.0 and IP 198.51.100.1</p>
</blockquote>

<h3 id="ranger">Ranger</h3>

<p>Ranger的Kafka plugin也是实现了Authorizer接口。Ranger的实现机制简单的介绍下，Ranger整体分为Admin和Plugin：</p>

<ol>
  <li>Ranger Plugin运行在服务进程内，在Kafka中，Ranger plugin代码就运行在broker内。</li>
  <li>Policy通过Ranger Admin存储在database中，plugin轮询地向admin请求最新的policy；policy存储在本地的一个文件中。</li>
  <li>在service请求到来的时候，ranger plugin中的policy engine会evaluate request，判断是否合法。</li>
</ol>

<p>Ranger的policy engine分为role based和tag based，kafka中使用的是tag based，evaluae的流程图如下：</p>

<p><img src="https://cwiki.apache.org/confluence/download/attachments/61322361/Ranger-Policy-Evaluation-Flow-with-Tags.png?version=2&amp;modificationDate=1444869949000&amp;api=v2" width="600px" /></p>

<p>Ranger Kafka 目前支持的功能还是比较少的，如下图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/11458053.jpg" width="600px" /></p>

<p>这些功能在Kafka自带的<code>SimpleAclAuthorizer</code>都是可以实现的。</p>

<h2 id="saslkerberos-and-ssl-implementation">Sasl/Kerberos and SSL implementation</h2>

<p>sasl 是broker的认证机制，ssl是数据传输的加密和认证机制。从协议的角度来说，kafka支持以下四种：</p>

<ul>
  <li>
    <p>PLAINTEXT (non-authenticated, non-encrypted)</p>

    <p>This channel will provide exact behavior for communication channels as previous releases</p>
  </li>
  <li>
    <p>SSL</p>

    <p>SSL  implementation. Authenticated principal in the session will be from the certificate presented or the peer host. </p>
  </li>
  <li>
    <p>SASL+PLAINTEXT</p>

    <p>SASL authentication will be used over plaintext channel. Once the sasl authentication established between client and server . Session will have client’s principal as authenticated user. There won’t be any wire encryption in this case as all the channel communication will be over plain text .</p>
  </li>
  <li>
    <p>SASL+SSL</p>

    <p>SSL will be established initially and  SASL authentication will be done over SSL. Once SASL authentication is established users principal will be used as authenticated user .  This option is useful if users want to use SASL authentication ( for example kerberos ) with wire encryption.</p>
  </li>
</ul>

<p>实现SSL需要做如下配置：</p>

<ol>
  <li>Generate SSL key and certificate for each Kafka broker
    <ul>
      <li>Ensure that common name (CN) matches exactly with the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not the malicious one.</li>
    </ul>
  </li>
  <li>Creating your own CA</li>
  <li>Signing the certificate</li>
  <li>Configuring Kafka Brokers</li>
  <li>Configuring Kafka Clients
    <ul>
      <li>需要将生成的<code>kafka.client.truststore.jks</code>拷贝到client</li>
      <li>如果进行双向认证则还需要生成和配置<code>kafka.client.keystore.jks</code></li>
    </ul>
  </li>
</ol>

<p>实现SASL需要：</p>

<ol>
  <li>Kerberos
    <ul>
      <li>客户端需要安装 kerberos client</li>
    </ul>
  </li>
  <li>Create Kerberos Principals
    <ul>
      <li>需要对应的用户principal</li>
    </ul>
  </li>
  <li>Make sure all hosts can be reachable using hostnames</li>
</ol>

<p>具体过程参考<a href="http://kafka.apache.org/documentation.html#security">http://kafka.apache.org/documentation.html#security</a></p>

<p><strong>zookeeper安全性</strong></p>

<ol>
  <li>不开通ibgw（端口），bcc无法直接访问</li>
  <li>zookeeper限制ip段，</li>
  <li>增加zookeeper authentication</li>
</ol>

<p>对每个resource都应该能够实现管理和控制。</p>

<h2 id="references">REFERENCES:</h2>

<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin">https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin</a></p>

<p><a href="https://kafka.apache.org/090/configuration.html">https://kafka.apache.org/090/configuration.html</a></p>

<p><a href="https://kafka.apache.org/090/ops.html">https://kafka.apache.org/090/ops.html</a></p>

<p><a href="https://kafka.apache.org/090/security.html">https://kafka.apache.org/090/security.html</a></p>

<p><a href="http://kafka.apache.org/documentation.html">http://kafka.apache.org/documentation.html</a></p>

<p><a href="http://people.csail.mit.edu/matei/courses/2015/6.S897/readings/kafka.pdf">Kafka: A distributed messaging system for log processing</a></p>

]]></content>
  </entry>
  
</feed>
