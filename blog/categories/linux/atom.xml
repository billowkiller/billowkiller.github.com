<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Billowkiller's Blog]]></title>
  <link href="http://billowkiller.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2014-08-23T15:23:10+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux thundering herd]]></title>
    <link href="http://billowkiller.github.io/blog/2014/07/17/thundering-herd/"/>
    <updated>2014-07-17T09:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/07/17/thundering-herd</id>
    <content type="html"><![CDATA[<p><em>modified from <a href="http://blog.csdn.net/russell_tao/article/details/7204260">http://blog.csdn.net/russell_tao/article/details/7204260</a></em></p>

<hr />

<h2 id="section">惊群现象</h2>

<p>什么是“惊群”？简单说来，多线程/多进程（linux下线程进程也没多大区别）等待同一个socket事件，当这个事件发生时，这些线程/进程被同时唤醒，就是惊群。可以想见，效率很低下，许多进程被内核重新调度唤醒，同时去响应这一个事件，当然只有一个进程能处理事件成功，其他的进程在处理该事件失败后重新休眠（也有其他选择）。这种性能浪费现象就是惊群。</p>

<p>惊群通常发生在server 上，当父进程绑定一个端口监听socket，然后fork出多个子进程，子进程们开始循环处理（比如accept）这个socket。每当用户发起一个TCP连接时，多个子进程同时被唤醒，然后其中一个子进程accept新连接成功，余者皆失败，重新休眠。</p>

<p>那么，我们不能只用一个进程去accept新连接么？然后通过消息队列等同步方式使其他子进程处理这些新建的连接，这样惊群不就避免了？没错，惊群是避免了，但是效率低下，因为这个进程只能用来accept连接。对多核机器来说，仅有一个进程去accept，这也是程序员在自己创造accept瓶颈。所以，我仍然坚持需要多进程处理accept事件。</p>

<h2 id="linux">linux解决的惊群</h2>

<p>其实，在linux2.6内核上，<strong>accept系统调用已经不存在惊群了</strong>（至少我在2.6.18内核版本上已经不存在）。大家可以写个简单的程序试下，在父进程中bind,listen，然后fork出子进程，所有的子进程都accept这个监听句柄。这样，当新连接过来时，大家会发现，仅有一个子进程返回新建的连接，其他子进程继续休眠在accept调用上，没有被唤醒。</p>

<p>对于一些已知的惊群问题，内核开发者增加了一个“<strong>互斥等待</strong>”选项。一个互斥等待的行为与睡眠基本类似，主要的不同点在于：</p>

<ul>
  <li>当一个等待队列入口有 WQ_FLAG_EXCLUSEVE 标志置位, 它被添加到等待队列的尾部. 没有这个标志的入口项, 相反, 添加到开始.</li>
  <li>当 wake_up 被在一个等待队列上调用时, 它在唤醒第一个有 WQ_FLAG_EXCLUSIVE 标志的进程后停止。也就是说，对于互斥等待的行为，比如如对一个listen后的socket描述符，多线程阻塞accept时，系统内核只会唤醒所有正在等待此时间的队列的第一个，队列中的其他人则继续等待下一次事件的发生，这样就避免的多个线程同时监听同一个socket描述符时的惊群问题。</li>
</ul>

<h2 id="nginx">nginx解决惊群</h2>

<p>但是很不幸，通常我们的程序没那么简单，不会愿意阻塞在accept调用上，我们还有许多其他网络读写事件要处理，linux下我们爱用epoll解决非阻塞socket。所以，即使accept调用没有惊群了，我们也还得处理惊群这事，因为epoll有这问题。上面说的测试程序，如果我们在子进程内不是阻塞调用accept，而是用<code>epoll_wait</code>，就会发现，新连接过来时，多个子进程都会在<code>epoll_wait</code>后被唤醒！</p>

<p>nginx就是这样，master进程监听端口号（例如80），所有的nginx worker进程开始用<code>epoll_wait</code>来处理新事件（linux下），如果不加任何保护，一个新连接来临时，会有多个worker进程在<code>epoll_wait</code>后被唤醒，然后发现自己accept失败。</p>

<p>nginx在同一时刻只允许一个nginx worker在自己的epoll中处理监听句柄。它的负载均衡也很简单，当达到最大connection的7/8时，本worker不会去试图拿accept锁，也不会去处理新连接，这样其他nginx worker进程就更有机会去处理监听句柄，建立新连接了。而且，由于timeout的设定，使得没有拿到锁的worker进程，去拿锁的频繁更高。</p>

<h2 id="nginx-1">nginx的锁</h2>

<p>在用户空间进程间锁实现的原理很简单，就是能弄一个让所有进程共享的东西，比如mmap的内存，比如文件，然后通过这个东西来控制进程的互斥。</p>

<p>nginx的实现分为两种情况：</p>

<ul>
  <li>一种是支持原子操作的情况，也就是由mmap的内存区域来进行控制的</li>
  <li>一种是不支持原子操作，这是是使用文件锁来实现。 </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux reentrant function]]></title>
    <link href="http://billowkiller.github.io/blog/2014/07/15/linux-reentrant-function/"/>
    <updated>2014-07-15T06:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/07/15/linux-reentrant-function</id>
    <content type="html"><![CDATA[<p>这种情况出现在多任务系统当中，在任务执行期间捕捉到信号并对其进行处理时，进程正在执行的指令序列就被信号处理程序临时中断。如果从信号处理程序返回，则继续执行进程断点处的正常指令序列，从重新恢复到断点重新执行的过程中，函数所依赖的环境没有发生改变，就说这个函数是可重入的，反之就是不可重入的。简单来说，<strong>可重入函数可以被中断的函数</strong>。</p>

<p>在进程中断期间，系统会保存和恢复进程的上下文，然而恢复的上下文仅限于返回地址，cpu寄存器等之类的少量上下文，而函数内部使用的诸如全局或静态变量，buffer等并不在保护之列，所以如果这些值在函数被中断期间发生了改变，那么当函数回到断点继续执行时，其结果就不可预料了。打个比方，比如<code>malloc</code>，将如一个进程此时正在执行<code>malloc</code>分配堆空间，此时程序捕捉到信号发生中断，执行信号处理程序中恰好也有一个<code>malloc</code>，这样就会对进程的环境造成破坏，因为malloc通常为它所分配的存储区维护一个链接表，插入执行信号处理函数时，进程可能正在对这张表进行操作，而信号处理函数的调用刚好覆盖了进程的操作，造成错误。</p>

<p><strong>基本上下面的函数是不可重入的：</strong></p>

<ul>
  <li>函数体内使用了静态的数据结构；</li>
  <li>函数体内调用了malloc()或者free()函数；</li>
  <li>函数体内调用了标准I/O函数。</li>
  <li>进行了浮点运算。许多的处理器/编译器中，浮点一般都是不可重入的 （浮点运算大多使用协处理器或者软件模拟来实现）。</li>
</ul>

<p><strong>两种情况需要考虑：</strong></p>

<ol>
  <li>信号处理程序A内外都调用了同一个不可重入函数B；B在执行期间被信号打断，进入A (A中调用了B),完事之后返回B被中断点继续执行，这时B函数的环境可能改变，其结果就不可预料了。</li>
  <li>多线程共享进程内部的资源，如果两个线程A，B调用同一个不可重入函数F，A线程进入F后，线程调度，切换到B，B也执行了F，那么当再次切换到线程A时，其调用F的结果也是不可预料的。</li>
</ol>

<p><strong>在信号处理程序中即使调用可重入函数也有问题要注意</strong>。作为一个通用的规则，当在信号处理程序中调用可重入函数时，应当在其前保存<code>errno</code>，并在其后恢复<code>errno</code>。（<strong>因为每个线程只有一个errno变量，信号处理函数可能会修改其值，要了解经常被捕捉到的信号是SIGCHLD，其信号处理程序通常要调用一种wait函数，而各种wait函数都能改变errno。</strong>）</p>

<p>如果一个函数对多个线程来说是可重入的，则说这个函数是<strong>线程安全的</strong>。但这并不能说明对信号处理程序来说该函数也是可重入的。如果函数对异步信号处理程序的重入是安全的，那么就可以说函数式<strong>异步-信号安全的</strong>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux epoll module]]></title>
    <link href="http://billowkiller.github.io/blog/2014/07/15/linux-epoll-module/"/>
    <updated>2014-07-15T02:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/07/15/linux-epoll-module</id>
    <content type="html"><![CDATA[<p>综合了几个blog以及自己查到的一些资料，总结下Linux中的IO多路复用，主要是<code>epoll</code>模型。</p>

<p><code>select</code>，<code>poll</code>，<code>epoll</code>都是Linux下IO多路复用的机制。Windows下为<code>IOCP</code>模型。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个文件描述符就绪，能够通知程序进行相应的读写操作。其中文件描述符是一个简单的整数，用以标明每一个被进程所打开的文件和socket，包括<code>filefd</code>、<code>socketfd</code>、<code>signalfd</code>、<code>timerfd</code>、<code>eventfd</code>等。<code>eventfd</code> 是一个比 <code>pipe </code>更高效的线程间事件通知机制。</p>

<p>但<code>select</code>，<code>poll</code>，<code>epoll</code>本质上都是<strong>同步I/O</strong>，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而<strong>异步I/O</strong>则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。</p>

<!--more-->

<h2 id="select">select实现</h2>

<p><img src="http://www.ibm.com/developerworks/cn/linux/l-async/figure4.gif" alt="" /></p>

<p><strong><code>select</code>的几大缺点：</strong></p>

<ul>
  <li>
    <u>每次调用`select`，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大</u>
  </li>
  <li>
    <u>同时每次调用`select`都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大</u>
  </li>
  <li>
    <u>`select`支持的文件描述符数量太小了，默认是`1024`</u>
  </li>
</ul>

<h2 id="poll">poll实现</h2>

<p><code>poll</code>的实现和<code>select</code>非常相似，只是描述fd集合的方式不同，poll使用<code>pollfd</code>结构而不是select的<code>fd_set</code>结构，其他的都差不多。poll用<strong>nfds</strong>参数指定最多监听多少个文件描述符和事件，能达到系统允许打开的最大文件描述符数目，这点和<code>epoll</code>一样。</p>

<h2 id="epoll">epoll</h2>

<p><code>epoll</code>是对<code>select</code>和<code>poll</code>的改进，能避免上述的三个缺点。我们先看一下<code>epoll</code>和<code>select</code>和<code>poll</code>的调用接口上的不同，<code>select</code>和<code>poll</code>都只提供了一个函数——<code>select</code>或者<code>poll</code>函数。而<code>epoll</code>提供了三个函数：</p>

<ul>
  <li><code>epoll_create</code>，创建一个epoll句柄；</li>
  <li><code>epoll_ctl</code>，注册要监听的事件类型；</li>
  <li><code>epoll_wait</code>，等待事件的产生。</li>
</ul>

<p>对于第一个缺点，<code>epoll</code>的解决方案在<code>epoll_ctl</code>函数中。每次注册新的事件到<code>epoll</code>句柄中时（在<code>epoll_ctl</code>中指定<code>EPOLL_CTL_ADD</code>），会把所有的fd拷贝进内核，而不是在<code>epoll_wait</code>的时候重复拷贝。<code>epoll</code>保证了每个fd在整个过程中只会拷贝一次。</p>

<p>对于第二个缺点，<code>epoll</code>的解决方案不像<code>select</code>或<code>poll</code>一样每次都把current轮流加入fd对应的设备等待队列中，而只在<code>epoll_ctl</code>时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。<code>epoll_wait</code>的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用<code>schedule_timeout()</code>实现睡一会，判断一会的效果，和<code>select</code>实现中的是类似的）。</p>

<p>对于第三个缺点，<code>epoll</code>没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以<code>cat /proc/sys/fs/file-max</code>察看,一般来说这个数目和系统内存关系很大。</p>

<h3 id="section">触发模型</h3>

<h4 id="eagain">1. EAGAIN</h4>

<p>先说下<code>EAGIN</code>这个返回值。</p>

<p>在一个非阻塞的socket上调用read/write函数, 返回<code>EAGAIN</code>或者<code>EWOULDBLOCK</code>(注: EAGAIN就是EWOULDBLOCK)。从字面上看, 意思是:EAGAIN: 再试一次，EWOULDBLOCK: 如果这是一个阻塞socket, 操作将被block，perror输出: Resource temporarily unavailable</p>

<p><strong>总结:</strong></p>

<p>这个错误表示资源暂时不够，能read时，读缓冲区没有数据，或者write时，写缓冲区满了。遇到这种情况，如果是阻塞socket，read/write就要阻塞掉。而如果是非阻塞socket，read/write立即返回-1， 同时<code>errno</code>设置为<code>EAGAIN</code>。</p>

<p>所以，<strong>对于阻塞socket，read/write返回-1代表网络出错了。但对于非阻塞socket，read/write返回-1不一定网络真的出错了。可能是Resource temporarily unavailable。这时你应该再试，直到Resource available。</strong></p>

<h4 id="lt--et">2. LT &amp; ET</h4>

<p><code>epoll</code>除了提供<code>select\poll</code>那种IO事件的<strong>电平触发(Level Triggered)</strong>外，还提供了<strong>边沿触发(Edge Triggered)</strong>，这就使得用户空间程序有可能缓存IO状态，减少<code>epoll_wait/epoll_pwait</code>的调用，提供应用程序的效率。</p>

<ul>
  <li>
    <p><strong>LT(level triggered)：</strong>水平触发，缺省方式，同时支持block和no-block socket，在这种做法中，内核告诉我们一个文件描述符是否被就绪了，如果就绪了，你就可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错的可能性较小。传统的<code>select\poll</code>都是这种模型的代表。</p>
  </li>
  <li>
    <p><strong>ET(edge-triggered)：</strong>边沿触发，高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪状态时，内核通过<code>epoll</code>告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如：你在发送、接受或者接受请求，或者发送接受的数据少于一定量时导致了一个EWOULDBLOCK错误)。但是请注意，如果一直不对这个fs做IO操作(从而导致它再次变成未就绪状态)，内核不会发送更多的通知。</p>
  </li>
</ul>

<p><strong>区别：</strong>LT事件不会丢弃，而是只要读buffer里面有数据可以让用户读取，则不断的通知你。而ET则只在事件发生之时通知。</p>

<p><strong>ET方式注意事项：</strong> 必须使用<strong>非阻塞套接口</strong>，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。最好以下面的方式调用ET模式的epoll接口：</p>

<ul>
  <li>基于非阻塞文件句柄</li>
  <li>
    <p>只有当read(2)或者write(2)返回<strong>EAGAIN</strong>时才需要挂起，等待。但这并不是说每次read()时都需要循环读，直到读到产生一个EAGAIN才认为此次事件处理完成，<strong>当read()返回的读到的数据长度小于请求的数据长度时，就可以确定此时缓冲中已没有数据了</strong>，也就可以认为此事读事件已处理完成。</p>

    <pre><code>  while(rs)
  {
    buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0);
    if(buflen &lt; 0)
    {
      // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读
      // 在这里就当作是该次事件已处理处.
      if(errno == EAGAIN)
       break;
      else
       return;
     }
     else if(buflen == 0)
     {
       // 这里表示对端的socket已正常关闭.
     }
     if(buflen == sizeof(buf)
       rs = 1;   // 需要再次读取
     else
       rs = 0;
  }
</code></pre>
  </li>
</ul>

<p><strong>还有，假如发送端流量大于接收端的流量(意思是epoll所在的程序读比转发的socket要快),由于是非阻塞的socket,那么<code>send()</code>函数虽然返回,但实际缓冲区的数据并未真正发给接收端,这样不断的读和发，当缓冲区满后会产生<code>EAGAIN</code>错误(参考<code>man send</code>),同时,不理会这次请求发送的数据.所以,需要封装<code>socket_send()</code>的函数用来处理这种情况,该函数会尽量将数据写完再返回，返回-1表示出错。在<code>socket_send()</code>内部,当写缓冲已满(<code>send()</code>返回-1,且<code>errno</code>为<code>EAGAIN</code>),那么会等待后再重试.这种方式并不很完美,在理论上可能会长时间的阻塞在<code>socket_send()</code>内部,但暂没有更好的办法.</strong></p>

<h4 id="accept">3. 正确的accept</h4>

<p>正确的accept，accept 要考虑 3 个问题</p>

<ol>
  <li>
    <p><strong>阻塞模式 accept 存在的问题</strong></p>

    <p>考虑这种情况：TCP连接被客户端夭折，即在服务器调用accept之前，客户端主动发送RST终止连接，导致刚刚建立的连接从就绪队列中移出，如果套接口被设置成阻塞模式，服务器就会一直阻塞在accept调用上，直到其他某个客户建立一个新的连接为止。但是在此期间，服务器单纯地阻塞在accept调用上，就绪队列中的其他描述符都得不到处理。</p>

    <p><strong>解决办法是把监听套接口设置为非阻塞</strong>，当客户在服务器调用accept之前中止某个连接时，accept调用可以立即返回-1，这时源自Berkeley的实现会在内核中处理该事件，并不会将该事件通知给epool，而其他实现把errno设置为ECONNABORTED或者EPROTO错误，我们应该忽略这两个错误。</p>
  </li>
  <li>
    <p><strong>慢系统调用被中断</strong></p>

    <p>当阻塞与某个慢系统调用的一个进程捕获某个信号且相应信号处理函数返回是，该系统调用可能返回一个<code>IENTR</code>错误。我们必须对慢系统调用返回<code>EINTR</code>有所准备。对于accept，以及诸如<code>read</code>、<code>write</code>、<code>select</code>和<code>open</code>之类函数来说，<strong>需要自己重启被中断的系统调用</strong>。不过有一个函数我们不能重启：<code>connect</code>。</p>
  </li>
  <li>
    <p><strong>ET模式下accept存在的问题</strong></p>

    <p>考虑这种情况：<strong>多个连接同时到达</strong>，服务器的TCP就绪队列瞬间积累多个就绪连接，由于是边缘触发模式，epoll只会通知一次，accept只处理一个连接，导致TCP就绪队列中剩下的连接都得不到处理。</p>

    <p><strong>解决办法是用while循环抱住accept调用</strong>，处理完TCP就绪队列中的所有连接后再退出循环。如何知道是否处理完就绪队列中的所有连接呢？accept返回-1并且errno设置为EAGAIN就表示所有连接都处理完。</p>
  </li>
</ol>

<p>综合以上两种情况，服务器应该使用非阻塞地accept，accept在ET模式下的正确使用方式为：</p>

<pre><code>while ((conn_sock = accept(listenfd,(struct sockaddr *) &amp;remote, (size_t *)&amp;addrlen)) &gt; 0) {
    handle_client(conn_sock);
}
if (conn_sock == -1) {
    if (errno != EAGAIN &amp;&amp; errno != ECONNABORTED &amp;&amp; errno != EPROTO &amp;&amp; errno != EINTR)
    perror("accept");
}
</code></pre>

<h4 id="section-1">4. 其他</h4>

<p><strong>EPOLLONETSHOT</strong></p>

<p>epoll模式中事件可能被触发多次，比如socket接收到数据交给一个线程处理数据，在数据没有处理完之前又有新数据达到触发了事件，另一个线程被激活获得该socket，从而产生多个线程操作同一socket，即使在ET模式下也有可能出现这种情况。采用EPOLLONETSHOT事件的文件描述符上的注册事件只触发一次，要想重新注册事件则需要调用epoll_ctl重置文件描述符上的事件，这样前面的socket就不会出现竞态。</p>

<p>如果一个工作线程处理完某个socket上的一次请求之后，又收到该socket上新的客户请求，则该线程将继续为这个socket服务，并且因为该socket上注册了EPOLLONESHORT事件，其他线程没有机会接触这个soket。如果工作线程之后没有收到该socket的下一批用户数据，则放弃该socket服务。同时重置该socket上的注册事件，使得epoll有机会再次检查到该socket上的EPOLLIN事件，进而是的其他线程有机会为该socket服务。</p>

<p><strong>注意：监听socket linstenfd上是不能注册EPOLLONESHOT事件，否则应用程序只能处理一个客户连接。后续的客户请求不再触发listenfd上的EPOLLIN事件。</strong></p>

<p><strong>EPOLLONESHOT和ET一样都是为了能进一步减少可读、可写和异常事件被触发的次数。</strong></p>

<h2 id="section-2">总结</h2>
<p><strong>epoll高效的原因：</strong></p>

<ul>
  <li><code>epoll</code>把用户关心的文件描述符上的时间放在内核的一个事件表中，从而无需向<code>select</code>和<code>poll</code>那样每次调用都要重复传入描述符集合。</li>
  <li>另一个原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了</li>
  <li>使用<code>mmap</code>加速内核与用户空间的消息传递</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Signal and fork]]></title>
    <link href="http://billowkiller.github.io/blog/2014/06/28/signal-and-forkmarkdown/"/>
    <updated>2014-06-28T04:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/06/28/signal-and-forkmarkdown</id>
    <content type="html"><![CDATA[<p>当线程调用fork时，就为子进程创建了整个进程地址空间的副本。子进程与父进程是完全不同的进程，只要两者都没有对内存作出改动，父进程和子进程之间还可以共享内存副本。注意一下几个情况：</p>

<ol>
  <li>子进程通过继承整个地址空间的副本，<strong>从父进程那里继承了所有互斥量、读写锁和条件变量的状态</strong>。也就是说，如果它在父进程中被锁住，则它在子进程中也是被锁住的。</li>
  <li>只有调用fork()的线程被复制到子进程（子进程中线程的ID），如果子进程中包含占有锁的线程的副本，那么子进程就没有办法知道它占有了那些锁并且需要释放那些锁，<strong>容易造成死锁</strong>。</li>
  <li>thread-specific data的销毁函数和清除函数都不会被调用。在多线程中调用fork()可能会引起内存泄露。比如在其他线程中创建的thread-specific data，在子进程中将没有指针来存取这些数据，<strong>造成内存泄露</strong>。</li>
</ol>

<p>因为以上这些问题，<strong>在线程中调用fork()的后，我们通常都会在子进程中调用exec()</strong>。因为exec()能让父进程中的所有互斥量，条件变量（pthread objects）在子进程中统统消失（用新数据覆盖所有的内存）。对于那些要使用fork()但不使用exec()的程序，pthread API提供了一个新的函数</p>

<pre><code>pthread_atfor(void (*prepare_func)(void), void(*parent_func)(void), void (*child_func)(void))
</code></pre>

<p>prepare_func在父进程调用fork之前调用，parent_func在fork执行后在父进程内被调用，child_func在fork执行后子进程内被调用。除非你打算很快的exec一个新程序，否则应该避免在一个多线程的程序中使用fork。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Signal and Thread]]></title>
    <link href="http://billowkiller.github.io/blog/2014/06/28/signal-and-thread/"/>
    <updated>2014-06-28T03:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/06/28/signal-and-thread</id>
    <content type="html"><![CDATA[<p>类UNIX信号以前是专为进程设计的，它比线程的出现早了很多年。当线程模型出现后，专家们试图也在线程上实现信号，这导致了一个问题：即使是在基于进程的编程模式中，信号的处理也可能是很复杂的，因为它打断了正在运行的thread of control， 在signal handler中只能调用可重入函数，修改全局变量的类型必须是<code>sig_atomic_t</code>类型，防止内存访问优化； 而把线程引入编程范型，就使信号的处理变得更加复杂。</p>

<p><strong>避免信号和线程一起使用是明智的选择。</strong>但是，将他们分开又是不可能或不实际的。只要有可能的话，仅仅在主线程内使用<code>pthread_sigmask()</code>来屏蔽信号，然后同步地在专用线程中使用<code>sigwait()</code>来处理信号。</p>

<!--more-->

<h2 id="section">信号模型映射到线程模型</h2>

<p>为了理解信号模型是怎样映射到线程模型的，我们需要知道信号模型的哪些方面是影响进程层面的（process-wide），哪些方面只会影响某个线程的。下面列出几点:</p>

<ol>
  <li>signal actions 是process-wide。如果一个没有处理的信号的默认动作是停止SIGSTOP或终止SIGKILL(该动作是让整个进程停止或终止，而不是只针对某个线程)，那么不管这个信号是发送给哪个线程，整个进程都会停止或终止。</li>
  <li>signal dispositions信号部署是process-wide。每个线程都有自己的信号屏蔽字，但是<strong>信号的处理是进程中所有线程共享的</strong>。这意味着尽管每个线程可以阻止某些信号，但当线程修改了与某个信号相关的处理行为之后，所有的线程都必须共享这个处理行为的改变。</li>
  <li>信号通常是被发送到<strong>任意一个线程</strong>，为了保证不会在多线程进程中一个信号多次被执行。但是以下几种情况是传递到<strong>单个线程</strong>的：
    <ul>
      <li>信号与硬件故障或计时器超时相关。</li>
      <li>当线程尝试向一个broken pipe写数据时，会产生一个SIGPIPE。</li>
      <li>使用<code>pthread_kill()</code>或者<code>pthread_sigqueue()</code>。这些函数允许一个线程发送信号到同一进程的另一个线程。</li>
    </ul>
  </li>
  <li><strong>信号掩码(signal mask)是线程私用的。</strong>在多线程的进程中，不存在process-wide的信号掩码。线程可以使用<code>pthread_sigmask()</code>来独立的屏蔽某些信号。通过这种方法，程序员可以控制那些线程响应那些信号。当线程被创建时，它将继承创建它的线程的信号掩码。</li>
  <li><strong>内核为每个线程和进程分别维护了一个未决信号的表</strong>。当使用<code>sigpending()</code>时，该函数返回的是整个进程未决信号表和调用该函数的线程的未决信号表的并集。当新线程被创建时，线程的pending signals被设置为空。当线程A阻塞某个信号S后，发送到A中的信号S将会被挂起，直到线程取消了对信号S的阻塞。</li>
  <li>如果一个信号处理函数打断了<code>pthread_mutex_lock()</code>，该<strong>函数会自动的重新执行</strong>。如果信号处理函数打断了<code>pthread_cond_wait()</code>，该函数要么自动重新自行（linux是这样实现的），或者返回0（这时应用要检查返回值，判断是否为假唤醒）。</li>
</ol>

<h2 id="section-1">异步信号的处理</h2>

<p>一个函数要么是可重入的（reentrant）,要么是不能被信号处理函数打断的，我们把这种函数叫做是<code>async-signal-safe</code>的。调用非<code>async-signal-safe</code>的函数是危险的，比如，考虑在线程A中，我们调用<code>malloc()</code>来进行内存分配，<code>malloc()</code>刚用互斥量锁住了全局链表，这是异步信号到达，在信号处理函数中也调用<code>malloc()</code>，这时该函数会阻塞在互斥量上，形成死锁（这个例子在单线程的进程中也会出现）。Pthread API不是<code>async-signal-safe</code>的，也就是说在信号处理函数中不要使用pthread相关的函数。</p>

<p><strong>解决这个问题</strong>的最好办法是，在不打断正常程序的前提下，把所有的异步信号都在同一处处理。在单线程程序中，这是做不到的，因为所有发送的信号都会打断程序。而在多线程程序中，我们可以<u>单独创建一个线程来接受信号，处理需要的信号，而不会打断其他线程的工作。</u></p>

<p>上面举的这个例子中还有一点没说到，就是<strong>信号处理函数也会被其他信号所打断</strong>。那我们怎么处理这个问题呢？<u>在处理信号之前，对所有的异步信号进行阻塞，等工作处理完毕后，再恢复阻塞的信号。</u>这个工作就靠下面这个函数执行：</p>

<pre><code>int sigwait(const sigset_t *set, int *sig)
</code></pre>

<ul>
  <li><code>sigwait()</code>的好处在于它可以简化信号处理，允许把异步产生的信号用同步方式处理。</li>
  <li>调用<code>sigwait()</code>等待的信号必须在调用线程中屏蔽，通常我们在所有线程中都会屏蔽。</li>
  <li>信号仅仅被交付一次。如果两个线程在<code>sigwait()</code>上阻塞（等待同一个信号），只有一个线程（不确定的线程）将收到送给进程的信号。这意味着不能让两个独立的子系统使用<code>sigwait()</code>来捕获相同的信号。信号捕获<code>sigaction</code>建立的信号处理程序和<code>sigwait</code>也同样只有一个可以执行。</li>
</ul>
]]></content>
  </entry>
  
</feed>
