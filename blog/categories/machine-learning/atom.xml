<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Billowkiller's Blog]]></title>
  <link href="http://billowkiller.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-03-05T15:42:47+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Expectation Maximization]]></title>
    <link href="http://billowkiller.github.io/blog/2016/03/05/em/"/>
    <updated>2016-03-05T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/03/05/em</id>
    <content type="html"><![CDATA[<p>Expectation Maximization, EM算法在参数估计里面有极大的用处，它用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率（MAP）估计。隐变量的概率模型参数的极大似然估计可以理解为，使用的方法还是的极大似然估计，但是要处理隐变量。极大后验概率是一种Beyesian Inference，其实就是把极大似然估计中的参数赋予权值，这个权值是预先定义好的先验概率。可以来看下下表中Frequentist-Bayesian对峙的部分，来感受下EM算法的应用范围：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/88568881.jpg" width="500px" /></p>

<!--more-->

<p>下面我们先从一个Two-Component Gaussian Mixture Model为例，介绍EM算法。</p>

<h2 id="two-component-gaussian-mixture-model">Two-Component Gaussian Mixture Model</h2>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/45836233.jpg" width="600px" /></p>

<p>上图是一个mixture example，左边是我们观察到数据的直方图，右边红线是最大似然拟合的高斯密度函数，绿色的点是用来做两个模型的分类用。</p>

<p>这里其实我们得到的是一些数据点，对于这些点的分布完全一无所知。先做出如下假设，这是两个高斯模型混合后的sample data。</p>

<script type="math/tex; mode=display"> Y_1 \sim N(\mu_1, \theta_1^2) </script>

<script type="math/tex; mode=display"> Y_2 \sim N(\mu_2, \theta_2^2) </script>

<script type="math/tex; mode=display"> Y = (1 - \Delta)\cdot Y_1 + \Delta \cdot Y_2,\  \Delta \in \{0,1\}, Pr(\Delta =1) = \pi</script>

<p>那么需要我们估计的参数就为 $(\pi, \theta_1, \theta_2) = (\pi, \mu_1, \sigma_1, \mu_2, \sigma_2)$，一共五个参数。使用似然估计，我们可以得到如下过程：</p>

<script type="math/tex; mode=display"> g_Y(y) = (1-\pi)\phi_{\theta_1}(y) + \pi \phi_{\theta_2}(y) </script>

<script type="math/tex; mode=display">log\ likelihood \to l(\theta; Z) = \sum_{i=1}^N log[(1-\pi)\phi_{\theta_1}(y_i) + \pi \phi_{\theta_2}(y_i)] </script>

<p>最大化 $l(\theta; Z)$ 无疑是困难的，因为对数中含有加号。如果我们知道隐变量 $\Delta$ 的取值，那么参数估计就会变得容易，$\phi$ 的估计也就是 $\Delta_i=1$ 的比例，另外 $\theta_1,\theta_2$ 也就变成 $\Delta_i=0，\Delta_i=1$ 的似然估计。</p>

<p>所以问题的关键是 $\Delta$ 的取值，解决问题的思路是采用迭代的方式，每次都用 $\Delta_i$ 的估计值替换：</p>

<script type="math/tex; mode=display">\gamma_i(\theta) = E(\Delta_i \vert \theta,Z) = Pr(\Delta_i=1 \vert \theta,Z)</script>

<p>如此 $\theta_1,\theta_2$ 自然也就可以由最大似然估计求出。在下一次过程中，$\gamma_i(\theta)$ 又可以由上一步估计的 $\theta_1,\theta_2$ 求出。所以我们首先需要给出参数的初始值，就可以由上述过程得到结果。算法入下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/24344888.jpg" width="550px" /></p>

<p>这里需要注意的是，如果我们在某个点取 $\hat{\mu}_1 = y_i, \hat{\sigma}_1=0$ 那么我们可以去到最大的似然值，无限大，但这并不是有用的解。所以我们其实是求解 <u>a good local maximum of the likelihood</u>，因此我们可以设多个初值，最后选择似然值最大的解。</p>

<h2 id="em-in-general">EM in General</h2>

<p>EM算法被用于data augmentation，关于data augmentation的解释如下：</p>

<blockquote>
  <p>maximization of the likelihodd is difficult, but made easier by enlarging the sample with latent data</p>
</blockquote>

<p>上面的例子中我们设的latent data为 $\Delta$，是出于我们对模型的假设；其他的latent data还可以为丢失的观察值。接下来我们介绍EM的通用形式，先给出算法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/67870944.jpg" width="550px" /></p>

<p>上面的算法中，$Z$ 为观察值，log likelihood是 $l(\theta, Z)$。latent or missing data 为 $Z^m$， 完整的数据为 $T=(Z, Z^m$)，对比于上面的例子 $(Z, Z^m) = (y, \Delta)$。</p>

<ul>
  <li>E step就是完全数据 $T$ 的对数似然函数 $l_0(\theta’; T)$ 关于在给定观察数据 $Z$ 和当前参数 $\theta^{(j)}$ 下对未观察数据 $Z^m$ 的条件概率分布 $Pr(Z^m \vert Z, \theta^{(j)})$ 的期望，得到的是 $Z^m$ 的估计。</li>
  <li>M step就是通过似然估计方法，求未观察数据 $Z^m$ 的条件概率分布的期望的最大值，得到参数 $\theta$ 的重新估计 $\theta’$，在下一个E中变为 $\theta^{(j+1)}$。</li>
</ul>

<p>上面叙述了EM的算法，那么为什么EM算法能有效，也就是近似实现对观测数据的极大似然估计呢？我们看到</p>

<script type="math/tex; mode=display"> Pr(Z^m \vert Z, \theta') = \frac{Pr(Z^m,Z  \vert \theta')}{Pr(Z \vert  \theta')} </script>

<script type="math/tex; mode=display"> \to Pr(Z \vert  \theta') = \frac{Pr(T  \vert  \theta')}{ Pr(Z^m \vert Z, \theta')} </script>

<script type="math/tex; mode=display"> \to l(\theta'; Z) = l_0(\theta'; T) - l_1(\theta'; Z^m \vert Z) </script>

<p>对由 $\theta$ 控制的分布 $T \vert Z$ 数据求期望可以得到：</p>

<script type="math/tex; mode=display"> l(\theta'; Z) = E[l_0(\theta'; T) \vert Z,\theta] - E[l_1(\theta'; Z^m \vert Z) \vert Z,\theta] = Q(\theta', \theta) - R(\theta', \theta) </script>

<p>在 $M\ step$ 中，EM算法求出可以使 $Q(\theta’, \theta)$ 最大化的 $\theta’$，而不是真正的目标函数 $l(\theta’; Z)$。为什么最大化 $Q(\theta’, \theta)$ 最终可以最大化 $l(\theta’; Z)$呢？</p>

<p>可以看到 $R(\theta^*, \theta)$ 是由 $\theta^*$ 决定的条件分布的log-likelihood的期望，这个分布和由 $\theta$ 决定的条件分布是相同的。因此由 Jensen’s inequality 可以得到，$R(\theta’, \theta) \le R(\theta, \theta)$。具体的推导可以参考《统计学习方法》。所以如果 $\theta’$ 最大化 $Q(\theta’, \theta)$ 则</p>

<script type="math/tex; mode=display"> l(\theta'; Z) - l(\theta; Z) = [Q(\theta', \theta) - Q(\theta, \theta)] - [R(\theta', \theta) - R(\theta, \theta)] \ge 0 </script>

<p>所以说EM迭代中，$l(\theta’; Z)$ 一直都会在增大。</p>

<blockquote>
  <p>Jensen’s inequality, $E[\phi(X)] \ge \phi[E(X)]$, for Random variable $X$ and convex function $\phi(x)$</p>
</blockquote>

<p>也就是说在 $M step$ 中完全的最大化是没有必要的，我们只需要找到一个 $\theta^{(j+1)}$ 使得 $Q(\theta^{(j+1)}, \theta^{(j)}) - Q(\theta^{(j)}, \theta^{(j)})$。所以我们得到的EM收敛条件也就是 </p>

<script type="math/tex; mode=display">% &lt;![CDATA[
 \theta^{(j+1)} - \theta^{(j)} < \epsilon\ or\ Q(\theta^{(j+1)}, \theta^{(j)}) - Q(\theta^{(j)}, \theta^{(j)}) < \epsilon  %]]&gt;</script>

<h2 id="em-as-max-max-procedure">EM as max-max Procedure</h2>

<p>EM算法还可以看成是F 函数的极大极大算法， F函数定义如下</p>

<script type="math/tex; mode=display"> F(\theta',  \tilde{P}) = E_{\tilde{P}}[l_0(\theta'; T)] - E_{\tilde{P}}[log \tilde{P}(Z^m)] </script>

<p>$\tilde{P}(Z^m)$也就是隐变量 $Z^m$ 的分布, $- E_{\tilde{P}}[log \tilde{P}(Z^m)]$ 也就是 $\tilde{P}(Z^m)$ 的熵。于是EM算法可以由下图表示：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-5/44573110.jpg" width="500px" /></p>

<p>也就是，设 $\theta^{(i)}$ 为第 $i$ 次迭代参数 $\theta$ 的估计，$\tilde{P}^{(i)}$ 为第 $i$ 次迭代参数 $\tilde{P}$ 的估计。在第 $i+1$ 次迭代的两步为：</p>

<ul>
  <li>对固定的 $\theta^{(i)}$，求 $\tilde{P}^{(i+1)}$ 使得 $F(\theta^{(i)},  \tilde{P})$ 极大化</li>
  <li>对固定的 $\tilde{P}^{(i+1)}$，求 $\theta^{(i+1)}$ 使得 $F(\theta,  \tilde{P}^{(i+1)})$ 极大化</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kernel Logistic Regression versus SVM]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/29/klr-svr/"/>
    <updated>2016-02-29T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/29/klr-svr</id>
    <content type="html"><![CDATA[<p>Logistic Regression和SVM都是分类方法，它们定义线性决策边界（linear decision boundaries）作为划分的依据。但二者在motivation方便完全不同，区别如下</p>

<ul>
  <li>LR初衷是为了让linear regression能够输出二元分类，估计一个实例属于某一类的概率；线性决策边界也只是回归函数的结果，在回归函数中使用阈值作为分类的标准，一般是0.5。决策边界在SVM中来的更为重要，整个模型的目标就是为了获得最优的决策边界。</li>
  <li>每个训练样本都对LR的过程有一定的影响，但SVM只依赖于在决策边界附近的某些点。</li>
  <li>LR适应于低维空间，并且由于使用最大似然估计，所以对噪声不敏感；SVM更适应于高维空间</li>
  <li>没有正规化的LR无法保证获得最好的分离超平面，只能获得margin附近的较高的置信度; SVM则能获得最优的分离超平面。</li>
</ul>

<p>那么二者会有什么联系呢，SVM的kernel function又是如何应用到LR模型的呢？</p>

<!--more-->

<h2 id="loss-function">Loss Function</h2>

<p>我们先来看下Loss Function，它是用来衡量学习函数与数据拟合的程度的。回顾对于机器学习来说有一下几个过程：</p>

<ol>
  <li>假设空间：函数的参数形式，包括lr，svm等</li>
  <li>拟合的衡量：Loss function，likelihood</li>
  <li>bias和variance的trade-off：regularization，bayesian estimator (MAP)</li>
  <li>在假设空间中寻找好的假设：optimization. convex - global. non-convex - multiple starts</li>
  <li>假设的验证：测试数据的预测，cross validation</li>
</ol>

<p>在线性学习方法中，通常我们是要找到一个假设 $y=f(\theta^T x)$，选决定f的参数形式，再通过最大化似然函数或者最小化损失函数找到对应的 $\theta$，常见的几个Loss Function：</p>

<p>For classfication $correct \to y \cdot f &gt; 0;\ incorrect \to y \cdot f &lt; 0$</p>

<ol>
  <li>0/1 loss： $min_\theta \sum_i L_{0/1}(\theta^T x)$. 定义 $L_{0/1}(\theta^T x)=1\ if\ y \cdot f &lt; 0$，其他情况等于0，非凸函数，难以优化。</li>
  <li>Hinge loss: $min_\theta \sum_i H(\theta^T x)$。接近 0/1 损失函数，定义 $H(\theta^T x) = max (0,1-y \cdot f)$</li>
  <li>Logistic loss: $min_\theta \sum_i log(1 + exp(-y \cdot \theta^T x))$. </li>
</ol>

<p>关于3，在逻辑回归中我们最大化似然函数其实就是最小化Logistic loss：</p>

<script type="math/tex; mode=display">\sum_i log \frac{1}{1+exp(-y^{(i)}\cdot \theta^T x^{(i)})} = \sum_i -log (1+exp(-y^{(i)}\cdot \theta^T x^{(i)}))</script>

<script type="math/tex; mode=display">\to min_\theta \sum_i log(1 + exp(-y \cdot \theta^T x))</script>

<p>For regression:</p>

<ol>
  <li>Square loss: $min_\theta \sum_i | y^{(i)} - \theta^T x^{(i)} |^2$</li>
</ol>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-22/70940842.jpg" width="400px" /></p>

<p>如上图所示，SVM使用Hinge Loss；Binomial Deviance就是Logistic Loss，因为output是binomial的；Exponential Loss 可以在Gradient Boost中应用到。</p>

<h2 id="connection-between-svm-and-logistic-regression">Connection between SVM and Logistic Regression</h2>

<p>回顾下soft-margin SVM的原始问题：</p>

<script type="math/tex; mode=display"> \underset{b,w,\xi}{min} \frac{1}{2}w^T w  + C\sum_{n=1}\xi_n</script>

<script type="math/tex; mode=display">s.t.\ y_i(w^T z_n + b) \ge 1-\xi_n,\ \xi_n \ge 0\ for\ all\ n</script>

<p>这里的 $\xi_n$ 实际上就是违反 margin 的距离，称为 margin violation。如果 $(x_n, y_n)$ 不违反 margin，那么 $\xi_n=0$，否则为 $\xi_n=1-y_n(w^T z_n + b)$，从这里我们看到实际上 $\xi_n = max(1-y_n(w^T z_n + b), 0)$，所以原来问题可以写成：</p>

<script type="math/tex; mode=display"> \underset{b,w,\xi}{min} \frac{1}{2}w^T w  + C\sum_{n=1}max(1-y_n(w^T z_n + b), 0)</script>

<p>上式写成 $min\ \frac{1}{2} w^Tw + C \sum \hat{err}$ 就比较熟悉了，这个就是L2 regularization: $min\ \lambda w^Tw + C \sum err$ 嘛。对应关系如下：</p>

<ul>
  <li>soft-margin $\to special\ \hat{err}$</li>
  <li>large margin $\to$ fewer hyperplanes $\to$ L2 regularization for small $w$</li>
  <li>larger C or C $\to$ smaller $\lambda\ to$ less regularization</li>
</ul>

<p>对比于SVM的原始问题，我们发现新的形式并不是凸二次规划问题；不能利用kernel trick；并且max函数不能微分，所以难以解决。那么为什么要变化成这种形式呢？如果设置 linear score: $s=w^T z_n + b$。我们观察下它的损失函数 $\hat{err}_{svm}(s,y) = max(1-ys, 0)$，另外再看下logistic loss: $err_{ll}(ys) = log(1 + exp(-ys))$.</p>

<p>可以对比下上图的损失函数曲线，二者比较接近，并且都是 0/1 损失函数的上限。所以可以把 SVM 当成 L2-regularized logistic regression。以下是几个二分类线性模型的对比：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/76344341.jpg" width="600px" /></p>

<h2 id="kernel-logistic-regression">Kernel Logistic Regression</h2>

<p>现在我们知道 SVM 和 Logistic Regression存在联系，那么是否可以综合二者的有点呢：</p>

<ul>
  <li>SVM flavor: 利用kernel function 得到 $w_{svm}，b_{svm}$ 得到超平面 </li>
  <li>LR flavor: 通过 scalling(A) 和 shifting(B) 匹配最大似然函数的方法细粒度地调优超平面
    <ul>
      <li>often $A &gt; 0$ if $w_{svm}$ reasonably good</li>
      <li>often $B \approx 0$ if $b_{svm}$ reasonably good</li>
    </ul>
  </li>
</ul>

<p>所以新的LR问题可以看成 two-level learning 问题：LR on SVM-transformed data</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/6560678.jpg" width="450px" /></p>

<p>于是我们得到Probabilistic SVM for Soft Binary Classification 的算法，因为 LR 是根据概率分类的，过程如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/75482355.jpg" width="450px" /></p>

<ul>
  <li>这个 Soft Binary Classifier 得到 bounary 与 SVM 的不同，因为有平移 B。</li>
  <li>只需要解两个变量，可以使用GD或者SGD等。</li>
  <li>Kernel SVM 相当于在 $Z$ 空间中进行 LR</li>
</ul>

<p>Kernel SVM 中有 $w_{svm}^T \phi(x) + b_{svm} = \sum_{SV} \alpha_n y_n K(x_n, x) + b_{svm}$，则最后 Probabilistic SVM 的结果为</p>

<script type="math/tex; mode=display"> g(x) = \theta(\sum_{SV} A\alpha_n y_n K(x_n, x) + Ab_{svm} + B)</script>

<p>但这个方法其实并不是在 $Z$ 空间里的最好的解，只是通过两次 scaling 和 shifting 接近最好的解，那么如何得到最好的解呢。这就是我们要介绍的Kernel Logistic Regression。</p>

<p>先来介绍下 Representer Theorem，它的定义如下：</p>

<blockquote>
  <p>the solution of regularization and interpolation problems with Hillbertian penalties can be expressed as a linear combination of the data.</p>
</blockquote>

<p>也就是最优的 $w_* = \sum_{n=1}^N \beta_n z_n$，我们可以把原来在 Hillbertian Space
 的计算放到低维空间中进行。</p>

<script type="math/tex; mode=display"> w_*z = \sum_{n=1}^N \beta_n z_n^Tz = \sum_{n=1}^N \beta_n K(x_n, x)</script>

<p>证明Representer Theorem如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/22880811.jpg" width="500px" /></p>

<p>这其实就表明所有的 L2-regularized linear model 都可以被 kernalized。把损失函数替换成 logistic loss，我们就得到 Kernel Logistic Regression 的优化函数。</p>

<script type="math/tex; mode=display"> \underset{w}{min}\ \frac{\lambda}{N}w^Tw + \frac{1}{N} \sum_{n=1}^N log(1+ exp(-y_n w^T z_n)) </script>

<p>利用 Representer Theorem，将原来对 $w$ 的求解转化为对 $\beta$ 求解：</p>

<script type="math/tex; mode=display"> \underset{\beta}{min}\ \frac{\lambda}{N} \sum_{n=1}^N \sum_{m=1}^N \beta_n \beta_m K(x_n, x_m) +  \frac{1}{N} \sum_{n=1}^N log(1+ exp(-y_n \sum_{m=1}^N \beta_m K(x_m, x_n)))</script>

<p>可以用 GD/SGD 等做最优化求解，<strong>解出来的 $\beta$ 并不同于 SVM 的 $\alpha$，基本上是非零的</strong>。综合上述，总结得到 KLR 其实就是</p>

<blockquote>
  <p>use representer theorem for kernel trick on L2-regularized logistic regression</p>
</blockquote>

<p>对 KLR 还有另外另外的解释</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/45320882.jpg" width="450px" /></p>

<h2 id="support-vector-regression">Support Vector Regression</h2>

<p>最后说下SVR，上面提到 regression 的损失函数 squared error $err(y, w^Tz) = (y- w^Tz)^2$，替换下KLR的损失函数我们可以得到</p>

<script type="math/tex; mode=display"> \underset{w}{min}\ \frac{\lambda}{N}w^Tw + \frac{1}{N} \sum_{n=1}^N (y- w^Tz)^2 </script>

<p>这个就是 <strong>kernel ridge regression</strong>，也可以通过解 $\beta$ 得到答案</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/56123028.jpg" width="500px" /></p>

<p>对 $\beta$ 求导得到</p>

<script type="math/tex; mode=display">\nabla E_{aug}(\beta) = \frac{2}{N}(\lambda K^TI\beta + K^TK\beta - K^Ty) = \frac{2}{N}K^T((\lambda I + K)\beta - y)</script>

<script type="math/tex; mode=display">\nabla E_{aug}(\beta) =0 \to \beta=(\lambda I + K)^{-1}y</script>

<p>因为 Merer’s condition，所以 $K$ 是半正定矩阵，继而上式有解。但是稠密矩阵求反需要 $O(N^3)$ 时间复杂度。可以对比下 linear ridge regression:</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/7052157.jpg" width="450px" /></p>

<p>根据前面所述的SVM和LR之间的关系，我们这里可以吧 kernel ridge regression 看成 least-squares SVM(LLSVM)。它有什么特点呢：</p>

<ul>
  <li>对比soft-margin SVM，有这相似的boundary，但是更多的支持向量。导致预测慢，因为 $\beta$ 比较稠密</li>
</ul>

<p>现在想让 $\beta$ 变得和 soft-margin SVM 的 $\alpha$ 一样稀疏。可以看到 tube regression 的特点，使用 $\epsilon$-insensitive error：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-29/74768649.jpg" width="450px" /></p>

<p>整理下是 L2-regularized tube regression</p>

<script type="math/tex; mode=display"> \underset{w}{min}\ \frac{\lambda}{N}w^Tw + \frac{1}{N} \sum_{n=1}^N\ max(0, \vert w^Tz_n - y \vert - \epsilon)</script>

<p>这个函数没有限制条件，但是max难以微分，并且不能很明显的看出有 sparse $\beta$。发现这个表达式和最开始 soft-SVM 的表示有点像，那么我们可以反向的把它模拟成 standard SVM 形式:</p>

<script type="math/tex; mode=display"> \underset{b,w,\xi^ \vee, \xi ^ \land}{min}\ \frac{1}{2} w^Tw + C \sum_{n=1}^N (\xi_n^ \vee + \xi_n ^ \land)</script>

<script type="math/tex; mode=display">s.t.\ -\epsilon-\xi_n^ \vee \le y_n - w^Tz_n-b \le \epsilon-\xi_n^ \land,\ \xi_n^ \vee \ge 0,\ \xi_n^ \land \ge 0</script>

<p>这就构成 SVR 的原始问题: minimize regularizer + (upper tube violations $\xi_n^ \land$ and lower violations $\xi_n^ \vee$)。这里参数 $C$ 就是 trade-off of regularization and tube violation. 现在要求 SVR 的对偶问题。</p>

<ul>
  <li>Lagrange multiplier $\alpha_n^ \vee\ for\ -\epsilon-\xi_n^ \vee \le y_n - w^Tz_n-b$</li>
  <li>Lagrange multiplier $\alpha_n^ \land\ for\ y_n - w^Tz_n-b \le \epsilon-\xi_n^ \land$</li>
</ul>

<p>一些KTT条件如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-1/97895297.jpg" width="450px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-1/34361452.jpg" width="500px" /></p>

<p>在 tube 中我们可以得到 </p>

<p>$\vert w^Tz_n + b- y_n \vert &lt; \epsilon$</p>

<p>$\to \xi_n^ \vee=0,\ \xi_n^ \land=0$</p>

<p>$\to (\epsilon + \xi_n^ \land - y_n + w^Tz_n +b) \neq 0,\ (\epsilon + \xi_n^ \vee + y_n - w^Tz_n -b) \neq 0$</p>

<p>$\to \alpha_n^ \vee =0,\ \alpha_n^ \land=0$</p>

<p>$\to \beta_n=0$</p>

<p>对于support vector来说，$\beta_n \neq 0$，是在tube上或者tube外的。综上可以得到sparse $\beta$。</p>

<u>总结得到的Linear/Kernel Model如下图：</u>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-1/58915472.jpg" width="450px" /></p>

<ul>
  <li>first row: less used due to worse performance</li>
  <li>second row: popular in <strong>LIBLINEAR</strong></li>
  <li>third row: less used due to dense $\beta$</li>
  <li>fourth row: popular in <strong>LIBSVM</strong></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Support Vector Machine]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/27/svm/"/>
    <updated>2016-02-27T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/27/svm</id>
    <content type="html"><![CDATA[<p>支持向量机(support vector machine, SVM)是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，可以形式化为凸二次规划问题的求解，也等价于正则化的合页损失函数的最小化问题。SVM还包括kernel trick，使得它可以成为实质上的非线性分类器。下面就介绍Perceptron到三种类型的SVM模型。</p>

<p><img src="http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/figs/svm2.PNG" width="400px" /></p>

<!--more-->

<h2 id="perceptron">Perceptron</h2>

<p>SVM可以说是Perceptron的一种进化。什么是Perceptron，Wiki的解释如下：</p>

<blockquote>
  <p>the perceptron is an algorithm for supervised learning of binary classifiers</p>
</blockquote>

<p>实则是一个二元线性分类器，通过一个线性的预测函数将观察点分成两类，这个预测函数就是特征空间中的一个分离超平面。对应于方程 $wx+b=0$, $w$ 为法向量或者权值，$b$ 是截距或偏置。</p>

<p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_i＝+1$ 的实例 $i$，有 $wx_i+b&gt;0$，对所有 $y_i＝-1$ 的实例 $i$，有 $wx_i+b&lt;0$，则称数据集为线性可分数据集（linearly separable data set）;否则，称数据集线性不可分。</p>

<p>对于误分类点有 $y_i(wx_i+b) &lt; 0$, 设误分类点集 $M$, 采用0/1损失函数, 则得到感知机的损失函数如下：</p>

<script type="math/tex; mode=display"> L(w, b) = -\sum_{x_i \in M} y_i(w x_i + b) </script>

<p>要使损失函数最小，可以采用随机梯度下降法。任意选取一个超平面 $w_0, b_0$，然后用梯度下降法不断地最小化目标函数，每次选取一个误分类点使其梯度下降。每次迭代如下，随机选取一个误分类点 $(x_i, y_i)$, 对 $(w, b)$ 更新, $\eta$ 为步长：</p>

<script type="math/tex; mode=display"> w \gets w + \eta y_ix_i </script>

<script type="math/tex; mode=display"> b \gets b + \eta y_i </script>

<u>下面我们来证明下经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。</u>

<p>存在超平面 $y_i(\hat{w}_{opt} \cdot \hat{x}_i) = w_{opt} \cdot x_i + b_{opt} = 0$，使 $|\hat{w}_{opt}| = 1$，那么可以对于任意的点 $i$，$y_i(\hat{w}_{opt} \cdot \hat{x}_i) &gt; 0$，所有存在 $\gamma$，使得</p>

<script type="math/tex; mode=display">y_i(\hat{w}_{opt} \cdot \hat{x}_i) = w_{opt} \cdot x_i + b_{opt} \ge \gamma</script>

<p>感知机算法从 $\hat{w}_0 = 0$ 开始，如果实例被误分类，则更新权重。设 $\hat{w}_{k-1}$ 是第 $k$ 个误分类点之前扩充的权值向量，则第 $k$ 个误分类点满足$y_i(\hat{w}_{k-1} \cdot \hat{x}_i) \le 0$，$\hat{w}$ 更新后有</p>

<script type="math/tex; mode=display"> \hat{w}_{k} = \hat{w}_{k-1} + \eta y_i \hat{x}_i </script>

<p>可以得到</p>

<script type="math/tex; mode=display"> \hat{w}_{k} \cdot \hat{w}_{opt} = \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta y_i \hat{w}_{opt} \cdot \hat{x}_i \ge \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma </script>

<script type="math/tex; mode=display">\to \hat{w}_{k} \cdot \hat{w}_{opt} \ge k \eta \gamma</script>

<p>另外假设 $R = max(|\hat{x}_i|)$，有</p>

<script type="math/tex; mode=display"> \|\hat{w}_{k}\|^2 = \|\hat{w}_{k-1}\|^2 + 2\eta y_i \hat{w}_{k-1} \cdot \hat{x}_i + \eta^2 \|\hat{x}_i\|^2 \le \|\hat{w}_{k-1}\|^2 + \eta^2 \|\hat{x}_i\|^2 \le \|\hat{w}_{k-1}\|^2 \eta^2 R^2 </script>

<script type="math/tex; mode=display">\to \|\hat{w}_{k}\|^2  \le k \eta^2 R^2 </script>

<p>我们可以得到 </p>

<script type="math/tex; mode=display">k\eta\gamma \le \hat{w}_{k} \cdot \hat{w}_{opt} \le \|\hat{w}_{k}\| \|\hat{w}_{opt}\| \le \sqrt{k}\eta R</script>

<p>于是 $k \le (R / \gamma)^2$，表示误分类次数 $k$ 是有上界的，也就是经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。</p>

<h3 id="pocket">pocket</h3>

<p>这里想另外介绍一种算法，Pocket算法。当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。 Pocket算法也就是用来权衡分离超平面和误分类点的。</p>

<p>从直觉上，我们知道如果当前超平面犯错越少越好，Pocket本质上就是在改错的时候多做一步，判断当前改正犯的错是否比之前更小，也就是贪心选择。</p>

<p>方法如图所示：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/79944104.jpg" width="500px" /></p>

<h2 id="linear-support-vector-machine">Linear Support Vector Machine</h2>

<p>Perceptron的问题是什么？它存在许多种解，只要是能够分割观察点的超平面全是它的解，既依赖于初值的选择，也依赖于迭代过程中误分类点的选择。这样的算法带来了不稳定性。为了得到唯一的超平面，需要增加一些约束条件。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/18916886.jpg" width="400px" /></p>

<p>我们认为点距离超平面越远，则越能够容忍噪声，并且对于overfitting的安全边界更大，也就是置信度越大。所以希望能够找到一个距离观察点最远的超平面作为我们的解，这个距离称为Margin，这样的超平面是唯一的。</p>

<p>点到平面的距离为沿着法向量方向的距离，所以有</p>

<script type="math/tex; mode=display"> distance(x, b, w) = \frac{1}{\|w\|} \vert w^Tx + b \vert </script>

<p>顺便提下，上式对法向量进行规范化，使得法向量为单位法向量，这个距离称<strong>几何间隔</strong>，否则是<strong>函数间隔</strong>。接下来，我们想要优化的目标可以写作：</p>

<script type="math/tex; mode=display"> \underset{b,w}{max} \frac{1}{\|w\|} </script>

<script type="math/tex; mode=display">s.t.\ every\ y_n(w^T x_n + b) > 0, \underset{n=1,2..N}{min} y_n(w^T x_n + b) = 1</script>

<p>这里我们有对distance进行缩放，除以 $w^Tx + b$。进一步对问题优化我们得到：</p>

<script type="math/tex; mode=display"> \underset{b,w}{max} \frac{1}{2}w^T w </script>

<script type="math/tex; mode=display">s.t.\ y_n(w^T x_n + b) \ge 1\ for\ all\ n</script>

<p>这样的一个问题其实就是<strong>凸二次规划问题</strong>。可以看下凸二次规划的解法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/59800326.jpg" width="500px" /></p>

<p>使用任意一款可以解决二次规划的语言包就可以套用上图解决我们的目标问题。这个时候解出来的 $b，w$ 称之为hard-margin，因为没有任何一个点违反我们的限制条件。后面我们会看到一个soft-margin，这个就类似于pocket之于perceptron，可以解决线性不可分的数据集。</p>

<p>对于少量的数据集可以用凸二次规划直接求解，但是数据量一旦增多，求解的速度就成问题。我们可以用拉格朗日乘子法求解原始问题的对偶问题，得到最优解。定义的拉格朗日函数为：</p>

<script type="math/tex; mode=display"> L(b, w, \alpha) = \frac{1}{2}w^Tw + \sum_{n=1}^N \alpha_n(1-y_n(w^T z_n +b)) </script>

<p>这里的 $z_n = \phi(x_n)$ 是为了表示可以对 $x_n$ 做非线性的转换，也就是下一章中提到的kernel function，这里可以直接理解为 $z_n=x_n$。可以这么理解上式：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/6390373.jpg" width="450px" /></p>

<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题，下面我们需要证明：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/47082134.jpg" width="400px" /></p>

<p>假设对于任意的 $\alpha’$，所有的 $\alpha_n’ \ge 0$, 那么</p>

<script type="math/tex; mode=display"> \underset{b,w}{min}(\underset{\alpha_n \ge 0}{max}\ L(b, w, \alpha)) \ge \underset{b,w}{min}\ L(b, w,\alpha’)</script>

<p>因为 $max \ge any$。则对于右式的最优解 $\alpha’$ 有 $best \in any$</p>

<script type="math/tex; mode=display"> \underset{b,w}{min}(\underset{\alpha_n \ge 0}{max}\ L(b, w, \alpha)) \ge \underset{\alpha_n' \ge 0}{max}\underset{b,w}{min}\ L(b, w,\alpha’)</script>

<p>对于大于等于符号来说，这是一个weak duality。如果等号成立则是strong duality，也就是对偶问题和原始问题的最优值相等。需要满足一些限制条件，那就是<a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT条件</a>。</p>

<p>求解对偶问题，首先求 （1）$\underset{b,w}{min}\ L(w, b, \alpha)$ </p>

<script type="math/tex; mode=display"> \nabla_w L(w, b, \alpha) = w - \sum_{i=1}^N \alpha_i y_i z_i = 0 \to w=\sum_{i=1}^N \alpha_i y_i z_i</script>

<script type="math/tex; mode=display">\nabla_b L(w, b, \alpha) = \sum_{i=1}^N \alpha_i y_i = 0 \to \sum_{i=1}^N \alpha_i y_i=0</script>

<p>带入原公式得到</p>

<script type="math/tex; mode=display"> L(w, b, \alpha)= -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_i y_j (z_i \cdot z_j) + \sum_{i=1}^N \alpha_i </script>

<p>接下来求解 （2） $\underset{b,w}{min}\ L(w, b, \alpha)$ 对 $\alpha$ 的极大值，极大值可以变为极小值</p>

<script type="math/tex; mode=display"> \underset{\alpha}{min}\ \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_i y_j (z_i \cdot z_j) - \sum_{i=1}^N \alpha_i</script>

<script type="math/tex; mode=display"> s.t.\ \sum_{i=1}^N \alpha_i y_i=0,\ \alpha_i \ge 0</script>

<p>求解（2）可以用到<a href="http://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2591592.html">SMO</a>（序列最小最优化）算法。现在回过头来看 $\alpha$ 的最优解，我们发现至少会有一个 $\alpha_j &gt; 0$，因为根据KKT条件有complementary slackness：</p>

<script type="math/tex; mode=display"> \alpha_i(y_i(w \cdot z_i +b)-1) = 0,\ i=1,2,...N</script>

<p>如果 $\alpha=0$ 则会导致 $w=0$，对于$\alpha_j &gt; 0$，我们看到会有 $y_j(w \cdot z_j +b)-1=0$。如此，可以定义分离超平面为</p>

<script type="math/tex; mode=display"> \sum_{i=1}^N \alpha_i y_i (z \cdot z_i) + b = 0 </script>

<script type="math/tex; mode=display"> b = y_i - \sum_{i=1}^N \alpha_i y_i (z_i \cdot z_j)</script>

<p>上面的推导表明，这些点 $(z_j, y_j)$ 也就是站在分离超平面的margin上的点，所以说SVM只依赖于边界上的点，它们被称为support vectors，支持向量，这也是SVM的由来。</p>

<h2 id="kernel-support-machine">Kernel Support Machine</h2>

<p>在上文中，我们已经了解到了SVM处理线性可分的情况，而对于非线性的情况，SVM 的处理方法是选择一个核函数 $K(⋅,⋅)$，<u>通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题</u>。</p>

<p><img src="http://my.csdn.net/uploads/201206/02/1338612063_1634.JPG" width="400px" /></p>

<p>例如，对于上面的数据集中两类数据，分别分布为两个圆圈的形状，这样的数据本身就是线性不可分的。理想的分界应该是一个二次曲面，可以写成：</p>

<script type="math/tex; mode=display">a_1X_1 + a_2X_1^2 + a_3X_2 + a_4X_2^2 + a_5X_1X_2 + a_6 = 0</script>

<p>上式其实就是一个五维的空间。一般地对于二次多项式的转换形式，我们有</p>

<script type="math/tex; mode=display"> \phi_2(x) = (1, x_1, x_2....x_d, x_1^2, x_1x_2,...x_1x_d,x_2x_1, x_2^2...x_d^2) </script>

<p>在这个 $O(d^2)$ 高维空间做计算无疑非常困难。幸运的是从上一章推导出的分离超平面中，我们可以看到<strong>分类决策其实只依赖输入和训练样本输入的内积</strong>，也就是说，我们可以直接计算 </p>

<script type="math/tex; mode=display"> \phi_2(x)^T \phi_2(x') = 1 + \sum_{i=1}^dx_ix_i' +  \sum_{i=1}^d\sum_{j=1}^d x_ix_j'x_ix_j' = 1 + x^Tx' + (x^Tx')^2</script>

<p>这里我们直接计算高维转换后的内积，有什么好处呢？注意到计算可以在原来的低维空间$O(d)$中发生，不需要再高维空间中计算。这样，称呼<strong>计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数</strong>。可以对原有空间进行一些线性变换，得到</p>

<script type="math/tex; mode=display"> \phi_2(x) = (1, \sqrt{2\gamma}x_1,....\gamma x_d^2) \to K_2(x, x')=1 + 2\gamma x^Tx' + \gamma^2 (x^Tx')^2 </script>

<p>推广之后，我们得到一般的多项式Kernel：</p>

<script type="math/tex; mode=display">K_n(x, x')= (\xi + \gamma x^Tx')^n,\ \xi>0,\gamma>0</script>

<p>对于不同的 $\xi,\gamma$ SVM是不同的，它们的支持向量也是不同的，因为对于SVM来说变化Kernel就意味着重新定义margin。下面是二次多项式Kernel的一些例子：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/72039507.jpg" width="450px" /></p>

<p>其他常用的核函数还包括高斯核，它可以把原来的低维空间扩展到无线大的高维空间中，它的一般公式为 $K(x,x’)=exp(-\gamma |x-x’|^2),\gamma&gt;0$。高斯核函数也被称为 Radial Basis Function(RBF) kernel。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/61331101.jpg" width="500px" /></p>

<p>满足核函数的充要条件是Mercer’s condition，包括：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/48999470.jpg" width="450px" /></p>

<h2 id="soft-margin-support-vector-machine">Soft-Margin Support Vector Machine</h2>

<p>下面我们来讨论下 Soft-Margin SVM。Soft-Margin可以支持数据集线性不可分的情况，允许一些误分类点的存在，在目标优化函数上会对这些误分类点增加处罚：</p>

<script type="math/tex; mode=display"> \underset{b,w,\xi}{min} \frac{1}{2}w^T w  + C\sum_{n=1}\xi_n</script>

<script type="math/tex; mode=display">s.t.\ y_i(w^T z_n + b) \ge 1-\xi_n,\ \xi_n \ge 0\ for\ all\ n</script>

<p>对于参数 $C$ 而言，它是大margin和误分类点的trade-off，大 $C$ 表示少误分类点，小 $C$ 表示大margin。同样计算拉格朗日对偶问题：</p>

<script type="math/tex; mode=display">L(b, w, \xi, \alpha, \beta) = \frac{1}{2}w^Tw + C\sum_{n=1}\xi_n + \sum_{n=1}^N \alpha_n(1-\xi_n-y_n(w^T z_n +b)) + \sum_{n=1}^N \beta_n (-\xi_n)</script>

<script type="math/tex; mode=display">want\ \underset{\alpha \ge 0, \beta \ge 0}{max}(\underset{b,w,\xi}{min}\ L(b, w, \xi, \alpha, \beta))</script>

<p>和hard-margin一样的计算后可以得到</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/23447671.jpg" width="350px" /></p>

<p>对于soft-margin的complementary slackness有</p>

<script type="math/tex; mode=display"> \alpha_n(1- \xi_n - y_n(w^T z_n +b)) = 0,\ (C-\alpha_n)\xi_n=0</script>

<p>存在以下三种情况：</p>

<ul>
  <li>$\alpha_n=0$: $\xi_n=0$, 在边界之外正确分类的点.</li>
  <li>$0&lt;\alpha_n&lt;C$: $\xi_n=0$，支持向量落在边界上。也正是通过这种情况计算截距 $b$.</li>
  <li>$\alpha_n=C$：这种情况比较复杂，可以有下图表示，支持向量的位置由$\xi_n$决定。$0&lt;\xi_n&lt;1$则分类正确，在间隔边界和分离超平面之间；$\xi_n=1$则在分离超平面上；$\xi_n&gt;1$则位于超平面误分类的一侧.</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/76271972.jpg" width="300px" /></p>

<h2 id="hinge-loss-function">Hinge Loss Function</h2>

<p>最后我们说明下SVM可以用合页损失函数表示，就是最小化以下目标函数：</p>

<script type="math/tex; mode=display"> \sum_{i=1}^N[1-y_i(w \cdot x_i + b)]_+ + \lambda\|w\|^2 </script>

<p>下标“+”表示，对 $[z]_+ = z,\ z&gt;0; [z]_+ = 0,\ z \le 0$</p>

<p>可以看到 $1-y_i(w \cdot x_i + b)$ 可以写成</p>

<script type="math/tex; mode=display"> y_i(w \cdot x_i + b) \ge 1-\xi_i,\ \xi_i \ge 0, \ i=1,2,...N </script>

<p>也就是soft-margin SVM的限制条件，那么取 $\lambda= 1/2C$ 则有</p>

<script type="math/tex; mode=display"> \underset{b,w}{min} \frac{1}{C}(\frac{1}{2}\|w\|^2 + C\sum_{n=1}\xi_n)</script>

<p>也就是soft-margin SVM的目标优化函数。合页损失函数的形状如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/97100573.jpg" width="400px" /></p>

<p>虚线显示的是感知机的损失函数 $[y_i(w \cdot x_i+b)]_+$。这时，当样本点 $(x_i，y_i)$ 被正确分类时，损失是0，否则损失是 $-y_i(w \cdot x_i+b)$。相比之下，合页损失函数不仅要分类正确，而且置信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PCA and SVD]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/26/pca-svd/"/>
    <updated>2016-02-26T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/26/pca-svd</id>
    <content type="html"><![CDATA[<p>PCA即为（Principal Components Analysis）主成分分析，SVD是（Singular value decomposition）奇异值分解。从字面上的理解就可以看出这两个并不是在同一语义层面的东西。之所以把这两个放在一块，一是为了文章的完整性，二是为了说明二者在数据转换（基转换）上的共性。</p>

<!--more-->

<h2 id="principal-components-analysis">Principal Components Analysis</h2>

<p>由于采样的受限，我们的观察值并不能最有效的反应出事物的特征，因此我们希望大而全的收集能收集到的所有数据。但是这里面存在两个问题：噪声和冗余。因此在描述事物的时候，我们希望能够排除这些多余的甚至是错误的数据，得到最简洁，最省力的数据。</p>

<p>那么什么是最简洁，最省力的数据呢？把我们的观察值想象成一个向量空间，排除噪声和冗余后，那么这个空间上的点应该可以用一系列的正交单位向量缩放后的向量和表示。这个就是PCA的目的。</p>

<p>在上面的描述中，有一些很重要的假设：</p>

<ul>
  <li>原有的基是通过线性转化转化为现有空间的基，否则就是Kernel PCA</li>
  <li>现有空间的基是正交的</li>
  <li>每个维度数据的均值和方差是充分统计的。</li>
  <li>数据中方差能够表示数据的重要程度。</li>
</ul>

<p>第一点比较容易理解，其实是做了一些限制，限制潜在最优基的数目并且相信数据集存在线性的连续性，即我们可以用线性的方式内推出独立的数据点。第二点就比较直接，直觉上是合理的并且可以用线性代数的矩阵分解解决。</p>

<p>第三点比较复杂，充分统计的意思是可以用均值和方差完整的描述数据的概率分布。如果方差为0，只用方差完整的描述概率分布的只有高斯分布。也就是说维度上的数据服从高斯分布，如果不服从呢，这就涉及到ICA算法（Independent Component Analysis）。根据中心极限定理，PCA还是比较robust的一种解决方案。</p>

<p>第四点来自信号处理，认为信号具有较大的方差，噪声有较小的方差，信噪比(Signal-to-noise ratio, SNR)就是信号与噪声的方差比，越大越好。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-24/39988407.jpg" width="600px" /></p>

<p>噪声可以用<em>SNR</em>量化表示，那么冗余呢？冗余可以用协方差表示。如果向量 $a$、向量 $b$ 的协方差为0，则代表 $a$ 和 $b$ 完全没有关系，协方差越大则关联的程度越高。<u>这里对所有的数据集都是*mean deviaton form*，即均值为0。</u>则我们可以得到向量 $a$ 和向量 $b$ 的协方差 $\delta_{ab}^2 = \frac{1}{n-1}ab^T$，之所以除以 $n-1$ 是为了得到无偏估计，因为样本中的最后一个值可以通过均值推到出来。</p>

<p>假设原有的输入是一个 $m \times n$ 的矩阵，m是特征维度，n是样本数量，那么它的协方差矩阵为 $S_x = \frac{1}{n-1}XX^T$. $S_x$ 是 $m \times m$ 的矩阵，<strong>表示特征之间的关联程度</strong>。$S_x$ 量化了原有任意两个维度数据之间的关系。那么既然如此，我们希望 $S_x$ 是什么样的？答案就是非对角元素全为0，表示任意维度之间的数据没有冗余，这个过程叫对角化（Diagonalize）得到的协方差矩阵我们成为 $S_y$。</p>

<p>有很多种方法可以实现对角化，PCA选择特征值分解的方法。现在问题定义如下，找到一个正交矩阵 $P$，$Y=PX$, 使得 $S_y = \frac{1}{n-1}YY^T$ 是对角化的矩阵。这时，$P$ 的每排就是 $X$ 的 principal components。这也是PCA名字的由来，$Y$ 是经过矩阵 $P$ 线性转换后的矩阵。$S_y$ 用 $P$ 表示：</p>

<script type="math/tex; mode=display"> S_y = \frac{1}{n-1}YY^T = \frac{1}{n-1}PXX^TP^T = \frac{1}{n-1}PAP^T </script>

<p>这里 $A=XX^T$ 是一个 $m \times m$  的对称矩阵。对称矩阵可以由特征向量构成的正交矩阵表示 $A=EDE^T$，$D$ 是对角矩阵，$E$ 的列向量为 $A$ 的特征向量。如果 $P=E^T$，即 $P$ 的行向量为 $A$ 的特征向量，则有</p>

<script type="math/tex; mode=display">S_y = \frac{1}{n-1}PAP^T = \frac{1}{n-1}(PP^T)D(PP^T) = \frac{1}{n-1}D</script>

<p>可以看到 $P$ 对角化 $S_y$，这就是PCA要求的。下面我们总结下</p>

<ul>
  <li>$X$ 的<strong>主成分</strong>也就是 $XX^T$ 的特征向量，或者 $P$ 的行向量。</li>
  <li>$S_y$ 的第 $i$ 个对角值（特征值）也就是 $X$ 在 $p_i$方向上的方差。</li>
</ul>

<p>所以PCA的计算很简单，就两个步骤</p>

<ol>
  <li>计算dataset的<em>mean deviaton form</em></li>
  <li>计算$XX^T$的特征分解。</li>
</ol>

<p>PCA可以选择最大的几个特征值降维，也可以防止overfitting，但是经过线性变换后拟合的函数就不好理解了。</p>

<h2 id="singular-value-decomposition">Singular value decomposition</h2>

<p>SVD是另外一种基变换的更通用的方法，二者在使用上通常可以互相的替换。SVD和上文中提到的特征值分解都是一种矩阵的对角化分解方法。</p>

<p>假设 $X$ 是任意 $m \times n$ 矩阵，$XX^T$ 是秩为 $r$ 的对称矩阵，我们定义</p>

<ul>
  <li>$(v_1, v_2,…v_r)$ 是 $XX^T$ 的 $n \times 1$ 的特征向量，特征值为 $(\lambda_1, \lambda_2,…\lambda_r)$, $(XX^T)v_i = \lambda_i v_i$。</li>
  <li>$\sigma_i = \sqrt{(n-1) \lambda_i}$ 为正实数，也被成为奇异值。</li>
  <li>$(u_1, u_2,…u_r)$ 是 $m \times 1$ 的正交向量集，$u_i = \frac{1}{\sigma_i}Xv_i$</li>
</ul>

<p>重新组织下第三个定义，有 $Xv_i=\sigma_iu_i$, $U = (u_1, u_2,…u_r)，V = (v_1, v_2,…v_r)$ 都是定义在 $r$ 维空间的正交基。用任意的 $(m-r), (n-r)$ 正交向量补充到 $U, V$ 中，得到 $m$ 和 $n$ 维的 $U、V$，我们有下面用一个矩阵乘法：$XV=U \Sigma$, 其中</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-24/86503163.jpg" width="200px" /></p>

<p>因为 $V$ 是正交的，所以上式又可以写成</p>

<script type="math/tex; mode=display"> X=U \Sigma V^T </script>

<p>这个就是奇异值分解，表示任意的矩阵都可以表示一个正交矩阵，一个对角矩阵和另一个正交矩阵的乘积，或者说是旋转、拉伸和另外一个旋转。其中，$U_{m \times m}$ 是左奇异向量矩阵，$V_{n \times n}$ 是右奇异向量矩阵。</p>

<p>关于奇异值分解的问题最常用的算法分为两大类，QR分解和Jacobi选择，这里就不细说。</p>

<h3 id="pcasvd">PCA和SVD的关系</h3>

<p>通常来说，PCA要求计算协方差矩阵的特征值和特征向量，因为协方差矩阵是对称的，因此是可对角化的，特征向量也是正交的。对于SVD，我们有</p>

<script type="math/tex; mode=display">XX^T=(U\Sigma V)(U\Sigma V)^T = U \Sigma^2 U^T</script>

<p>复习下介绍PCA时我们的到的公式：</p>

<script type="math/tex; mode=display">XX^T = (n-1)P^TS_yP</script>

<p>这时二者的关系就很清晰了，$P、U$ 都是正交矩阵：</p>

<ul>
  <li>
    <p>$XX^T$ 的特征值 $\lambda=\frac{\sigma^2}{n-1}$</p>
  </li>
  <li>
    <p>$\frac{1}{\sqrt{n-1}}X$ 经过SVD分解后的 $U$ 的列向量也正是PCA中的主成分。</p>
  </li>
</ul>

<p>所以我们在PCA的最后一步中可以用SVD或者特征分解。但是SVD在数值上的精确程度会高于特征分解，因为计算 $XX^T$ 可能会带来一些精度的损失。</p>

<h3 id="svd">SVD的说明</h3>

<p>可以对SVD进行一些有趣的变换:</p>

<script type="math/tex; mode=display"> U^TX = \Sigma V^T </script>

<script type="math/tex; mode=display"> U^TX = Z </script>

<p>定义 $Z=\Sigma V^T$ 可以看到 $U^T$ 是改变了 $X$ 的基，使其变成 $Z$, 这里是改变了 $X$ 的列向量。同理对于 $V$ 来说，$V^TX^T = U^T \Sigma$ 这是改变 $X$ 的行向量。而 $\Sigma$ 则表示对某些维度的缩放，之所以说某些维度是 $\Sigma$ 中有为0的奇异值，非0奇异值的个数也就是矩阵的秩的大小。</p>

<p>$\Sigma$中奇异值的大小和特征值有关系，表示特征的重要程度，因此我们可以令奇异值较小的数0，这样重新计算 $X$ 的时候也就进行降噪和去冗余。</p>

<p>总的来说，无论是特征分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。通过特征分解和奇异值分解我们可以更加明白这些矩阵信息背后的真实含义，简化我们对矩阵的认识。</p>

<p>关于更多对SVD物理意义的说明可以参考<a href="http://www.ams.org/samplings/feature-column/fcarc-svd">http://www.ams.org/samplings/feature-column/fcarc-svd</a>.</p>

<h2 id="limits-and-extensions-of-pca">Limits And Extensions of PCA</h2>

<p>可以看到PCA是无参数分析的，所以只需要做出上文提到的假设，无需对参数进行训练和选择就可以得到结果。但是也正是上述假设所限，如果一个人正好知道数据的一些先验知识，那么他也无法通过这些先验知识得到更好的分析结果，这个时候如果能够将这些先验知识融入有参数的算法中会得到更好的结果。</p>

<p>如果这个先验知识表示需要做些数据的非线性转化（kernel transformation），那么这样的参数算法就是<code>kernel PCA</code>。</p>

<p>有时候需要作出如下假设，主成分不必正交，特征数据的分布也不是高斯分布。那么可以用<code>Idependent Component Analysis</code>解决，它和PCA有同意的目的，降噪和去冗余。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ensemble Methods]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/18/ensemble/"/>
    <updated>2016-02-18T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/18/ensemble</id>
    <content type="html"><![CDATA[<p>Ensemble<code>|ɒnˈsɒmbl|</code> Methods 称为集成方法，它还有其他类似的名字，meta-algorithm、aggregation model，这些都代表这同一个意思，就是不同弱分类器的组合成一个强分类器。这里的弱分类器要比随机猜测的结果好，错误率小于50%；弱分类器可以是决策树、逻辑回归、朴素贝叶斯等算法。Ensemble的形式有很多种：</p>

<ul>
  <li>不同算法的集成;</li>
  <li>同一算法在不同设置下的集成;</li>
  <li>数据集不同部分分配给不同分类器之后的集成。</li>
</ul>

<p>那么这多个弱分类器又是如何组合的呢，下面给出一个big picture，后面的文章也是对其的阐述。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-18/38802537.jpg" width="450px" /></p>

<!--more-->

<p>弱分类的组合可以是linear或者stacking的，linear又可以是uniform或者non-uniform.
stacking或者stacked generalization的大意是non-linear combining. 通常stacking的组合算法有LR，GBM, KNN, NN, RF 和 ET，可以参考<a href="http://mlwave.com/kaggle-ensembling-guide/">http://mlwave.com/kaggle-ensembling-guide/</a>. 作者Wolpert是这样形容的：</p>

<blockquote>
  <p>stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn’t duplicated in what the other generalizer’s have to say), the better the resultant stacked generalization. </p>
</blockquote>

<p>那么为什么要把这些分类器进行组合呢？组合之后是否能获得更好的效果？可以看下下面的例子。</p>

<p>假设我们有10个samples的测试集，正确的结果是<code>1111111111</code>。现在有三个分类器，它们只有70%的正确率，那么三个分类器进行majority vote，可以得到如下的正确率：</p>

<script type="math/tex; mode=display">0.7 * 0.7 * 0.7 + \binom{3}{2}0.7 * 0.7 * 0.3 = 0.784</script>

<p>也就是由原来的70%提升到了78%。正确率会随着分类器的增加而增加。5个分类器的正确率大约为83%。这个在统计学上就是“Wisdom of Crowds”。但是这个结果提高的前提在于sample的diversity，也就是减少sample之间的correlation。例如：</p>

<pre><code>1111111100 = 80% accuracy
1111111100 = 80% accuracy
1011111100 = 70% accuracy
</code></pre>

<p>在这个例子中得到<code>1111111100</code>还是只有80%的正确率，而</p>

<pre><code>1111111100 = 80% accuracy
0111011101 = 70% accuracy
1000101111 = 60% accuracy
</code></pre>

<p>经过Ensemle就可以得到<code>1111111101</code>，90%的正确率。</p>

<p>那么如何证明多个组合会比单个的结果好呢，可以用Uniform Linear的组合进行下面的理论描述。</p>

<script type="math/tex; mode=display"> Let\ G(x) = \frac{1}{T}\sum_{t=1}^T g_t(x)</script>

<script type="math/tex; mode=display">   avg((g_t(x) - f(x))^2) = avg(g_t^2 - 2g_tf + f^2)</script>

<script type="math/tex; mode=display">   =avg(g_t^2) - G^2 + (G-f)^2</script>

<script type="math/tex; mode=display">   =avg((g_t-G)^2) + (G-f)^2</script>

<script type="math/tex; mode=display"> avg(E(g_t)) = avg((g_t-G)^2) + E(G) \ge E(G) </script>

<p>更一般的，有如下的预测模型$\hat{F}(x)^T = [\hat{f}_1(x), \hat{f}_2(x)…\hat{f}_M(x)]$, 用最小二乘法寻找线性最小值</p>

<script type="math/tex; mode=display"> \hat{w} = argmin_w E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 </script>

<script type="math/tex; mode=display"> \hat{w} = E[\hat{F}(x)\hat{F}(x)^T]^{-1}E[\hat{F}(x)Y] </script>

<script type="math/tex; mode=display"> E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 \le E[Y-\hat{f}_m(x)]^2 \forall m</script>

<h2 id="bagging">Bagging</h2>

<p>Bagging或者bootstrap aggregation是上文提到的Uniform Linear aggregation。其中用到bootstrapping，这是是一种resample的方法，定义如下</p>

<blockquote>
  <p>re-sample N examples from original sample <strong>uniformly with replacement</strong> – can also use arbitrary N’ instead of original N</p>
</blockquote>

<p>有training set $Z$, 对每个bootstrap sample $Z^{*b}, b=1,2…B$，bagging定义如下：</p>

<script type="math/tex; mode=display"> \hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x) </script>

<h2 id="adaboost">AdaBoost</h2>

<p>AdaBoost或者Adaptive Boosting中只要弱分类器的正确率优于随机选择，那么通过AdaBoost就会得到非常好的结果。它是一种Boosting方法，所谓Boosting就是改变训练数据的概率分布（权值分布）针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。对应于上面的non-uniform linear model。</p>

<p>AdaBoost强调对错误分类的反复学习，但是最后对错误率较高的分类器赋予低权重。每轮学习中都会重新对分类器赋予不同的权重，这是为了得到更多关于数据的不同假设。如何得到更多的不同假设呢？</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-19/91708853.jpg" width="300px" /></p>

<p>那么我们希望在$t+1$迭代学习的时候，$t$轮的结果尽可能的随机，即有错误率</p>

<script type="math/tex; mode=display"> \epsilon_t = \frac{\sum_{n=1}^N u_n^{(t+1)}I(y_n \ne g_t(x_n))}{\sum_{n=1}^N u_n^{(t+1)}} = \frac{1}{2} </script>

<p>于是可以 multiply incorrect $\propto (1 - \epsilon_t)$; multiply correct $\propto \epsilon_t$</p>

<p>定义scalling factor $\blacklozenge_t = \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}}$</p>

<script type="math/tex; mode=display"> incorrect \gets incorrect \cdot \blacklozenge_t \\ correct \gets correct \div \blacklozenge_t </script>

<p>最后对scalling factor取自然对数作为权值 $\alpha_t$ 将弱分类器线性组合在一块，取自然对数的逻辑如下：</p>

<script type="math/tex; mode=display"> \epsilon_t = 1/2 => \blacklozenge_t = 1 => \alpha_t = 0 \\
\epsilon_t = 0 => \blacklozenge_t = \infty => \alpha_t = \infty </script>

<p>最后伪代码为</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/40495628.jpg" width="450px" /></p>

<p>比较常见的弱分类器是Decision Stump，和AdaBoost组成<code>AdaBoost-Stump</code>，具有efficient feature selection and efficiency的特点。</p>

<h2 id="random-forest">Random Forest</h2>

<p>Bagging是通过平均带有噪声但是近似无偏的模型来减少variance，而对于足够深的Decision Tree来说，它的bias可以非常少，但是variance非常高。所以自然的想将二者结合，综合他们的优点。Bagged Tree并不能减少bias，但是可以有效的减少variance。这个Boosting正好相反，Boosting通过自适应的变化树的样子来减少bias。Random Forest就是Bagging + Decision Tree(C&amp;RT)。EST给出RF的本质：</p>

<blockquote>
  <p>The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much.</p>
</blockquote>

<p>我们了解到增加hypothesis diversity可以提高最终结果的表现，那么Random Forest就将这种Random性发挥到极致，得到多样的hypothesis。为了增加随机性，可以做了以下的措施：</p>

<ul>
  <li>re-sample new feature subspace for each b(x) in C&amp;RT, 记得bagging进行data randomness for diversity, 那么在RF中是feature randomness for diversity</li>
  <li>random low-dimensional projections for each b(x) in C&amp;RT, 这就是对feature进行投影，进行feature combination，在特征空间中随机选择若干特征组投影到若干个方向上。</li>
</ul>

<p>伪代码如下（只用了第一个Randomness）：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/2671550.jpg" width="500px" /></p>

<p>RF还有的特点是训练的时候可以自带<strong>Model Selection和Feature Selection</strong>。那么RF是如何做到的？</p>

<h3 id="model-selection">Model Selection</h3>
<p>在RF中使用bagging时，每个bootstrap sample都会有一定的概率没有选择原来sample中的一些数据，这些数据就是out-of-bag (OOB) Samples. 在一个RF中，某个Decision Tree训练没有用到数据 $(x_n, y_n)$的概率为：$(1 - \frac{1}{N}) ^ N$，当N无限大的时候，接近 $\frac{1}{e}$.</p>

<p>可以用OOB来validate G, $E_{oob}G = \frac{1}{N} \sum_{n=1}^N err(y_n, G_n^-(x_n))$, $G_n^-$ 表示 $x_n$在OOB中的Decision Tree。这样可以用 $E_{oob}$ 对bagging/RF进行self-validation。</p>

<p>这有什么用呢，当然是进行模型选择了，可以使用 $E_{oob}$ 进行RF的参数选择，例如feature subspace。下图表示使用Validation中进行的模型选择，可以看到RF中少了re-training的步骤。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/30059076.jpg" width="250px" /></p>

<h3 id="feature-selection">Feature Selection</h3>

<p>在模型训练的时候通常希望能够去掉多余的、无关的特征，这样能够得到的好处有：</p>

<ul>
  <li><strong>efficiency</strong>: simpler hypothesis and shorter prediction time</li>
  <li><strong>generalization</strong>: feature noise removed</li>
  <li><strong>interpretability</strong></li>
</ul>

<p>通常可以通过特征的重要性来进行特征的选择，对于线性模型来说就是 $w_i$ 的绝对值，也就是特征对于最终结果的影响程度。那么在非线性的RF模型中呢？</p>

<p>所用的方法就是random test，例如特征 $i$ 被选择，那么在特征 $i$ 的数据集中加入随机变量重新训练则会降低模型正确率，而对于不重要的特征，怎么改变数据集当然对模型没有什么影响。</p>

<p>RF中使用的random test就是一种常用的统计学工具permutation test，也就是将特征 $i$ 的数据做重新排列。数据表达也就是：</p>

<script type="math/tex; mode=display"> importance(i) = performance(\mathcal{D}) - performance(\mathcal{D}^{(p)}) </script>

<script type="math/tex; mode=display"> \mathcal{D}^{(p)}\ is\ \mathcal{D}\ with\ \{x_{n,i}\}\ replaced\ by\ permuted\ \{x_{n,i}\}_{n=1}^N </script>

<p>$performance(\mathcal{D}^{(p)})$需要重新训练和评估，那么有什么办法可以避免呢？我们可以重新定义 $importance(i) = E_{oob}(G^-) - E_{oob}^{(p)}(G^-)$，表达式后项就是一个permuted OOB value。</p>

<p>具体过程如下，当 $b$ 个树生成的时候，记录OOB评估的 $G^-$ 的正确率，然后OOB sample中的特征 $i$ 数据重新随机排列后再次评估的 $G^-$ 的正确率，二者相减得到特征 $i$的重要性。</p>

<p>RF的缺点是，如果随机过程表现的不稳定，则需要很多的Decision Tree来支持。所以需要重新检查 $G$ 的稳定性来确保有足够多的树。</p>

<h2 id="gradient-boosted-decision-tree">Gradient Boosted Decision Tree</h2>

<p>回忆下假设AdaBoost的分类器输出是binary的，则权值迭代可以转化为</p>

<script type="math/tex; mode=display"> u_n^{t+1} = \begin{cases}{u_n^t \cdot \blacklozenge_t\ if\ incorrect}\\{u_n^t \div \blacklozenge_t\ if\ correct}\end{cases} = u_n^t \cdot \blacklozenge_t^{-y_ng_t(x_n)} = u_n^t \cdot exp(-y_n\alpha_tg_t(x_n))</script>

<script type="math/tex; mode=display"> u_n^{(T+1)} = u_n^{(1)} \cdot \prod_{t=1}^Texp(-y_n\alpha_tg_t(x_n)) = \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) </script>

<p>在AdaBoost中 $G(x) = sign(\sum_{t=1}^T\alpha_tg_t(x_n))$ 括号中的表达式也被成为voting score。现在我们想要 $y_n(voting\ score)$ 为正且越大越好，也就是让 $u_n^{(T+1)}$ 越小越好。</p>

<p>所以AdaBoost的过程也就是让 $\sum_{n=1}^N u_n^{(t)}$ 减小，也就是最小化</p>

<script type="math/tex; mode=display"> \sum_{n=1}^Nu_n^{(T+1)} =  \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) </script>

<p>注意到上式是一个损失函数，Exponential Loss Function，之所以用指数损失函数是为了后续的计算方便，可以对比下不同的损失函数。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-22/70940842.jpg" width="400x" /></p>

<p>注意到在gradient descent中第 $t$ 次迭代有：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/31496263.jpg" width="400px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/14307315.jpg" width="400px" /></p>

<p>所以找到一个好的 $h(function\ direction)$ 函数也就是最小化 $\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$。对于二元分类，$y_n, h(x_n) \in {-1, +1}$，有</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/93309458.jpg" width="400px" /></p>

<p>也就是每次迭代都需要最小化其中的hypothesis $E_{in}^{u^{(t)}}(h)$，那么谁最小化 $E_{in}^{u^{(t)}}(h)$呢，当然是AdaBoost中的reweighted sample所对应的 $g_t$ 了。</p>

<p>所以现在需要优化 $\eta_t$ 得到梯度下降方向最佳步长，原来的 $\hat{E}_{ADA}$ 变为 $(\sum_{n=1}^N u_n^{(t)}) \cdot ((1-\epsilon_t)exp(-\eta) + \epsilon_t exp(+\eta))$。微分后容易得到 $\eta_t=ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}} = \alpha_t$。这和我们上面得到的scaling Factor是一致的。</p>

<p>所以AdaBoost是steepest descent with approximate functional gradient. 我们总结下，AdaBoost的数学表达式：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/16682553.jpg" width="400px" /></p>

<p>Gradient Boost就是把二元分类的假设推广到任意的假设并且损失函数也可以任意的。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/85486720.jpg" width="400px" /></p>

<p>当选择平方损失函数时，有如下的可以看到</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/61222916.jpg" width="460px" /></p>

<p>现在需要对 $h$ 加一些限制，否则 $h(x_n) = -\infty \cdot (s_n - y_n)$</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/34172769.jpg" width="400px" /></p>

<p>最优 $g_t = h$ 就是 $(x_n, y_n-s_n)$ 上的最小二乘回归函数，$y_n - s_n$就是残差。于是原来的Gradient Boost表达式变成：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/37566404.jpg" width="400px" /></p>

<p>最小化 $\eta$ 就是 $(g_t\ transformed\ input,\ residual)$ 上的单变量线性回归。所以GradientBoost for regression 的 $\alpha_t = optimal\ \eta\ by\ g_t\ transformed\ linear\ regression$.</p>

<p>Gradient Boosted Decision Tree也就是使用回归算法为Decision Tree。伪代码如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/38067174.jpg" width="500px" /></p>

<p>总结下Ensemble Methods的有点：</p>

<ul>
  <li>cure underfitting, 通过feature transform加强$G(x)$的Bias。</li>
  <li>cure overfitting, 通过多样化假设的合并达到regularization的目的，减少$G(x)$的variance。</li>
</ul>

]]></content>
  </entry>
  
</feed>
