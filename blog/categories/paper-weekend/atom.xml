<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Paper Weekend | Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/blog/categories/paper-weekend/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-08-08T12:14:34+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Column-Stores vs. Row-Stores]]></title>
    <link href="http://billowkiller.github.io/blog/2016/08/06/mesa/"/>
    <updated>2016-08-06T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/08/06/mesa</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Ashish et al. VLDB 2014</p>
</blockquote>

<p>谷歌Mesa是一个服务于谷歌广告业务的近实时分析型数据仓库，主要用于广告主的报表、内部审计和预测服务。作为底层的存储系统，Mesa能处理PB级的数据，每秒百万的行更新，并且需要应对每日上亿的查询请求，因此它需要满足的设计目标是：</p>

<blockquote>
  <p>near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes.</p>
</blockquote>

<!--more-->

<p>据Google描述，随着他们的广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。为了应付如此持续增长并且重要数据的处理、存储和查询需求，Mesa的具体需求包括：</p>

<ul>
  <li>
    <p><code>原子更新</code>。某一单个的用户行为可能会引起多个关系数据级别的更新，从而影响定义在某个指标集上（例如：点击和成本）跨某个维度集（例如：广告客户和国家）的数千张一致性视图。所以系统状态不会在查询时处于一个只有部分更新生效的状态。</p>
  </li>
  <li>
    <p><code>一致性和正确性</code>。出于业务和法律的原因，该系统必须返回一致和正确的数据。即使某个查询牵涉到多个数据中心，我们仍然需要提供强一致性和可重复的查询结果。</p>
  </li>
  <li>
    <p><code>可用性</code>。系统不允许出现单点故障。不会出现由于计划中或非计划中的维护或故障所造成的停机，即使出现影响整个数据中心或地域性的断电也不能造成停机。</p>
  </li>
  <li>
    <p><code>近实时的更新吞吐率</code>。系统必须支持大约每秒几百万行规模的持续更新，包括添加新数据行和对现有数据行的增量更新。这些更新必须在几分钟内对跨不同视图和数据中心的查询可见。</p>
  </li>
  <li>
    <p><code>查询性能</code>。系统必须对那些对时间延迟敏感的用户提供支持，按照超低延迟的要求为他们提供实时的客户报表，而分批提取用户需要非常高的吞吐率。总的来说，系统必须支持将99%的点查询的延迟控制在数百毫秒之内，并且整体查询控制在每天获取万亿行的吞吐量。</p>
  </li>
  <li>
    <p><code>可伸缩性</code>。系统规模必须可以随着数据规模和查询总量的增长而伸展。举个例子，它必须支持万亿行规模和PB级的数据。但是即使上述参数再出现显著增长，更新和查询的性能必须仍然得以保持。</p>
  </li>
  <li>
    <p><code>在线的数据和元数据转换</code>。为了支持新功能的启用或对现有数据粒度的变更，客户端经常需要对数据模式进行转换或对现有数据的值进行修改。这些变更必须对正常的查询和更新操作没有干扰。</p>
  </li>
</ul>

<p>所有Google现有的大数据技术都无一能满足所有以上的需求。BigTable无法提供<code>原子性和强一致性</code>。而Megastore、Spanner和F1虽然为跨地域复制的数据提供了<code>强一致性</code>的访问，但是他们无法支持Mesa客户端所有需要的<code>峰值更新吞吐率</code>。既有的ROLAP或者MOLAP方法也不能在提供<code>近实时查询</code>同时支持分钟级别的<code>数据更新和聚合</code>。但是Mesa在其不同的基础设施中充分利用了现有的Google技术组件。它使用了BigTable来存储所有<code>持久化的元数据</code>，使用了Colossus (Google的分布式文件系统)来存储数据文件。此外，Mesa还利用了MapReduce来处理连续的数据。</p>

<p>以下有几个技术要点：</p>

<ul>
  <li>为了存储的可伸缩性和可用性，数据是水平分片，并且重复的。</li>
  <li>为了得到一致性和update时候的查询可用性，利用了多版本控制。</li>
  <li>为了更新可伸缩行，数据的更新是批量的和周期性的，并且赋值一个新的版本。</li>
  <li>为了多数据中心数据更新的一致性，Mesa利用基于Paxos的分布式同步协议。</li>
</ul>

<p>Mesa的contributions包括以下一些内容：</p>

<ul>
  <li>高吞吐量的PB级别的数据仓库，同时维持ACID属性以提供事务的处理能力</li>
  <li>新的版本管理方式，利用批量更新，得到低延迟和高吞吐量</li>
  <li>应用数据是通过一个独立和冗余的程序进行异步复制，但是关键的元数据是同步复制。这样可以减少管理副本的同步消耗，并且得到数据更新的高吞吐量</li>
  <li>在线的schema change，并不会影响现有程序的正确性和性能</li>
  <li>软件错误或硬件错误带来的数据损毁的容忍性</li>
</ul>

<h3 id="section">数据模型</h3>

<p>在Mesa的数据是多维度的，定义了细粒度的一个事实表。在这个事实表中分为两种属性，一个是 Key， 一个是 Value。Key是多维的，并且具有层级，例如 date 可以组织成 day, month, year。单个事实表在这些层级之间的聚合可以被物化，这样就支持数据分析的上卷和下钻。Value 也就是一些数据 Metrics。</p>

<p>数据是组织成表的，每个表有 schema 用于指定表结构。schema 里面包含了 Key 空间和 Value 空间，指明他们的类型和聚合方式；另外还包含 aggregation function $F: V \times V \to V$。一般来说聚合方法有 SUM、MAX、MIN和REPLACE；schema 还指明了table的一个或多个索引方法。</p>

<p>数据按对应的索引序排序分割成限定大小的文件，每个文件内部再按行分割成行组row blocks进行压缩存储，每个行组内部的数据在实际存储时，按列式存储的layout进行转换压缩，提高压缩率。而索引文件由行键取固定长度的前缀组成，映射到对应数据行在行组内部的存储偏移量。因为是一个前缀，所以索引可能不能精确定位一行的位置，而是定位这一行在行组内的偏移范围，在通过二分查找的方式在行组内部定位到具体的行。</p>

<h3 id="section-1">查询和更新</h3>

<p>Mesa中存储的数据是多版本的，更新时版本号是向前叠加的，只有处理完当前的版本才会更新下一个版本。这使得当新的更新正在处理时，Mesa可以向用户提供前置状态的<code>一致性数据</code>，也就是 update 的<code>原子性</code>。这种严格的版本顺序还可以保证数据的正确性。</p>

<p>通常，每隔几分钟，上游系统就会执行一次数据更新的批处理，结果为产生数据提交文件。独立的各个<code>无状态</code>的数据提交者实例，负责对跨（Mesa运行所在的）全部数据中心的更新操作进行协调。提交者为每个更新批处理分配一个新的版本号，并基于<code>Paxos一致算法</code>向版本数据库发布全部与该更新关联的元数据。当一个更新满足提交的条件时，意味着一个给定的更新已经被全球范围内的大量Mesa实例进行了合并，提交者会将该次更新的版本号声明为新的提交版本号，并将该值存储在版本数据库里。</p>

<p>查询通常都是根据提交版本号来分发的，因为查询通常都是根据提交版本号来分发的，所以Mesa不需要在更新和查询之间进行任何的锁操作。更新都是由Mesa实例在批处理中进行异步实施的。这些属性使得Mesa获得了非常高的查询和更新吞吐率，同时也对数据一致性提供了保障。</p>

<h3 id="section-2">版本管理</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-8-7/25645530.jpg" width="500px" /></p>

<p>为了支持查询的高效性，如上图，Mesa中的版本分为Base，Cumulative和Singleton，含义在图中也有了清晰的表达，主要的目的是为了<code>打平查询时计算的消耗，将数据聚合前置</code>。Singleton就是上文中的上游产生的数据，每个几分钟产生一个。Base，Cumulative分别是由Base Compaction和Cumulative Compaction产生。二者都会按照一定的规则进行，显然后者的频率会远高于前者。</p>

<p>singleton中因为row都是有序，所以两个singleton归并的时间是线性的。多个文件的归并可以使用最小堆算法，达到 nlogn 的时间。如何找到需要版本号的多个文件，可以使用最短路径算法。</p>

<p>归并方法与HBase的Memory Store -&gt; flush -&gt; Minor compaction -&gt; major compaction的整体流程概念也很相似。但是与HBase／Bigtable的区别在于，批量的写操作，Mesa是直接通过外部系统提交批量数据来实现，HBase则是借助于Memory Store／Flush机制来缓冲，将随机写累积成批量写，Mesa因此需要外部系统配合，不过，也减少了服务自身的复杂性。底层文件合并，目的都是为了提升检索效率，但是HBase更多的是做简单（或者说通用）的数据归并动作，将无效数据（比如Delte掉的数据，超过历史版本数量的数据）剔除，减少文件数量等，本质上不改变数据的形态，性能的收益是从检索本身的开销上获得的，而对于Mesa来说，其工作的重点是历史数据的聚合（剔除一行的动作是通过负值Value来实现），本质上是将细粒度数据转化为粗粒度数据的过程（当然，和前面说的，实际上，取决于聚合函数的定义，可能可以达到其它目的），性能的改善更多的是从数据的形态变化上获得的。</p>

<p>Mesa中Version版本的概念也与BigTable／HBase相比较，形式类似，但是整体目的用途，还是不同的。HBase系统中版本，尽管理论上可以认为是行／列以外的又一个存储维度，但实际系统的设计导向，基本就是作为区分数据历史版本使用，有不少其它系统（比如Percolator／Trafodion）借助于HBase的Cell版本功能，构建起MVCC的机制来实现例如跨行跨表原子操作，OLTP等特性。但是在Mesa中，除了构建原子操作，从查询方式（返回所有0-n版本聚合后的数据）和预聚合合并Delta文件等系统整体设计思路的角度来看，其版本的功能指向，更接近时间序列的概念，用来罗列相同Key下的可聚合的细粒度数据，当然，实际应用可能性取决于聚合函数的定义（比如假设有一个聚合函数是LastVersion，那就接近HBase的版本概念了）。</p>

<h3 id="section-3">其他工程优化</h3>

<p>delta 剪枝：类似于key range，bloomfilter等，可以快速数据块中是否含有需要的信息。</p>

<p>Scan to seek： 如果查询条件的过滤字段不是索引组合键的第一个字段，对过滤字段的左侧字段进行枚举检索，尽量减少需要进行范围检索的工作</p>

<p>Resume key：海量数据的返回是分批返回的，返回结果中附带一个Resume key用作标识当前进度，便于后续查询可以在此基础上继续进行</p>

<p>Mesa系统内部的日常数据维护工作可能涉及到大量读写工作，采用了MR作业来分布式的进行，而要保证MR作业的时间可控性，正确的进行分区，避免数据倾斜会是一个需要妥善解决的问题，Mesa采用数据采样的方式，对每个Delta文件同步存储一个采样文件，通过预读采样文件，决定MR任务分区的键值</p>

<p>对于表结构Schema的在线变更，Mesa提供两种方式，一是使用新的schema全量拷贝数据到新的版本上，二是在特定的表结构变更场景中（比如增加字段的这种表结构变更操作），在查询的时候动态判断schema版本，对返回数据进行转换，（增加字段的变更为例，Mesa会为老数据补上默认值），这个过程只需要维持一段特定的时间，因为MESA内的数据经过一段时间会进行Delta增量合并操作，在合并过程中Mesa再对涉及到的历史数据进行格式转换操作。</p>

<p>数据校验采用线上和线下两种方式。线上测试在每个update和query操作中进行，针对每个数据文件和index文件，检查checksum，并且查看row key的排序和范围。线下测试会进行全局的checksum校验，这个checksum依赖row的顺序，对于不同粒度（table，version，index）会有不同的checksum粒度。</p>

<h3 id="lessons-learned">Lessons Learned</h3>

<ul>
  <li>分布式并行化，非中心化的思想横贯其中。</li>
  <li>注意模块化和抽象化，以及分层的设计理念</li>
  <li>减少对应用层的假设，需要设计的更加通用化，对现在和未来应用的假设越少越好</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Column-Stores vs. Row-Stores]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores/"/>
    <updated>2016-07-31T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Abadi et al. SIGMOD 2008</p>
</blockquote>

<p>在数据分析领域，例如数据仓库、决策支持、BI应用等，面向列的存储结构通常会比面向行的存储结构表现要好一个数量级以上，因为只需要读取需要的属性，列存储对于只读的查询 IO 更高效。本文否定了用列存储的思想优化行存储的方式，提出对两种存储方式都有效的优化，并探讨列存储真正高效的原因。</p>

<!--more-->

<h3 id="row-vs-column">Row vs. column</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/39462037.jpg" width="500px" /></p>

<p>Row store 更加适合添加和修改一条记录，但读取不必要的数据；Column Store 适合读取相关数据，但写的时候需要多次磁盘IO。所以 Column stores 适合读密集型的数据集。对比如下：</p>

<table border="1" cellspacing="0" cellpadding="0"> <tbody><tr>  <td valign="top" style="background:#5B9BD5;"><p align="left"><strong><span style="color:white;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">行式存储</span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">列式存储</span></strong></p></td> </tr> <tr>  <td valign="top" style="background:#DEEAF6;"><p align="center"><strong>优点</strong></p></td>  <td valign="top" style="background:#DEEAF6;"><p align="left">Ø&nbsp; 数据被保存在一起</p>  <p align="left">Ø&nbsp; INSERT/UPDATE容易</p></td>  <td valign="top" style="background:#DEEAF6;"><p>Ø&nbsp; 查询时只有涉及到的列会被读取</p>  <p>Ø&nbsp; 投影(projection)很高效</p>  <p>Ø&nbsp; 任何列都能作为索引</p></td> </tr> <tr>  <td valign="top"><p align="center"><strong>缺点</strong></p></td>  <td valign="top"><p align="left">Ø&nbsp; 选择(Selection)时即使只涉及某几列，所有数据也都会被读取</p></td>  <td valign="top"><p>Ø&nbsp; 选择完成时，被选择的列要重新组装</p>  <p>Ø&nbsp; INSERT/UPDATE比较麻烦</p></td> </tr></tbody></table>

<p>通常来说它比 Row store 在某些应用中更快的原因有：</p>

<ul>
  <li>只读取需要的列</li>
  <li>更好的缓存有效性</li>
  <li>适合压缩</li>
</ul>

<h3 id="row-oriented-execution">Row-oriented Execution</h3>

<p>首先看下在商用的行存储DBMS中如何实现列数据库：</p>

<ol>
  <li>
    <p>Vertical Partitioning</p>

    <blockquote>
      <p>The most straightforward way to emulate a column-store approach in a row-store is to fully vertically partition each relation.</p>
    </blockquote>

    <p>垂直切割表格形成两列的tuple表（table key, attribute）, 查询的时候直接访问必要的列。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/64076524.jpg" width="450px" /></p>

    <p>遇到的问题： 需要添加 Position 列，浪费磁盘和带宽；Tuple 有额外 Header，例如 PostgreSQL 里有 24 bytes。</p>
  </li>
  <li>
    <p>Index-only Plan</p>

    <blockquote>
      <p>base relations are stored using a standard, row-oriented design, but an additional unclustered B+Tree index is added on every column of every table.</p>
    </blockquote>

    <p>构造在query中所需要用到的所有列的数据块的集合，所以查询的时候根本不需要查询底层的（按行存储的）表。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/2612463.jpg" width="450px" /></p>

    <p>遇到的问题：分开的数据块如果需要全表扫表的话，会很慢。</p>
  </li>
  <li>
    <p>Materialized Views</p>

    <blockquote>
      <p>Create optimal set of MVs for given query workload to provide just the required data, avoid overheads and perform better.</p>
    </blockquote>

    <p>使用query中所需要用到的列的物化视图集合，尽管使用很多空间，但是可以得到更好的效率，典型的空间换时间。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/44662651.jpg" width="300px" /></p>

    <p>遇到的问题：适用性比较窄；需要知道查询的先验知识。</p>
  </li>
</ol>

<h3 id="column-oriented-execution">Column-oriented Execution</h3>

<p>下面来看下面向列存储的一些优化方法：</p>

<ol>
  <li>
    <p>Compression</p>

    <blockquote>
      <p>Low information entropy (high data value locality) leads to High compression ratio.</p>
    </blockquote>

    <p>这个比较重要，有结果显示压缩和不压缩性能上会差别一个数量级，具体的好处是：</p>

    <ul>
      <li>节省磁盘空间</li>
      <li>减少磁盘和网络IO</li>
      <li>如果能在压缩数据上面做计算，那 CPU 消耗也可以降低</li>
    </ul>

    <p>几个注意点：</p>

    <ul>
      <li>注意压缩率和解压效率的权衡，使用轻量级的压缩算法，牺牲一些压缩率。</li>
      <li>使用类似run-length encoding (1,1,1,2,2 -&gt; 1<em>3,2</em>2)，可以直接在压缩数据上做计算。</li>
      <li>特别是对于排好序的数据，压缩效率会更高。</li>
    </ul>
  </li>
  <li>
    <p>Late Materialization</p>

    <p>通常来说，查询结果是要得到一个 entity，而不是一个 column，所以需要对多个列进行 join。比较朴素的做法是，数据按列存储在磁盘上，当一个query需要多个 column 的时候，会从这些属性中构造 tuples，接着进行一些其他操作（select，aggregate，join…）。虽然这样做仍然会优于 row-oriented 的存储，但是会有许多优化空间，使用延迟物化就是一个。</p>

    <p>举一个例子，<code>SELECT R.a FROM R WHERE R.c = 5 AND R.b = 10</code>。首先，<code>R.c</code> 和 <code>R.b</code> 的输出是一个 bit string，这个其实就是中间结果 position lists，对着两个结果进行 <code>bitwise and</code>，最后这个 final position list 提取 <code>R.a</code>。</p>

    <p>带来的好处有：</p>

    <ul>
      <li>非必要的 tuple 构建省略了（selection 和 aggregation带来的）</li>
      <li>可以直接对着压缩数据进行操作</li>
      <li>更好的 cache performance，没有其他属性的数据污染 cache</li>
      <li>可以结合下一个优化点</li>
    </ul>
  </li>
  <li>
    <p>Block Iteration</p>

    <blockquote>
      <p>Operators operate on blocks of tuples at once like batch processing.</p>
    </blockquote>

    <p>对 Block 而不是 Tuple 进行操作，这个同样可以应用在 Row-oriented stores 上，带来的好处有：</p>

    <ul>
      <li>如果列是固定宽度的，则可以变成对数组的操作</li>
      <li>减少 tuple 的 overhead</li>
      <li>有效利用并行化</li>
    </ul>
  </li>
</ol>

<h3 id="section">结论</h3>

<p>Vertical portioning 和 Index only plan 并不能起到很好的效果，Materialized view 表现的最好。</p>

<ul>
  <li>Block processing improves the performance by a factor of 5% to 50%</li>
  <li>Compression improves the performance by almost a factor of two on avg</li>
  <li>Late materialization improves performance by almost a factor of three</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Realtime Data Processing at Facebook]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook/"/>
    <updated>2016-07-13T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Chen et al. SIGMOD 2016</p>
</blockquote>

<p>论文说明Facebook是如何建立实时系统的。有段话说明batch和stream的关系：</p>

<blockquote>
  <p>Streaming versus batch processing is not an either/or decision. Originally, all data warehouse processing at Facebook was batch processing. We began developing [some of our streaming systems] about five years ago… using a mix of streaming and batch processing can speed up long pipelines by hours. Furthermore, streaming-only systems can be authoritative. We do not need to treat realtime system results as an approximation and batch results as “the truth.”</p>
</blockquote>

<!--more-->

<p>典型的实时系统有Twitter的<code>Storm</code>和<code>Heron</code>、Google的<code>Millwheel</code>，LinkedIn的<code>Samza</code>。Facebook有<code>Puma</code>、<code>Swift</code>、<code>Stylus</code>，在实时系统中他们有以下几方面的考虑：</p>

<ol>
  <li>易用性：处理要求有多复杂，sql是否足够，还是需要通用语言？用户对新应用的编写，测试、调试和部署有多快，如何对应用进行监控？</li>
  <li>性能：延迟，吞吐量，每台机器的以及聚合时候的？</li>
  <li>容错性：容忍错误种类，数据处理和输出时的语义保证，系统如何存储恢复内存状态？</li>
  <li>扩展性：数据分片以及重新分片的并行处理，数据量增加和减小时候系统如何处理，对于老数据的重放？</li>
  <li>正确性：ACID保证？</li>
</ol>

<p>数据处理需求允许秒级的延迟，所以fb中的数据传输是通过一个持久存储的；分离传输和处理带来的好处是易用、容错和可扩展以及一部分正确性保证。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/63234135.jpg" width="500px" /></p>

<ul>
  <li>
    <p>scribe</p>

    <p>Scribe从各种数据源上收集数据，放到一个共享队列上，然后push到后端的中央存储系统上。当中央存储系统出现故障时，scribe可以暂时把日志写到本地文件中，待中央存储系统恢复性能后，scribe把本地日志续传到中央存储系统上。
需要注意的是，各个数据源须通过thrift向scribe传输数据（每条数据记录包含一个category和一个message）。可以在scribe配置用于监听端口的thrift线程数。在后端，scribe可以将不同category的数据存放到不同目录中，以便于进行分别处理。后端的日志存储方式可以是各种各样的store，包括file（文件），buffer（双层存储，一个主储存，一个副存储），network（另一个scribe服务器），bucket（包含多个store，通过hash的将数据存到不同store中），null(忽略数据)，thriftfile（写到一个Thrift TFileTransport文件中）和multi（把数据同时存放到不同store中）</p>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-14/99114695.jpg" width="500px" /></p>

<ul>
  <li>puma 使用支持Java UDF的类sql语言，目的在于快速开发部署</li>
  <li>swift 使用脚本语言支持流式处理，简单的API和checkpointing。</li>
  <li>stylus C++编写，类似Strom的流处理引擎。</li>
  <li>Laser 高吞吐低延迟的内存KV存储，基于RocksDB（基于Google的LevelDB）<a href="https://influxdata.com/blog/benchmarking-leveldb-vs-rocksdb-vs-hyperleveldb-vs-lmdb-performance-for-influxdb/">benchmark</a></li>
  <li>Scuba fast slice-and-dice analysis data store，支ad-hoc query和可视化</li>
  <li>Hive  数据仓库</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/55531065.jpg" width="600px" /></p>

<ul>
  <li>Language paradigm
    <ul>
      <li>声明语言，SQL，编写作为最简单和快速，群众基础好。缺点是表达能力不够。</li>
      <li>函数语言，使用预制的算子串联处理过程，简单并且对算子的组合增加表达能力。</li>
      <li>过程语言，C++，JAVA，Python，灵活，性能最高，对数据结构和执行过程有完全掌控力，但是要求也最高。S4/Storm/Heron/Samza。</li>
    </ul>
  </li>
  <li>Data transfer
    <ul>
      <li>直接传输，RPC或者内存消息队列。延迟少，毫秒级。</li>
      <li>Broker，另外一个broker进程处理数据传输，增加overhead但是扩展性好。支持多对多的传输，支持反压。典型的是Heron。</li>
      <li>持久存储，persistent message bus。多路复用，输入输出不同的速度，不同的时间点，重复读。</li>
    </ul>

    <p>邻近节点处于DAG中，两种依赖关系，窄依赖和宽依赖，实现在数据传输中。</p>
  </li>
  <li>
    <p>Processing semantics</p>

    <p>stream processor有三种行为：</p>

    <ul>
      <li>处理input event，反序列化输入event，查询外部系统，更新内存状态，并没有side-effect</li>
      <li>输出产生，根据input event和内存状态，向下游产生输出。可以在检查点之前或之后。</li>
      <li>保存检查点，内存状态、input stream offset、output value</li>
    </ul>

    <p>根据这些行为的实现方式，有两种语义信息：</p>

    <ul>
      <li>状态语义，input event是at-least once, at-most once, exactly</li>
      <li>输出语义，output value….</li>
    </ul>

    <p>无状态的处理器只有输出语义，有状态的有两种语义。状态语义只依赖于保存offset和内存状态的顺序。</p>

    <ul>
      <li>at-least-once，先内存状态后offset</li>
      <li>at-most-once，先offset，再内存状态</li>
      <li>exactly，原子的</li>
    </ul>

    <p>输出语义依赖数据保存到checkpint的顺序：</p>

    <ul>
      <li>at-least-once，先emit output再offset和内存状态</li>
      <li>at-most-once，先offset和内存状态，再emit output</li>
      <li>exactly，原子的事务性</li>
    </ul>

    <p>可以做一些side-effect-free的操作加速吞吐量，例如保存checkpoint的同时进行反序列化。</p>
  </li>
  <li>
    <p>State-saving mechanism </p>

    <ul>
      <li>Replication</li>
      <li>local database persistence</li>
      <li>remote database persistence</li>
      <li>upstream backup</li>
      <li>global consistent snapshot</li>
    </ul>
  </li>
  <li>
    <p>Backfill processing</p>

    <ul>
      <li>stream only.</li>
      <li>保存两个系统，一个用于batch，一个用于流处理</li>
      <li>开发流处理系统可以运行在batch环境中，spark streaming 和 flink. </li>
    </ul>
  </li>
</ul>

<h3 id="lessons-learned">Lessons learned</h3>

<ol>
  <li>拥有多个不同的实时处理系统是有好处的. “Writing a simple application lets our users deploy something quickly and prove its value first, then invest the time in building a more complex and robust application.”</li>
  <li>回放对于debug来说很重要.</li>
  <li>“Writing a new streaming application requires more than writing the application code. The ease or hassle of deploying and maintaining the application is equally important. Laser and Puma apps are deployed as a service. Stylus apps are owned by the individual teams who write them, but we provide a standard framework for monitoring them.” Making Puma apps self-service was key to scaling to the hundreds of data pipelines using Puma today.</li>
  <li>使用告警来检查应用的处理速度低于输入速度，未来考虑自动缩放。</li>
  <li>Streaming versus batch processing does not need to be an either/or decision.</li>
  <li>易用性很重要.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[google论文：MapReduce]]></title>
    <link href="http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-mapreduce/"/>
    <updated>2013-05-14T01:09:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-mapreduce</id>
    <content type="html"><![CDATA[<p>论文：<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/mapreduce-osdi04.pdf">英文版</a>，<a href="http://blademaster.ixiezi.com/2010/03/27/google-mapreduce%E4%B8%AD%E6%96%87%E7%89%88/">中文版</a></p>

<hr />

<ol>
  <li>
    <h2 id="section">导论</h2>
  </li>
</ol>

<h3 id="section-1">1.1 定义</h3>

<p>先给个定义：
MapReduce是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户首先创建一个Map函数处理一个基于key/value
pair的数据集合，输出中间的基于key/value
pair的数据集合；然后再创建一个Reduce函数用来合并所有的具有相同中间key值的中间value值。</p>

<p>使用这个抽象模型，我们只要表述我们想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面。设计这个抽象模型的灵感来自Lisp和许多其他函数式语言的Map和Reduce的原语。</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr2_zps9c617225.png" alt="big picture of MapReduce" /></p>

<!--more-->
<p>### 1.2 概述</p>

<ul>
  <li>Programmers must specify:
    <ul>
      <li>map (k, v) → &lt;k’, v’&gt;*</li>
      <li>reduce (k’, v’) → &lt;k’, v’&gt;*
        <ul>
          <li>All values with the same key are reduced together</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optionally, also:
    <ul>
      <li>partition (k’, number of partitions) → partition for k’
        <ul>
          <li>Often a simple hash of the key, e.g., hash(k’) mod n</li>
          <li>Divides up key space for parallel reduce operations</li>
        </ul>
      </li>
      <li>combine (k’, v’) → &lt;k’, v’&gt;*
        <ul>
          <li>Mini-reducers that run in memory after the map phase</li>
          <li>Used as an optimization to reduce network traffic</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The execution framework handles everything else…
    <ul>
      <li>Scheduling: assigns workers to map and reduce tasks</li>
      <li>“Data distribution”: moves processes to data</li>
      <li>Synchronization: gathers, sorts, and shuffles intermediate data</li>
      <li>Errors and faults: detects worker failures and restarts</li>
    </ul>
  </li>
  <li>Limited control over data and execution flow
    <ul>
      <li>All algorithms must expressed in m, r, c, p</li>
    </ul>
  </li>
  <li>You don’t know:
    <ul>
      <li>Where mappers and reducers run</li>
      <li>When a mapper or reducer begins or finishes</li>
      <li>Which input a particular mapper is processing</li>
      <li>Which intermediate key a particular reducer is processing</li>
    </ul>
  </li>
</ul>

<ol>
  <li>
    <h2 id="section-2">实现</h2>
  </li>
</ol>

<h3 id="section-3">2.1 流程</h3>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr1_zps85dad9ca.png" alt="Execution Overview" height="500px" /></p>

<p>上图展示了我们的MapReduce实现中操作的全部流程。</p>

<ol>
  <li>用户程序首先调用的MapReduce库将输入文件分成M个数据片段，每个数据片段的大小一般从16MB到64MB。然后用户程序在机群中创建大量的程序副本。</li>
  <li>这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是worker程序，由master分配任务。有M个Map任务和R个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。</li>
  <li>被分配了map任务的worker程序读取相关的输入数据片段，从输入的数据片段中解析出key/value
pair，然后把key/value
pair传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value
pair，并缓存在内存中。</li>
  <li>缓存中的key/value
pair通过分区函数分成R个区域，之后周期性的写入到本地磁盘上。缓存的key/value
pair在本地磁盘上的存储位置将被回传给master，由master负责把这些存储位置再传送给Reduce
worker。</li>
  <li>当Reduce
worker程序接收到master程序发来的数据存储位置信息后，使用RPC从Map
worker所在主机的磁盘上读取这些缓存数据。当Reduce
worker读取了所有的中间数据后，通过对key进行排序后使得具有相同key值的数据聚合在一起。由于许多不同的key值会映射到相同的Reduce任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li>
  <li>Reduce
worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce
worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。</li>
  <li>当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对MapReduce调用才返回。</li>
</ol>

<h3 id="mapreduce">2.2 map和Reduce的同步</h3>

<ul>
  <li>Cleverly-constructed data structures
    <ul>
      <li>Bring partial results together</li>
    </ul>
  </li>
  <li>Sort order of intermediate keys
    <ul>
      <li>Control order in which reducers process keys</li>
    </ul>
  </li>
  <li>Partitioner
    <ul>
      <li>Control which reducer processes which keys</li>
    </ul>
  </li>
  <li>Preserving state in mappers and reducers
    <ul>
      <li>Capture dependencies across multiple keys and values</li>
    </ul>
  </li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr3_zps5ead0c7b.png" alt="map和Reduce的同步" height="300px" /></p>

<h3 id="section-4">2.3 本地聚合</h3>

<ul>
  <li>Ideal scaling characteristics:
    <ul>
      <li>Twice the data, twice the running time</li>
      <li>Twice the resources, half the running time</li>
    </ul>
  </li>
  <li>Why can’t we achieve this?
    <ul>
      <li>Synchronization requires communication</li>
      <li>Communication kills performance</li>
    </ul>
  </li>
  <li>Thus… avoid communication!
    <ul>
      <li>Reduce intermediate data via local aggregation</li>
      <li>Combiners can help</li>
    </ul>
  </li>
</ul>

<h3 id="shuffle-and-sort">2.4 Shuffle and Sort</h3>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/mr4_zps7ee59e35.png" alt="Shuffle and Sort" height="400px" /></p>

<h3 id="master">2.5 Master</h3>

<p>Master持有一些数据结构，它存储每一个Map和Reduce任务的状态（空闲、工作中或完成)，以及Worker机器(非空闲任务的机器)的标识。</p>

<p>Master就像一个数据管道，中间文件存储区域的位置信息通过这个管道从Map传递到Reduce。因此，对于每个已经完成的Map任务，master存储了Map任务产生的R个中间文件存储区域的大小和位置。当Map任务完成时，Master接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。</p>

<p>master周期性的ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息，master将把这个worker标记为失效。所有由这个失效的worker完成的Map任务被重设为初始的空闲状态，之后这些任务就可以被安排给其他的worker。同样的，worker失效时正在运行的Map或Reduce任务也将被重新置为空闲状态，等待重新调度。</p>

<p>master周期性的将数据写入磁盘，即检查点（checkpoint）。如果这个master任务失效了，可以从最后一个检查点开始启动另一个master进程。然而，由于只有一个master进程，master失效后再恢复是比较麻烦的，因此我们现在的实现是如果master失效，就中止MapReduce运算。客户可以检查到这个状态，并且可以根据需要重新执行MapReduce操作。</p>

<ol>
  <li>
    <h2 id="section-5">性能优化</h2>
  </li>
</ol>

<h3 id="straggler">3.1 straggler</h3>

<p>影响一个MapReduce的总执行时间最通常的因素是straggler(落伍者)：在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过预期。</p>

<p>当一个MapReduce操作接近完成的时候，master调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，我们都把这个任务标记成为已经完成。我们调优了这个机制，通常只会占用比正常操作多几个百分点的计算资源。我们发现采用这样的机制对于减少超大MapReduce操作的总处理时间效果显著。</p>

<h3 id="partitioning-function">3.2 分区函数(partitioning function)</h3>

<p>我们在中间key上使用分区函数来对数据进行分区，之后再输入到后续任务执行进程。一个缺省的分区函数是使用hash方法(比如，hash(key)
mod
R)进行分区。hash方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对key值进行的分区将非常有用。</p>

<p>使用“hash(Hostname(urlkey)) mod
R”作为分区函数就可以把所有来自同一个主机的URLs保存在同一个输出文件中。</p>

<h3 id="section-6">3.3 顺序保证</h3>

<p>在给定的分区中，中间key/value
pair数据的处理顺序是按照key值增量顺序处理的。</p>

<h3 id="combiner">3.4 Combiner函数</h3>

<p>用户指定一个可选的combiner函数，combiner函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</p>

<p>一般情况下，Combiner和Reduce函数是一样的。Combiner函数和Reduce函数之间唯一的区别是MapReduce库怎样控制函数的输出。</p>

<h3 id="section-7">3.5 跳过损坏的记录</h3>

<ul>
  <li>Map/Reduce functions sometimes fail for particular inputs
    <ul>
      <li>Best solution is to debug &amp; fix
        <ul>
          <li>Not always possible \~ third-party source libraries</li>
        </ul>
      </li>
      <li>On segmentation fault:
        <ul>
          <li>Send UDP packet to master from signal handler</li>
          <li>Include sequence number of record being processed</li>
        </ul>
      </li>
      <li>If master sees two failures for same record:
        <ul>
          <li>Next worker is told to skip the record</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<ol>
  <li>
    <h2 id="section-8">要点和例子</h2>
  </li>
</ol>

<h3 id="points-need-to-be-emphasized">4.1 Points need to be emphasized</h3>

<ul>
  <li>No reduce can begin until map is complete</li>
  <li>Master must communicate locations of intermediate files</li>
  <li>Tasks scheduled based on location of data</li>
  <li>If map worker fails any time before reduce finishes, task must be
completely rerun</li>
  <li>MapReduce library does most of the hard work for us!</li>
</ul>

<h3 id="section-9">4.2 例子</h3>

<ol>
  <li>分布式的Grep：Map函数输出匹配某个模式的一行，Reduce函数是一个恒等函数，即把中间数据复制到输出。</li>
  <li>计算URL访问频率：Map函数处理日志中web页面请求的记录，然后输出(URL,1)。Reduce函数把相同URL的value值都累加起来，产生(URL,记录总数)结果。</li>
  <li>倒转网络链接图：Map函数在源页面（source）中搜索所有的链接目标（target）并输出为(target,source)。Reduce函数把给定链接目标（target）的链接组合成一个列表，输出(target,list(source))。</li>
  <li>每个主机的检索词向量：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。Map函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的URL。Reduce函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的(主机名,检索词向量)。</li>
  <li>倒排索引：Map函数分析每个文档输出一个(词,文档号)的列表，Reduce函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出(词,list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。</li>
  <li>
    <p>分布式排序：Map函数从每个记录提取key，输出(key,record)。Reduce函数不改变任何的值。这个运算依赖分区机制(在4.1描述)和排序属性(在4.2描述)。</p>
  </li>
  <li>
    <h2 id="hadoop">Hadoop</h2>
  </li>
</ol>

<p><strong>术语对照</strong></p>

<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>翻译</strong></span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>Hadoop</strong><strong>术语</strong></span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>Google</strong><strong>术语</strong></span></p>
</td>
<td width="277">
<p class="TableContents" align="center"><span style="font-size: 14px;"><strong>相关解释</strong></span></p>
</td>
</tr>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;">作业</span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;">Job</span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;">Job</span></p>
</td>
<td width="277">
<p class="TableContents"><span style="font-size: 14px;">用户的每一个计算请求，就称为一个作业。</span></p>
</td>
</tr>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;">作业服务器</span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;">JobTracker</span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;">Master</span></p>
</td>
<td width="277">
<p class="TableContents"><span style="font-size: 14px;">用户提交作业的服务器，同时，它还负责各个作业任务的分配，管理所有的任务服务器。</span></p>
</td>
</tr>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;">任务服务器</span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;">TaskTracker</span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;">Worker</span></p>
</td>
<td width="277">
<p class="TableContents"><span style="font-size: 14px;">任劳任怨的工蜂，负责执行具体的任务。</span></p>
</td>
</tr>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;">任务</span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;">Task</span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;">Task</span></p>
</td>
<td width="277">
<p class="TableContents"><span style="font-size: 14px;">每一个作业，都需要拆分开了，交由多个服务器来完成，拆分出来的执行单位，就称为任务。</span></p>
</td>
</tr>
<tr>
<td width="109">
<p class="TableContents" align="center"><span style="font-size: 14px;">备份任务</span></p>
</td>
<td width="136">
<p class="TableContents" align="center"><span style="font-size: 14px;">Speculative Task</span></p>
</td>
<td width="142">
<p class="TableContents" align="center"><span style="font-size: 14px;">Buckup Task</span></p>
</td>
<td width="277">
<p class="TableContents"><span style="font-size: 14px;">每一个任务，都有可能执行失败或者缓慢，为了降低为此付出的代价，系统会未雨绸缪的实现在另外的任务服务器上执行同样一个任务，这就是备份任务。</span></p>
</td>
</tr>
</tbody>
</table>
<p>具体可以看博文<a href="http://www.cnblogs.com/duguguiyu/archive/2009/02/28/1400278.html">http://www.cnblogs.com/duguguiyu/archive/2009/02/28/1400278.html</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[google论文：GFS]]></title>
    <link href="http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-gfs/"/>
    <updated>2013-05-14T01:09:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2013/05/14/googlelun-wen-gfs</id>
    <content type="html"><![CDATA[<p>论文：<a href="http://www.cs.rochester.edu/meetings/sosp2003/papers/p125-ghemawat.pdf">英文版</a>，<a href="http://blademaster.ixiezi.com/2010/03/27/the-google-file-system%E4%B8%AD%E6%96%87%E7%89%88/">中文版</a></p>

<hr />

<ol>
  <li>
    <h2 id="section">导论</h2>
  </li>
</ol>

<p>先给个定义：GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，但可以提供容错功能。它可以给大量的用户提供总体性能较高的服务。</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/GoogleFileSystemGFS.svg/500px-GoogleFileSystemGFS.svg.png" alt="Google File System. Designed for system-to-system interaction, and not for user-to-system interaction. The chunk servers replicate the data automatically" /></p>

<p><strong>Assumptions in Google File System (GFS)</strong></p>

<ul>
  <li>GFS should be built with commodity hardware
    <ul>
      <li>Inexpensive disks and machines</li>
    </ul>
  </li>
  <li>GSF stores a modest number of large files
    <ul>
      <li>GSF stores a modest number of large files
        <ul>
          <li>e.g. Big-table, Map-Reduce records</li>
        </ul>
      </li>
      <li>Do not optimize for small files</li>
    </ul>
  </li>
  <li>Workloads
    <ul>
      <li>Large streaming reads (1MB or more) and small random reads (a
few KBs)</li>
      <li>Sequential appends to files by hundreds of data producers
        <ul>
          <li>Utilizing the fact that files are seldom modified again</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>High sustained bandwidth is more important than latency
    <ul>
      <li>Response time for individual read and write is not critical
<!--more-->
<strong>Prerequisite</strong></li>
    </ul>
  </li>
</ul>

<ol>
  <li>组件失效被认为是常态事件，而不是意外事件。</li>
  <li>以通常的标准衡量，我们的文件非常巨大。</li>
  <li>绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。</li>
  <li>应用程序和文件系统API的协同设计提高了整个系统的灵活性。e.g.
    <ul>
      <li>放松了在GFS一致性模型的要求</li>
      <li>引入了原子性的记录追加操作</li>
      <li>三个冗余的数据可以不是位一致，但是要求校验和验证</li>
    </ul>
  </li>
  <li>系统的工作负载
    <ul>
      <li>读操作
        <ul>
          <li>大规模的流式读取</li>
          <li>小规模的随机读取</li>
        </ul>
      </li>
      <li>写操作
        <ul>
          <li>许多大规模的、顺序的、数据追加方式的写操作</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>系统必须高效的、行为定义明确的实现多客户端并行追加数据到同一个文件里
    <ul>
      <li>使用最小的同步开销来实现的原子的多路追加数据操作是必不可少的</li>
      <li>文件可以在稍后读取，或者是消费者在追加的操作的同时读取文件</li>
    </ul>
  </li>
  <li>高性能的稳定网络带宽远比低延迟重要
    <ul>
      <li>高速率的、大批量的处理数据</li>
      <li>极少有程序对单一的读写操作有严格的响应时间要求</li>
    </ul>
  </li>
</ol>

<h2 id="section-1"><strong>2. 架构</strong></h2>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/ache_zps14a9a2bc.png" alt="GFS Achitecture" height="300px" /></p>

<ul>
  <li>Files are divided into chunks</li>
  <li>Fixed-size chunks (64MB)</li>
  <li>Replicated over chunkservers, called replicas</li>
  <li>Unique 64-bit chunk handles</li>
  <li>
    <p>Chunks as Linux files</p>
  </li>
  <li>
    <p>Single master</p>
  </li>
  <li>Multiple chunkservers
    <ul>
      <li>Grouped into Racks</li>
      <li>Connected through switches</li>
    </ul>
  </li>
  <li>Multiple clients</li>
  <li>Master/chunkserver coordination
    <ul>
      <li>HeartBeat messages</li>
    </ul>
  </li>
</ul>

<p><strong><em>注意逻辑的Master节点和物理的Master服务器的区别</em></strong></p>

<h3 id="master">Master节点</h3>

<p>Master节点管理所有的文件系统元数据。这些元数据包括名字空间、访问控制信息、文件和Chunk的映射信息、以及当前Chunk的位置信息。Master节点还管理着系统范围内的活动，比如，Chunk租用管理、失效Chunk的回收、以及Chunk在Chunk服务器之间的迁移。</p>

<p>单一的Master节点可以通过全局的信息精确定位Chunk的位置以及进行复制决策。另外，我们必须减少对Master节点的读写，避免Master节点成为系统的瓶颈。客户端并不通过Master节点读写文件数据。反之，客户端向Master节点询问它应该联系的Chunk服务器。客户端将这些元数据信息缓存一段时间，后续的操作将直接和Chunk服务器进行数据读写操作。</p>

<h3 id="master-1">Master服务器</h3>

<p>Master服务器存储3种主要类型的元数据，包括：文件和Chunk的命名空间、文件和Chunk的对应关系、每个Chunk副本的存放地点。所有的元数据都保存在Master服务器的内存中。前两种类型的元数据（命名空间、文件和Chunk的对应关系）同时也会以记录变更日志的方式记录在操作系统的系统日志文件中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程Master服务器上。采用保存变更日志的方式，我们能够简单可靠的更新Master服务器的状态，并且不用担心Master服务器崩溃导致数据不一致的风险。Master服务器不会持久保存Chunk位置信息。Master服务器在启动时，或者有新的Chunk服务器加入时，向各个Chunk服务器轮询它们所存储的Chunk的信息。</p>

<p>Master服务器并不保存持久化保存哪个Chunk服务器存有指定Chunk的副本的信息。Master服务器只是在启动的时候轮询Chunk服务器以获取这些信息。Master服务器能够保证它持有的信息始终是最新的，因为它控制了所有的Chunk位置的分配，而且通过周期性的心跳信息监控Chunk服务器的状态。</p>

<ul>
  <li>简化了在有Chunk服务器加入集群、离开集群、更名、失效、以及重启的时候，Master服务器和Chunk服务器数据同步的问题。</li>
  <li>只有Chunk服务器才能最终确定一个Chunk是否在它的硬盘上。Master服务器无需维护一个这些信息的全局视图</li>
</ul>

<h3 id="section-2">操作日志</h3>

<p>操作日志包含了关键的元数据变更历史记录。这对GFS非常重要。这不仅仅是因为操作日志是元数据唯一的持久化存储记录，它也作为判断同步操作顺序的逻辑时间基线。文件和Chunk，连同它们的版本，都由它们创建的逻辑时间唯一的、永久的标识。</p>

<p>必须确保日志文件的完整，确保只有在元数据的变化被持久化后，日志才对客户端是可见的。把日志复制到多台远程机器，并且只有把相应的日志记录写入到本地以及远程机器的硬盘后，才会响应客户端的操作请求。</p>

<p>为了缩短Master启动的时间，我们必须使日志足够小。Master服务器在日志增长到一定量时对系统状态做一次Checkpoint，将所有的状态数据写入一个Checkpoint文件。在灾难恢复的时候，Master服务器就通过从磁盘上读取这个Checkpoint文件，以及重演Checkpoint之后的有限个日志文件就能够恢复系统。</p>

<p>由于创建一个Checkpoint文件需要一定的时间，所以Master服务器的内部状态被组织为一种格式，这种格式要确保在Checkpoint过程中不会阻塞正在进行的修改操作。</p>

<h3 id="section-3"><strong>一致性模型</strong></h3>

<ul>
  <li>Relaxed consistency model</li>
  <li>
    <p>Two types of mutations</p>

    <ul>
      <li>Writes
        <ul>
          <li>Cause data to be written at an application-specified file
offset</li>
        </ul>
      </li>
      <li>Record appends
        <ul>
          <li>Operations that append data to a file</li>
          <li>Cause data to be appended atomically at least once</li>
          <li>Offset chosen by GFS, not by the client</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>States of a file region after a mutation</p>

    <ul>
      <li>Consistent
        <ul>
          <li>All clients see the same data, regardless which replicas
they read from</li>
        </ul>
      </li>
      <li>Defined
        <ul>
          <li>consistent + all clients see what the mutation writes in its
entirety</li>
        </ul>
      </li>
      <li>Undefined
        <ul>
          <li>consistent +but it may not reflect what any one mutation has
written</li>
        </ul>
      </li>
      <li>Inconsistent
        <ul>
          <li>Clients see different data at different times</li>
          <li>The client retries the operation</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>经过了一系列的成功的修改操作之后，GFS确保被修改的文件region是已定义的，并且包含最后一次修改操作写入的数据。GFS通过以下措施确保上述行为：（a）
对Chunk的所有副本的修改操作顺序一致，（b）使用Chunk的版本号来检测副本是否因为它所在的Chunk服务器宕机而错过了修改操作而导致其失效。失效的副本不会再进行任何修改操作，Master服务器也不再返回这个Chunk副本的位置信息给客户端。它们会被垃圾收集系统尽快回收。</p>

<p>使用GFS的应用程序可以利用一些简单技术实现这个宽松的一致性模型，这些技术也用来实现一些其它的目标功能，包括：</p>

<ul>
  <li>尽量采用追加写入而不是覆盖</li>
  <li>Checkpoint
    <ul>
      <li>to verify how much data has been successfully written</li>
    </ul>
  </li>
  <li>自验证的写入操作
    <ul>
      <li>Checksums to detect and remove <em>padding</em></li>
    </ul>
  </li>
  <li>自标识的记录。
    <ul>
      <li>Unique Identifiers to identify and discard <em>duplicates</em></li>
    </ul>
  </li>
</ul>

<h2 id="section-4"><strong>3. 系统交互</strong></h2>

<ul>
  <li>Master uses leases to maintain a consistent mutation order among
replicas</li>
  <li>Primary is the chunkserver who is granted a chunk lease</li>
  <li>All others containing replicas are secondaries</li>
  <li>Primary defines a mutation order between mutations</li>
  <li>
    <p>All secondaries follows this order</p>
  </li>
  <li>
    <p>数据流和控制流分开</p>
  </li>
  <li>数据以管道的方式，顺序的沿着一个精心选择的Chunk服务器链推送
    <ul>
      <li>Data transfer is pipelined over TCP connections</li>
      <li>Each machine forwards the data to the “closest” machine</li>
      <li>全双工的交换网络</li>
    </ul>
  </li>
  <li><strong>Benefits</strong>：Avoid bottle necks and minimize latency</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/c_zps5923b611.png" alt="Write control and Data Flow" /></p>

<ol>
  <li>客户机向Master节点询问哪一个Chunk服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk持有租约，Master节点就选择其中一个副本建立一个租约。</li>
  <li>Master节点将主Chunk的标识符以及其它副本（又称为secondary副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便后续的操作。只有在主Chunk不可用，或者主Chunk回复信息表明它已不再持有租约的时候，客户机才需要重新跟Master节点联系。</li>
  <li>客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk服务器接收到数据并保存在它的内部LRU缓存中，一直到数据被使用或者过期交换出去。由于数据流的网络传输负载非常高，通过分离数据流和控制流，我们可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会哪个Chunk服务器保存了主Chunk。</li>
  <li>当所有的副本都确认接收到了数据，客户机发送写请求到主Chunk服务器。这个请求标识了早前推送到所有副本的数据。主Chunk为接收到的所有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中。</li>
  <li>主Chunk把写请求传递到所有的二级副本。每个二级副本依照主Chunk分配的序列号以相同的顺序执行这些操作。</li>
  <li>所有的二级副本回复主Chunk，它们已经完成了操作。</li>
  <li>主Chunk服务器回复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主Chunk和一些二级副本执行成功。（如果操作在主Chunk上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的region处于不一致的状态。我们的客户机代码通过重复执行失败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤（3）到步骤（7）做几次尝试。</li>
</ol>

<h3 id="section-5">记录追加的原子性</h3>

<ul>
  <li>The client specifies only the data (not file offset)</li>
  <li>Similar to writes
    <ul>
      <li>Mutation order is determined by the primary</li>
      <li>All secondaries use the same mutation order</li>
    </ul>
  </li>
  <li>GFS appends data to the file at least once atomically
    <ul>
      <li>The chunk is padded if appending the record exceeds the maximum
size –&gt; <em>padding</em></li>
      <li>If a record append fails at any replica, the client retries the
operation –&gt; <em>record duplicates</em></li>
      <li>File region may be defined but interspersed with <em>inconsistent</em></li>
    </ul>
  </li>
</ul>

<h3 id="section-6">快照</h3>

<ul>
  <li>Goals
    <ul>
      <li>To quickly create branch copies of huge data sets</li>
      <li>To easily checkpoint the current state</li>
    </ul>
  </li>
  <li>Copy-on-write technique
    <ul>
      <li>Metadata for the source file or directory tree is duplicated</li>
      <li>Reference count for chunks are incremented</li>
      <li>Chunks are copied later at the first write</li>
    </ul>
  </li>
</ul>

<h3 id="master-operation"><strong>Master Operation</strong></h3>

<ul>
  <li>Namespaces are represented as a lookup table mapping full pathnames
to metadata</li>
  <li>Use locks over regions of the namespace to ensure proper
serialization</li>
  <li>
    <p>Each master operation acquires a set of locks before it runs</p>
  </li>
  <li>
    <p>GFS has no directory (i-node) structure</p>

    <ul>
      <li>Simply uses directory-like file names: /foo, /foo/bar
        <ul>
          <li>Thus listing files in a directory is slow</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Concurrent Access
    <ul>
      <li>Read lock on a parent path, write lock on the leaf file name
        <ul>
          <li>protect delete, rename and snapshot of in-use files</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Rebalancing
    <ul>
      <li>Places new replicas on chunk servers with below-average disk
space utilizations</li>
    </ul>
  </li>
  <li>Re-replication
    <ul>
      <li>When the number of replicas falls below 3 (or user-specified
threshold)
        <ul>
          <li>The master assigns the highest priority to copy (clone) such
chunks</li>
        </ul>
      </li>
      <li>Spread replicas of a chunk across racks</li>
    </ul>
  </li>
</ul>

<p><strong>Example of Locking Mechanism</strong></p>

<p>Preventing /home/user/foo from being created while /home/user is being
snapshotted to /save/user</p>

<ul>
  <li>Snapshot operation
    <ul>
      <li>Read locks on /home and /save</li>
      <li>Write locks on /home/user and /save/user</li>
    </ul>
  </li>
  <li>File creation
    <ul>
      <li>read locks on /home and /home/user</li>
      <li>write locks on /home/user/foo</li>
    </ul>
  </li>
  <li>Conflict locks on /home/user</li>
</ul>

<ol>
  <li>
    <h2 id="section-7">其他细节</h2>
  </li>
</ol>

<h3 id="section-8"><strong>垃圾回收</strong></h3>

<ul>
  <li>Deleted files
    <ul>
      <li>Deletion operation is logged</li>
      <li>File is renamed to a hidden name(deferred deletion)， then may be
removed later or get recovered</li>
      <li>The master regularly scans and removes hidden files, existed
more than three days
        <ul>
          <li>HeartBeat messages inform chunk servers of deleted chunks</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Orphaned chunks (unreachable chunks)
    <ul>
      <li>Identified and removed during a regular scan of the chunk
namespace</li>
    </ul>
  </li>
  <li>Stale replicas
    <ul>
      <li>Chunk version numbering
        <ul>
          <li>increases when the master grants a new lease of the chunk</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="replica-operations">Replica Operations</h3>

<ul>
  <li>Creation
    <ul>
      <li>Disk space utilization</li>
      <li>Number of recent creations on each chunkserver</li>
      <li>Spread across many racks</li>
    </ul>
  </li>
  <li>Re-replication
    <ul>
      <li>Prioritized: How far it is from its replication goal…</li>
      <li>The highest priority chunk is cloned first by copying the chunk
data directly from an existing replica</li>
    </ul>
  </li>
  <li>Rebalancing
    <ul>
      <li>Periodically</li>
    </ul>
  </li>
</ul>

<h3 id="fault-tolerance">Fault Tolerance</h3>

<ul>
  <li>Fast Recovery
    <ul>
      <li>The master and the chunk server are designed to restore their
state in seconds no matter how they terminated.</li>
      <li>Servers are routinely shut down just by killing the process</li>
    </ul>
  </li>
  <li>Master Replications
    <ul>
      <li>Master has the maps from file names to chunks</li>
      <li>One (primary) master manages chunk mutations
        <ul>
          <li>Several shadow masters are provided for read-only accesses
            <ul>
              <li>Snoop operation logs and apply these operations exactly
as the primary does</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Data Integrity
    <ul>
      <li>Corruption of stored data
        <ul>
          <li>High temperature of storage devices causes such errors</li>
        </ul>
      </li>
      <li>Checksums for each 64KB in a chunk
        <ul>
          <li>chunk servers verifies the checksum of data before sending
it to the client or other chunk servers</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

]]></content>
  </entry>
  
</feed>
