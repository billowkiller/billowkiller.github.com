<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-08-08T12:14:34+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Column-Stores vs. Row-Stores]]></title>
    <link href="http://billowkiller.github.io/blog/2016/08/06/mesa/"/>
    <updated>2016-08-06T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/08/06/mesa</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Ashish et al. VLDB 2014</p>
</blockquote>

<p>谷歌Mesa是一个服务于谷歌广告业务的近实时分析型数据仓库，主要用于广告主的报表、内部审计和预测服务。作为底层的存储系统，Mesa能处理PB级的数据，每秒百万的行更新，并且需要应对每日上亿的查询请求，因此它需要满足的设计目标是：</p>

<blockquote>
  <p>near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes.</p>
</blockquote>

<!--more-->

<p>据Google描述，随着他们的广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。为了应付如此持续增长并且重要数据的处理、存储和查询需求，Mesa的具体需求包括：</p>

<ul>
  <li>
    <p><code>原子更新</code>。某一单个的用户行为可能会引起多个关系数据级别的更新，从而影响定义在某个指标集上（例如：点击和成本）跨某个维度集（例如：广告客户和国家）的数千张一致性视图。所以系统状态不会在查询时处于一个只有部分更新生效的状态。</p>
  </li>
  <li>
    <p><code>一致性和正确性</code>。出于业务和法律的原因，该系统必须返回一致和正确的数据。即使某个查询牵涉到多个数据中心，我们仍然需要提供强一致性和可重复的查询结果。</p>
  </li>
  <li>
    <p><code>可用性</code>。系统不允许出现单点故障。不会出现由于计划中或非计划中的维护或故障所造成的停机，即使出现影响整个数据中心或地域性的断电也不能造成停机。</p>
  </li>
  <li>
    <p><code>近实时的更新吞吐率</code>。系统必须支持大约每秒几百万行规模的持续更新，包括添加新数据行和对现有数据行的增量更新。这些更新必须在几分钟内对跨不同视图和数据中心的查询可见。</p>
  </li>
  <li>
    <p><code>查询性能</code>。系统必须对那些对时间延迟敏感的用户提供支持，按照超低延迟的要求为他们提供实时的客户报表，而分批提取用户需要非常高的吞吐率。总的来说，系统必须支持将99%的点查询的延迟控制在数百毫秒之内，并且整体查询控制在每天获取万亿行的吞吐量。</p>
  </li>
  <li>
    <p><code>可伸缩性</code>。系统规模必须可以随着数据规模和查询总量的增长而伸展。举个例子，它必须支持万亿行规模和PB级的数据。但是即使上述参数再出现显著增长，更新和查询的性能必须仍然得以保持。</p>
  </li>
  <li>
    <p><code>在线的数据和元数据转换</code>。为了支持新功能的启用或对现有数据粒度的变更，客户端经常需要对数据模式进行转换或对现有数据的值进行修改。这些变更必须对正常的查询和更新操作没有干扰。</p>
  </li>
</ul>

<p>所有Google现有的大数据技术都无一能满足所有以上的需求。BigTable无法提供<code>原子性和强一致性</code>。而Megastore、Spanner和F1虽然为跨地域复制的数据提供了<code>强一致性</code>的访问，但是他们无法支持Mesa客户端所有需要的<code>峰值更新吞吐率</code>。既有的ROLAP或者MOLAP方法也不能在提供<code>近实时查询</code>同时支持分钟级别的<code>数据更新和聚合</code>。但是Mesa在其不同的基础设施中充分利用了现有的Google技术组件。它使用了BigTable来存储所有<code>持久化的元数据</code>，使用了Colossus (Google的分布式文件系统)来存储数据文件。此外，Mesa还利用了MapReduce来处理连续的数据。</p>

<p>以下有几个技术要点：</p>

<ul>
  <li>为了存储的可伸缩性和可用性，数据是水平分片，并且重复的。</li>
  <li>为了得到一致性和update时候的查询可用性，利用了多版本控制。</li>
  <li>为了更新可伸缩行，数据的更新是批量的和周期性的，并且赋值一个新的版本。</li>
  <li>为了多数据中心数据更新的一致性，Mesa利用基于Paxos的分布式同步协议。</li>
</ul>

<p>Mesa的contributions包括以下一些内容：</p>

<ul>
  <li>高吞吐量的PB级别的数据仓库，同时维持ACID属性以提供事务的处理能力</li>
  <li>新的版本管理方式，利用批量更新，得到低延迟和高吞吐量</li>
  <li>应用数据是通过一个独立和冗余的程序进行异步复制，但是关键的元数据是同步复制。这样可以减少管理副本的同步消耗，并且得到数据更新的高吞吐量</li>
  <li>在线的schema change，并不会影响现有程序的正确性和性能</li>
  <li>软件错误或硬件错误带来的数据损毁的容忍性</li>
</ul>

<h3 id="section">数据模型</h3>

<p>在Mesa的数据是多维度的，定义了细粒度的一个事实表。在这个事实表中分为两种属性，一个是 Key， 一个是 Value。Key是多维的，并且具有层级，例如 date 可以组织成 day, month, year。单个事实表在这些层级之间的聚合可以被物化，这样就支持数据分析的上卷和下钻。Value 也就是一些数据 Metrics。</p>

<p>数据是组织成表的，每个表有 schema 用于指定表结构。schema 里面包含了 Key 空间和 Value 空间，指明他们的类型和聚合方式；另外还包含 aggregation function $F: V \times V \to V$。一般来说聚合方法有 SUM、MAX、MIN和REPLACE；schema 还指明了table的一个或多个索引方法。</p>

<p>数据按对应的索引序排序分割成限定大小的文件，每个文件内部再按行分割成行组row blocks进行压缩存储，每个行组内部的数据在实际存储时，按列式存储的layout进行转换压缩，提高压缩率。而索引文件由行键取固定长度的前缀组成，映射到对应数据行在行组内部的存储偏移量。因为是一个前缀，所以索引可能不能精确定位一行的位置，而是定位这一行在行组内的偏移范围，在通过二分查找的方式在行组内部定位到具体的行。</p>

<h3 id="section-1">查询和更新</h3>

<p>Mesa中存储的数据是多版本的，更新时版本号是向前叠加的，只有处理完当前的版本才会更新下一个版本。这使得当新的更新正在处理时，Mesa可以向用户提供前置状态的<code>一致性数据</code>，也就是 update 的<code>原子性</code>。这种严格的版本顺序还可以保证数据的正确性。</p>

<p>通常，每隔几分钟，上游系统就会执行一次数据更新的批处理，结果为产生数据提交文件。独立的各个<code>无状态</code>的数据提交者实例，负责对跨（Mesa运行所在的）全部数据中心的更新操作进行协调。提交者为每个更新批处理分配一个新的版本号，并基于<code>Paxos一致算法</code>向版本数据库发布全部与该更新关联的元数据。当一个更新满足提交的条件时，意味着一个给定的更新已经被全球范围内的大量Mesa实例进行了合并，提交者会将该次更新的版本号声明为新的提交版本号，并将该值存储在版本数据库里。</p>

<p>查询通常都是根据提交版本号来分发的，因为查询通常都是根据提交版本号来分发的，所以Mesa不需要在更新和查询之间进行任何的锁操作。更新都是由Mesa实例在批处理中进行异步实施的。这些属性使得Mesa获得了非常高的查询和更新吞吐率，同时也对数据一致性提供了保障。</p>

<h3 id="section-2">版本管理</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-8-7/25645530.jpg" width="500px" /></p>

<p>为了支持查询的高效性，如上图，Mesa中的版本分为Base，Cumulative和Singleton，含义在图中也有了清晰的表达，主要的目的是为了<code>打平查询时计算的消耗，将数据聚合前置</code>。Singleton就是上文中的上游产生的数据，每个几分钟产生一个。Base，Cumulative分别是由Base Compaction和Cumulative Compaction产生。二者都会按照一定的规则进行，显然后者的频率会远高于前者。</p>

<p>singleton中因为row都是有序，所以两个singleton归并的时间是线性的。多个文件的归并可以使用最小堆算法，达到 nlogn 的时间。如何找到需要版本号的多个文件，可以使用最短路径算法。</p>

<p>归并方法与HBase的Memory Store -&gt; flush -&gt; Minor compaction -&gt; major compaction的整体流程概念也很相似。但是与HBase／Bigtable的区别在于，批量的写操作，Mesa是直接通过外部系统提交批量数据来实现，HBase则是借助于Memory Store／Flush机制来缓冲，将随机写累积成批量写，Mesa因此需要外部系统配合，不过，也减少了服务自身的复杂性。底层文件合并，目的都是为了提升检索效率，但是HBase更多的是做简单（或者说通用）的数据归并动作，将无效数据（比如Delte掉的数据，超过历史版本数量的数据）剔除，减少文件数量等，本质上不改变数据的形态，性能的收益是从检索本身的开销上获得的，而对于Mesa来说，其工作的重点是历史数据的聚合（剔除一行的动作是通过负值Value来实现），本质上是将细粒度数据转化为粗粒度数据的过程（当然，和前面说的，实际上，取决于聚合函数的定义，可能可以达到其它目的），性能的改善更多的是从数据的形态变化上获得的。</p>

<p>Mesa中Version版本的概念也与BigTable／HBase相比较，形式类似，但是整体目的用途，还是不同的。HBase系统中版本，尽管理论上可以认为是行／列以外的又一个存储维度，但实际系统的设计导向，基本就是作为区分数据历史版本使用，有不少其它系统（比如Percolator／Trafodion）借助于HBase的Cell版本功能，构建起MVCC的机制来实现例如跨行跨表原子操作，OLTP等特性。但是在Mesa中，除了构建原子操作，从查询方式（返回所有0-n版本聚合后的数据）和预聚合合并Delta文件等系统整体设计思路的角度来看，其版本的功能指向，更接近时间序列的概念，用来罗列相同Key下的可聚合的细粒度数据，当然，实际应用可能性取决于聚合函数的定义（比如假设有一个聚合函数是LastVersion，那就接近HBase的版本概念了）。</p>

<h3 id="section-3">其他工程优化</h3>

<p>delta 剪枝：类似于key range，bloomfilter等，可以快速数据块中是否含有需要的信息。</p>

<p>Scan to seek： 如果查询条件的过滤字段不是索引组合键的第一个字段，对过滤字段的左侧字段进行枚举检索，尽量减少需要进行范围检索的工作</p>

<p>Resume key：海量数据的返回是分批返回的，返回结果中附带一个Resume key用作标识当前进度，便于后续查询可以在此基础上继续进行</p>

<p>Mesa系统内部的日常数据维护工作可能涉及到大量读写工作，采用了MR作业来分布式的进行，而要保证MR作业的时间可控性，正确的进行分区，避免数据倾斜会是一个需要妥善解决的问题，Mesa采用数据采样的方式，对每个Delta文件同步存储一个采样文件，通过预读采样文件，决定MR任务分区的键值</p>

<p>对于表结构Schema的在线变更，Mesa提供两种方式，一是使用新的schema全量拷贝数据到新的版本上，二是在特定的表结构变更场景中（比如增加字段的这种表结构变更操作），在查询的时候动态判断schema版本，对返回数据进行转换，（增加字段的变更为例，Mesa会为老数据补上默认值），这个过程只需要维持一段特定的时间，因为MESA内的数据经过一段时间会进行Delta增量合并操作，在合并过程中Mesa再对涉及到的历史数据进行格式转换操作。</p>

<p>数据校验采用线上和线下两种方式。线上测试在每个update和query操作中进行，针对每个数据文件和index文件，检查checksum，并且查看row key的排序和范围。线下测试会进行全局的checksum校验，这个checksum依赖row的顺序，对于不同粒度（table，version，index）会有不同的checksum粒度。</p>

<h3 id="lessons-learned">Lessons Learned</h3>

<ul>
  <li>分布式并行化，非中心化的思想横贯其中。</li>
  <li>注意模块化和抽象化，以及分层的设计理念</li>
  <li>减少对应用层的假设，需要设计的更加通用化，对现在和未来应用的假设越少越好</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Column-Stores vs. Row-Stores]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores/"/>
    <updated>2016-07-31T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Abadi et al. SIGMOD 2008</p>
</blockquote>

<p>在数据分析领域，例如数据仓库、决策支持、BI应用等，面向列的存储结构通常会比面向行的存储结构表现要好一个数量级以上，因为只需要读取需要的属性，列存储对于只读的查询 IO 更高效。本文否定了用列存储的思想优化行存储的方式，提出对两种存储方式都有效的优化，并探讨列存储真正高效的原因。</p>

<!--more-->

<h3 id="row-vs-column">Row vs. column</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/39462037.jpg" width="500px" /></p>

<p>Row store 更加适合添加和修改一条记录，但读取不必要的数据；Column Store 适合读取相关数据，但写的时候需要多次磁盘IO。所以 Column stores 适合读密集型的数据集。对比如下：</p>

<table border="1" cellspacing="0" cellpadding="0"> <tbody><tr>  <td valign="top" style="background:#5B9BD5;"><p align="left"><strong><span style="color:white;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">行式存储</span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">列式存储</span></strong></p></td> </tr> <tr>  <td valign="top" style="background:#DEEAF6;"><p align="center"><strong>优点</strong></p></td>  <td valign="top" style="background:#DEEAF6;"><p align="left">Ø&nbsp; 数据被保存在一起</p>  <p align="left">Ø&nbsp; INSERT/UPDATE容易</p></td>  <td valign="top" style="background:#DEEAF6;"><p>Ø&nbsp; 查询时只有涉及到的列会被读取</p>  <p>Ø&nbsp; 投影(projection)很高效</p>  <p>Ø&nbsp; 任何列都能作为索引</p></td> </tr> <tr>  <td valign="top"><p align="center"><strong>缺点</strong></p></td>  <td valign="top"><p align="left">Ø&nbsp; 选择(Selection)时即使只涉及某几列，所有数据也都会被读取</p></td>  <td valign="top"><p>Ø&nbsp; 选择完成时，被选择的列要重新组装</p>  <p>Ø&nbsp; INSERT/UPDATE比较麻烦</p></td> </tr></tbody></table>

<p>通常来说它比 Row store 在某些应用中更快的原因有：</p>

<ul>
  <li>只读取需要的列</li>
  <li>更好的缓存有效性</li>
  <li>适合压缩</li>
</ul>

<h3 id="row-oriented-execution">Row-oriented Execution</h3>

<p>首先看下在商用的行存储DBMS中如何实现列数据库：</p>

<ol>
  <li>
    <p>Vertical Partitioning</p>

    <blockquote>
      <p>The most straightforward way to emulate a column-store approach in a row-store is to fully vertically partition each relation.</p>
    </blockquote>

    <p>垂直切割表格形成两列的tuple表（table key, attribute）, 查询的时候直接访问必要的列。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/64076524.jpg" width="450px" /></p>

    <p>遇到的问题： 需要添加 Position 列，浪费磁盘和带宽；Tuple 有额外 Header，例如 PostgreSQL 里有 24 bytes。</p>
  </li>
  <li>
    <p>Index-only Plan</p>

    <blockquote>
      <p>base relations are stored using a standard, row-oriented design, but an additional unclustered B+Tree index is added on every column of every table.</p>
    </blockquote>

    <p>构造在query中所需要用到的所有列的数据块的集合，所以查询的时候根本不需要查询底层的（按行存储的）表。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/2612463.jpg" width="450px" /></p>

    <p>遇到的问题：分开的数据块如果需要全表扫表的话，会很慢。</p>
  </li>
  <li>
    <p>Materialized Views</p>

    <blockquote>
      <p>Create optimal set of MVs for given query workload to provide just the required data, avoid overheads and perform better.</p>
    </blockquote>

    <p>使用query中所需要用到的列的物化视图集合，尽管使用很多空间，但是可以得到更好的效率，典型的空间换时间。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/44662651.jpg" width="300px" /></p>

    <p>遇到的问题：适用性比较窄；需要知道查询的先验知识。</p>
  </li>
</ol>

<h3 id="column-oriented-execution">Column-oriented Execution</h3>

<p>下面来看下面向列存储的一些优化方法：</p>

<ol>
  <li>
    <p>Compression</p>

    <blockquote>
      <p>Low information entropy (high data value locality) leads to High compression ratio.</p>
    </blockquote>

    <p>这个比较重要，有结果显示压缩和不压缩性能上会差别一个数量级，具体的好处是：</p>

    <ul>
      <li>节省磁盘空间</li>
      <li>减少磁盘和网络IO</li>
      <li>如果能在压缩数据上面做计算，那 CPU 消耗也可以降低</li>
    </ul>

    <p>几个注意点：</p>

    <ul>
      <li>注意压缩率和解压效率的权衡，使用轻量级的压缩算法，牺牲一些压缩率。</li>
      <li>使用类似run-length encoding (1,1,1,2,2 -&gt; 1<em>3,2</em>2)，可以直接在压缩数据上做计算。</li>
      <li>特别是对于排好序的数据，压缩效率会更高。</li>
    </ul>
  </li>
  <li>
    <p>Late Materialization</p>

    <p>通常来说，查询结果是要得到一个 entity，而不是一个 column，所以需要对多个列进行 join。比较朴素的做法是，数据按列存储在磁盘上，当一个query需要多个 column 的时候，会从这些属性中构造 tuples，接着进行一些其他操作（select，aggregate，join…）。虽然这样做仍然会优于 row-oriented 的存储，但是会有许多优化空间，使用延迟物化就是一个。</p>

    <p>举一个例子，<code>SELECT R.a FROM R WHERE R.c = 5 AND R.b = 10</code>。首先，<code>R.c</code> 和 <code>R.b</code> 的输出是一个 bit string，这个其实就是中间结果 position lists，对着两个结果进行 <code>bitwise and</code>，最后这个 final position list 提取 <code>R.a</code>。</p>

    <p>带来的好处有：</p>

    <ul>
      <li>非必要的 tuple 构建省略了（selection 和 aggregation带来的）</li>
      <li>可以直接对着压缩数据进行操作</li>
      <li>更好的 cache performance，没有其他属性的数据污染 cache</li>
      <li>可以结合下一个优化点</li>
    </ul>
  </li>
  <li>
    <p>Block Iteration</p>

    <blockquote>
      <p>Operators operate on blocks of tuples at once like batch processing.</p>
    </blockquote>

    <p>对 Block 而不是 Tuple 进行操作，这个同样可以应用在 Row-oriented stores 上，带来的好处有：</p>

    <ul>
      <li>如果列是固定宽度的，则可以变成对数组的操作</li>
      <li>减少 tuple 的 overhead</li>
      <li>有效利用并行化</li>
    </ul>
  </li>
</ol>

<h3 id="section">结论</h3>

<p>Vertical portioning 和 Index only plan 并不能起到很好的效果，Materialized view 表现的最好。</p>

<ul>
  <li>Block processing improves the performance by a factor of 5% to 50%</li>
  <li>Compression improves the performance by almost a factor of two on avg</li>
  <li>Late materialization improves performance by almost a factor of three</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hbase Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/20/hbase/"/>
    <updated>2016-07-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/20/hbase</id>
    <content type="html"><![CDATA[<p>犯懒了，留两篇好文章，基本的东西也都在这文章里面。得空再补补吧。</p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426877.html">深入HBase架构解析（一）</a></p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426950.html">深入HBase架构解析（二）</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Realtime Data Processing at Facebook]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook/"/>
    <updated>2016-07-13T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Chen et al. SIGMOD 2016</p>
</blockquote>

<p>论文说明Facebook是如何建立实时系统的。有段话说明batch和stream的关系：</p>

<blockquote>
  <p>Streaming versus batch processing is not an either/or decision. Originally, all data warehouse processing at Facebook was batch processing. We began developing [some of our streaming systems] about five years ago… using a mix of streaming and batch processing can speed up long pipelines by hours. Furthermore, streaming-only systems can be authoritative. We do not need to treat realtime system results as an approximation and batch results as “the truth.”</p>
</blockquote>

<!--more-->

<p>典型的实时系统有Twitter的<code>Storm</code>和<code>Heron</code>、Google的<code>Millwheel</code>，LinkedIn的<code>Samza</code>。Facebook有<code>Puma</code>、<code>Swift</code>、<code>Stylus</code>，在实时系统中他们有以下几方面的考虑：</p>

<ol>
  <li>易用性：处理要求有多复杂，sql是否足够，还是需要通用语言？用户对新应用的编写，测试、调试和部署有多快，如何对应用进行监控？</li>
  <li>性能：延迟，吞吐量，每台机器的以及聚合时候的？</li>
  <li>容错性：容忍错误种类，数据处理和输出时的语义保证，系统如何存储恢复内存状态？</li>
  <li>扩展性：数据分片以及重新分片的并行处理，数据量增加和减小时候系统如何处理，对于老数据的重放？</li>
  <li>正确性：ACID保证？</li>
</ol>

<p>数据处理需求允许秒级的延迟，所以fb中的数据传输是通过一个持久存储的；分离传输和处理带来的好处是易用、容错和可扩展以及一部分正确性保证。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/63234135.jpg" width="500px" /></p>

<ul>
  <li>
    <p>scribe</p>

    <p>Scribe从各种数据源上收集数据，放到一个共享队列上，然后push到后端的中央存储系统上。当中央存储系统出现故障时，scribe可以暂时把日志写到本地文件中，待中央存储系统恢复性能后，scribe把本地日志续传到中央存储系统上。
需要注意的是，各个数据源须通过thrift向scribe传输数据（每条数据记录包含一个category和一个message）。可以在scribe配置用于监听端口的thrift线程数。在后端，scribe可以将不同category的数据存放到不同目录中，以便于进行分别处理。后端的日志存储方式可以是各种各样的store，包括file（文件），buffer（双层存储，一个主储存，一个副存储），network（另一个scribe服务器），bucket（包含多个store，通过hash的将数据存到不同store中），null(忽略数据)，thriftfile（写到一个Thrift TFileTransport文件中）和multi（把数据同时存放到不同store中）</p>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-14/99114695.jpg" width="500px" /></p>

<ul>
  <li>puma 使用支持Java UDF的类sql语言，目的在于快速开发部署</li>
  <li>swift 使用脚本语言支持流式处理，简单的API和checkpointing。</li>
  <li>stylus C++编写，类似Strom的流处理引擎。</li>
  <li>Laser 高吞吐低延迟的内存KV存储，基于RocksDB（基于Google的LevelDB）<a href="https://influxdata.com/blog/benchmarking-leveldb-vs-rocksdb-vs-hyperleveldb-vs-lmdb-performance-for-influxdb/">benchmark</a></li>
  <li>Scuba fast slice-and-dice analysis data store，支ad-hoc query和可视化</li>
  <li>Hive  数据仓库</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/55531065.jpg" width="600px" /></p>

<ul>
  <li>Language paradigm
    <ul>
      <li>声明语言，SQL，编写作为最简单和快速，群众基础好。缺点是表达能力不够。</li>
      <li>函数语言，使用预制的算子串联处理过程，简单并且对算子的组合增加表达能力。</li>
      <li>过程语言，C++，JAVA，Python，灵活，性能最高，对数据结构和执行过程有完全掌控力，但是要求也最高。S4/Storm/Heron/Samza。</li>
    </ul>
  </li>
  <li>Data transfer
    <ul>
      <li>直接传输，RPC或者内存消息队列。延迟少，毫秒级。</li>
      <li>Broker，另外一个broker进程处理数据传输，增加overhead但是扩展性好。支持多对多的传输，支持反压。典型的是Heron。</li>
      <li>持久存储，persistent message bus。多路复用，输入输出不同的速度，不同的时间点，重复读。</li>
    </ul>

    <p>邻近节点处于DAG中，两种依赖关系，窄依赖和宽依赖，实现在数据传输中。</p>
  </li>
  <li>
    <p>Processing semantics</p>

    <p>stream processor有三种行为：</p>

    <ul>
      <li>处理input event，反序列化输入event，查询外部系统，更新内存状态，并没有side-effect</li>
      <li>输出产生，根据input event和内存状态，向下游产生输出。可以在检查点之前或之后。</li>
      <li>保存检查点，内存状态、input stream offset、output value</li>
    </ul>

    <p>根据这些行为的实现方式，有两种语义信息：</p>

    <ul>
      <li>状态语义，input event是at-least once, at-most once, exactly</li>
      <li>输出语义，output value….</li>
    </ul>

    <p>无状态的处理器只有输出语义，有状态的有两种语义。状态语义只依赖于保存offset和内存状态的顺序。</p>

    <ul>
      <li>at-least-once，先内存状态后offset</li>
      <li>at-most-once，先offset，再内存状态</li>
      <li>exactly，原子的</li>
    </ul>

    <p>输出语义依赖数据保存到checkpint的顺序：</p>

    <ul>
      <li>at-least-once，先emit output再offset和内存状态</li>
      <li>at-most-once，先offset和内存状态，再emit output</li>
      <li>exactly，原子的事务性</li>
    </ul>

    <p>可以做一些side-effect-free的操作加速吞吐量，例如保存checkpoint的同时进行反序列化。</p>
  </li>
  <li>
    <p>State-saving mechanism </p>

    <ul>
      <li>Replication</li>
      <li>local database persistence</li>
      <li>remote database persistence</li>
      <li>upstream backup</li>
      <li>global consistent snapshot</li>
    </ul>
  </li>
  <li>
    <p>Backfill processing</p>

    <ul>
      <li>stream only.</li>
      <li>保存两个系统，一个用于batch，一个用于流处理</li>
      <li>开发流处理系统可以运行在batch环境中，spark streaming 和 flink. </li>
    </ul>
  </li>
</ul>

<h3 id="lessons-learned">Lessons learned</h3>

<ol>
  <li>拥有多个不同的实时处理系统是有好处的. “Writing a simple application lets our users deploy something quickly and prove its value first, then invest the time in building a more complex and robust application.”</li>
  <li>回放对于debug来说很重要.</li>
  <li>“Writing a new streaming application requires more than writing the application code. The ease or hassle of deploying and maintaining the application is equally important. Laser and Puma apps are deployed as a service. Stylus apps are owned by the individual teams who write them, but we provide a standard framework for monitoring them.” Making Puma apps self-service was key to scaling to the hundreds of data pipelines using Puma today.</li>
  <li>使用告警来检查应用的处理速度低于输入速度，未来考虑自动缩放。</li>
  <li>Streaming versus batch processing does not need to be an either/or decision.</li>
  <li>易用性很重要.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Funny Facts about Rating]]></title>
    <link href="http://billowkiller.github.io/blog/2016/04/09/rating/"/>
    <updated>2016-04-09T16:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/04/09/rating</id>
    <content type="html"><![CDATA[<p>本文分析Wilson score confidence interval，它在Reddit评论评分中的应用，另外分析了Hacker News、Reddit的评分算法。先来看下错误的评分。</p>

<p>Case 1: 网站的内容排名，得分高的排前面，得分低的排后面。如何计算得分。</p>

<p><strong>Solution 1:  Score = (Positive ratings) - (Negative ratings)</strong></p>

<u>未考虑内容的受欢迎程度。</u>

<!--more-->

<blockquote>
  <p>Suppose one item has 600 positive ratings and 400 negative ratings: 60% positive. Suppose item two has 5,500 positive ratings and 4,500 negative ratings: 55% positive. This algorithm puts item two above item one.</p>
</blockquote>

<p><strong>Solution 2: Score = Average rating = (Positive ratings) / (Total ratings)</strong></p>

<u>数据较多的时候比较适合，但是数据量少的时候会有比较大的Bias。</u>

<blockquote>
  <p>Average rating works fine if you always have a ton of ratings, but suppose item 1 has 2 positive ratings and 0 negative ratings. Suppose item 2 has 100 positive ratings and 1 negative rating. This algorithm puts item two below item one.</p>
</blockquote>

<p><strong>Correct Solution: Score = Lower bound of <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval">Wilson score confidence interval</a> for a Bernoulli parameter</strong></p>

<u>We need to balance the proportion of positive ratings with the uncertainty of a small number of observations. </u>

<p><img src="http://www.evanmiller.org/images/rating-equation.png" alt="" /></p>

<p>Here $\hat{p}$ is the observed fraction of positive ratings, $z_{\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution, and $n$ is the total number of ratings. </p>

<p>上式适合于二元评分机制，并不适用于例如5-star评分。</p>

<p>Wilson score confidence interval除了排序问题，还可以应用在如下地方：</p>

<ul>
  <li>Detect spam/abuse: What percentage of people who see this item will mark it as spam?</li>
  <li>Create a “best of” list: What percentage of people who see this item will mark it as “best of”?</li>
  <li>Create a “Most emailed” list: What percentage of people who see this page will click “Email”?</li>
</ul>

<h3 id="hacker-news-ranking-algorithm">Hacker News ranking algorithm</h3>

<p>Hacker News由Lisp的一种方言Arc实现，代码现在是开源的。它实现的内容得分公式为：</p>

<script type="math/tex; mode=display">Score = \frac{P-1}{(T+2)^G}</script>

<ul>
  <li>P = points of an item (and -1 is to negate submitters vote)</li>
  <li>T = time since submission (in hours)</li>
  <li>G = Gravity, defaults to 1.8</li>
</ul>

<p>Gravity和Time对内容得分有很大影响：</p>

<ul>
  <li>T增加导致得分降低，这是新闻老化的效果</li>
  <li>当Gravity增加的时候，上述降低会来的更快</li>
</ul>

<p>查看Time和Gravity的影响，在<a href="http://www.wolframalpha.com">Wolfram Alpha</a>的输入框中输入：</p>

<pre><code>plot(
    (30 - 1) / (t + 2)^1.8, 
    (60 - 1) / (t + 2)^1.8,
    (200 - 1) / (t + 2)^1.8
) where t=0..24
</code></pre>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/75012257.jpg" alt="" /></p>

<pre><code>plot(
    (p - 1) / (t + 2)^1.8, 
    (p - 1) / (t + 2)^0.5,
    (p - 1) / (t + 2)^2.0
) where t=0..24, p=10
</code></pre>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/43469961.jpg" alt="" /></p>

<h3 id="reddit-ranking-algorithm">Reddit ranking algorithm</h3>

<p>在Hacker News中新闻和评论都是采用同样的评分算法，而在Reddit中，它们采用不同的算法。</p>

<p>Reddit 同样是开源的，并且使用Python实现，为了加速计算，评分算法使用 Pyrex 实现。先来看下新闻的评分算法，hot algorithm：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/36257254.jpg" alt="" /></p>

<ul>
  <li>同样，submission time对评分的影响很大，人们总是喜欢新鲜的事情</li>
  <li>和Hacker News不同，新闻评分并不会随着时间流逝而降低，虽然它会比较新的新闻得分低。</li>
</ul>

<p>对于同样的votes，但是submission time不同的新闻，它们的得分可以参考：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/83965192.jpg" alt="" /></p>

<p>公式中的logarithm表示，最开始的votes拥有比较大的权重：开始的10 upvotes和接下的100 upvotes具有相同的权重，接下是 1000 upvotes。这就是upvotes的平滑效果。</p>

<p>比较有意思的是Reddit中的downvotes，它认为具有比较多upvotes和downvotes的新闻代表争议，它们的得分应该会比只有upvotes的新闻得分低。看看downvotes的影响：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/95155391.jpg" alt="" /></p>

<p>下面看下Reddit的评论的评分算法：</p>

<p>Reddit认为hot algorithm并不适用于comment，原因如下：</p>

<ul>
  <li>In a comment system you want to rank the best comments highest regardless of their submission time</li>
</ul>

<p>Reddit用到我们上面提到过的Wilson score interval，由这个interval得到the confidence sort，解释如下：</p>

<ul>
  <li>The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone</li>
  <li>The confidence sort gives a comment a provisional ranking that it is 85% sure it will get to</li>
  <li>The more votes, the closer the 85% confidence score gets to the actual score</li>
  <li>Wilson’s interval has good properties for a small number of trials and/or an extreme probability</li>
</ul>

<blockquote>
  <p>If a comment has one upvote and zero downvotes, it has a 100% upvote rate, but since there’s not very much data, the system will keep it near the bottom. But if it has 10 upvotes and only 1 downvote, the system might have enough confidence to place it above something with 40 upvotes and 20 downvotes — figuring that by the time it’s also gotten 40 upvotes, it’s almost certain it will have fewer than 20 downvotes. And the best part is that if it’s wrong (which it is 15% of the time), it will quickly get more data, since the comment with less data is near the top.</p>
</blockquote>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/41802353.jpg" alt="" /></p>

<h3 id="references">REFERENCES</h3>

<p><a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html">How Not To Sort By Average Rating</a></p>

<p><a href="https://medium.com/hacking-and-gonzo/how-hacker-news-ranking-algorithm-works-1d9b0cf2c08d#.utg0mohjf">How Hacker News ranking algorithm works</a></p>

<p><a href="https://medium.com/hacking-and-gonzo/how-reddit-ranking-algorithms-work-ef111e33d0d9#.ub7osc36n">How Reddit ranking algorithms work</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/04/06/kafka/"/>
    <updated>2016-04-06T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/04/06/kafka</id>
    <content type="html"><![CDATA[<p>Kafka最早是由LinkedIn开发的一个分布式发布-订阅消息系统，现在已经是Apache的一个开源项目。它具有以下的一些特点：</p>

<ul>
  <li>作为分布式系统，很容易 scale out</li>
  <li>消息以时间复杂度为O(1)的方式持久化到磁盘，支持离线消费和实时消费</li>
  <li>发布和订阅都支持高吞吐量，单机支持每秒100K条以上消息的传输</li>
  <li>支持多个订阅端，并且可以在异常情况下自动对这些消费者进行负载均衡</li>
</ul>

<p>消息系统的好处包括：</p>

<ul>
  <li>解耦生产者和消费者</li>
  <li>持久化直到消息已经被完全处理</li>
  <li>扩展性</li>
  <li>灵活性 &amp; 峰值处理能力</li>
  <li>可恢复性，系统的一部分组件失效时，不会影响到整个系统。</li>
  <li>顺序保证</li>
  <li>缓冲，有助于控制和优化数据流经过系统的速度。</li>
  <li>异步通信</li>
</ul>

<!--more-->

<h2 id="section">名词解释</h2>

<ul>
  <li>
    <p>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</p>
  </li>
  <li>
    <p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>
  </li>
  <li>
    <p>Partition：Parition是物理上的概念，每个Topic包含一个或多个Partition.</p>
  </li>
  <li>
    <p>Producer：负责发布消息到Kafka broker</p>
  </li>
  <li>
    <p>Consumer：消息消费者，向Kafka broker读取消息的客户端。</p>
  </li>
  <li>
    <p>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>
  </li>
</ul>

<h2 id="section-1">架构图</h2>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310020.png" width="500px" /></p>

<p>如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310025.png" width="500px" /></p>

<p>上图示意消费者和生产者的工作方式。同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>

<p><img src="http://sookocheff.com/img/kafka/kafka-in-a-nutshell/log-anatomy.png" width="500px" /></p>

<p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。每条消息都被append到某个Partition中，具体存储到哪一个Partition是根据Partition机制。如果Partition机制比较合理，不同的消息可以并行写入不同broker的不同Partition里，能极大的提高了吞吐率。另因为磁盘限制，Kafka提供两种策略删除旧数据：一是基于时间，二是基于Partition文件大小。</p>

<h3 id="kafka-delivery-guarantee">Kafka delivery guarantee</h3>

<p>Producer向broker发送消息时，默认情况下一条消息从Producer到broker是确保了 <code>At least once</code>，但如果设置Producer为异步发送则实现 <code>At most once</code>。<code>Exactly once</code>还未实现（生成一种消息主键，幂等重试）。</p>

<p>Consumer在从broker读取消息后，可以选择<code>autocommit</code>或<code>手动commit</code>。区别在于一个是读完消息先commit再处理消息，一个是读完消息先处理再commit。前者实现 <code>At most once</code>，后者实现 <code>At least once</code>，但如果消息的处理有幂等性，则可以理解为<code>Exactly once</code>。如果要做到严格<code>Exactly once</code>，则让offset和操作输入存在同一个地方，<strong>保证数据的输出和offset的更新要么都完成，要么都不完成</strong>，参考Spark Kafka的<code>DirectAPI</code>的实现以及<code>Druid</code>。</p>

<p>Kafka允许的<code>Exactly once</code>语义其实和它的特性有很大关系：
* 每条消息有序和不可变的放在partition中，同时分配一个递增的offset。这样通过&lt;partition, offset&gt;就可以确定一条消息，并且它的前辈和后辈都是不变的。
* 消费者拉数据，从而由消费者自身进行流控。
* 消费者可以根据&lt;partition, offset&gt;寻找消息，允许message rewind，并且根据消息的metadata可以保证消息接收的不重不丢。</p>

<h2 id="high-available">High Available</h2>

<p>Kafka的HA包括Data Replication和Leader Election两方面。</p>

<h3 id="data-replication">Data Replication</h3>

<p><img src="http://cdn4.infoqstatic.com/statics_s2_20160405-0343u1/resource/articles/kafka-analysis-part-2/zh/resources/0416000.png" width="500px" /></p>

<p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。Kafka分配Replica的算法如下：</p>

<ol>
  <li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>
  <li>将第i个Partition分配到第（i mod n）个Broker上</li>
  <li>将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</li>
</ol>

<p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR(in-sync replica)中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW(high watermark)并且向Producer发送ACK。HW会从leader持续发送到follower并被保存到每个broker的磁盘中。</p>

<p>对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p>

<p>Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。</p>

<h3 id="leader-election">Leader Election</h3>

<p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。对比Majority Vote则需要2f+1个Replica。</p>

<p>Kafka中，如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除，包括两种情况：长时间未向leader发送fetch request，消息lag超过阈值。
为了防止ISR里面的慢节点，Producer选择是否被commit阻塞。</p>

<p>选举时候，Kafka会在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。这种方式改善每个follower都使用zk watch的方法进行选举的问题：</p>

<ul>
  <li>brain split</li>
  <li>herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</li>
  <li>ZooKeeper负载过重 </li>
</ul>

<h2 id="kafka-">Kafka 网络模型</h2>

<p>Kafka使用的网络模型是典型的reactor模式，一个acceptor处理新来的连接请求，分配给N个processor处理，每个processor都有selector从socket中读取数据，生成request对象放到requestChannel中。requestChannel包含一个requestQueue和一个responseQueues，requestQueue是一个blocking queue，它的大小为<code>queued.max.requests</code>；responseQueues 包含N个blocking queue，对应每个processor。Kafka会有M个Handler threads用于处理responseQueues中的request，并且生成response放到对应的response队列中，处理过程如下：KafkaRequestHandler循环从RequestChannel中取Request并交给kafka.server.KafkaApis处理具体的业务逻辑。。</p>

<p>这里面涉及的数目在配置文件中都有体现，上述的每个acceptor包括processor在Kafka中称为NIO socket server，数量在<code>listeners</code>中定义，例如<code>PLAINTEXT://myhost:9092, SSL://:9091 </code>。N 取值 <code>background.threads</code>，M 取值 <code>num.io.threads</code>。</p>

<p>在 NIO socket server 中会给每个processor的responseQueue都注册一个ResponseListener，一旦有Response产生就会通知对应的processor发送Response到客户端。</p>

<h2 id="kafka-clients-operations">Kafka Clients’ Operations</h2>

<p>这一章节介绍一个Kafka client对于Kafka Resources可能的操作类型。</p>

<p>Operation包括以下几种：Read, Write, Create, Delete, Alter, Describe, ClusterAction。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/80246194.jpg" width="450px" /></p>

<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-4+-+Command+line+and+centralized+administrative+operations">KIP-4</a></p>

<h3 id="createdelete">Create/Delete</h3>

<p>Create/Delete就是创建和删除Topics。</p>

<p>具体过程如下：</p>

<ol>
  <li>broker发送<code>TopicMetadataRequest</code>到controller。</li>
  <li>Controller在ZooKeeper的/brokers/topics节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。</li>
  <li>对于删除Topic操作，Topic工具会将该Topic名字存于/admin/delete_topics。若delete.topic.enable为true，则Controller注册在/admin/delete_topics上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest，若为false则Controller不会在/admin/delete_topics上注册Watch，也就不会对该事件作出反应。</li>
  <li>对于创建Topic操作，Controller从/brokers/ids读取当前所有可用的Broker列表，对于set_p中的每一个Partition：
    <ul>
      <li>从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）</li>
      <li>将新的Leader和ISR写入/brokers/topics/[topic]/partitions/[partition]</li>
    </ul>
  </li>
  <li>直接通过RPC向相关的Broker发送LeaderAndISRRequest。</li>
</ol>

<p>创建Topic顺序图如下所示。</p>

<p><img src="http://cdn4.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-3/zh/resources/0606003.png" width="500px" /></p>

<p>有两点说明：</p>

<ul>
  <li>对于<code>auto.create.topics.enable=false</code>的Kafka，如果对未存在的topic进行produce，则会导致producer <code>org.apache.kafka.common.errors.TimeoutException</code>错误。</li>
  <li>除了使用broker进行创建Topic，还可以通过Kafka的AdminUtils直接指定zk、topic、partitions，replicationFactor，把相关信息写入zk来创建Topic。</li>
</ul>

<h3 id="alterdescribe">Alter/Describe</h3>

<p>alter/describe 是对配置修改或查看的操作，包括Topic和Client两个部分。改变方法也是通过Kafka的AdminUtils和ConfigCommand，指定zk、topic或要修改的properties。</p>

<h3 id="clusteraction">ClusterAction</h3>

<p>包括LeaderAndIsrRequest，StopReplicaRequest，UpdateMetadataRequest，ControlledShutdownRequest</p>

<h3 id="read">Read</h3>

<p>consumer 订阅Topic数据。consumer根据consumer group的分配算法如下</p>

<pre><code>rebalance process for consumer C_i in group G
For each topic T that C_i subscribes to {
 remove partitions owned by C_i from the ownership registry
 read the broker and the consumer registries from Zookeeper
 compute P_T = partitions available in all brokers under topic T
 compute C_T = all consumers in G that subscribe to topic T
 sort P_T and C_T
 let j be the index position of C_i in C_T and let N = |P_T|/|C_T|
 assign partitions from j*N to (j+1)*N - 1 in P_T to consumer C_i
 for each assigned partition p {
 set the owner of p to C_i in the ownership registry
 let O_p = the offset of partition p stored in the offset registry
 invoke a thread to pull data in partition p from offset O_p
 }
}
</code></pre>

<p>当订阅topic的时候，kafka会注册一个<code>ConsumerRebalanceListener</code>，当发生以下任何一个事件的时候，会引发上述的rebalance算法：</p>

<ul>
  <li>Number of partitions change for any of the subscribed list of topics</li>
  <li>Topic is created or deleted</li>
  <li>An existing member of the consumer group dies</li>
  <li>A new member is added to an existing consumer group via the join API</li>
</ul>

<p>Kafka的Consumer API可以透明地处理上述情况，另外还包括server的fail，partition的聚合。</p>

<p>读取数据发生在<code>poll</code>函数中，每次调用时，consumer都会使用上次消费的offset作为这次的起始offset，并且顺序的读取记录。当然上次消费的offset也可以由<code>seek</code>函数直接指定。</p>

<p>读取数据在Kafka内部是一个FetchRequest，处理的过程为
* authorize
* FetchResponse
* recordAndMaybeThrottle(quota控制)</p>

<h3 id="write">Write</h3>

<p>producer 发布数据的动作。produce的api接口可以选择同步或异步，默认情况下<code>send</code>接口是异步的，它将数据放到缓冲区后就立即返回，等到数据到达一定带下后再发送出去，节约IO开销。可以直接在send()后调用get()，这样可以达到同步的效果。另外一个重要的参数是acks，用来设置produce 请求完成的标准，也就是数据要写到几个broker中才算是完成。</p>

<p>发布数据在Kafka内部是一个ProducerRequest，处理过程为：</p>

<ul>
  <li>authorize</li>
  <li>produceResponse</li>
  <li>recordAndMaybeThrottle(quota控制)</li>
</ul>

<h3 id="partition-and-key">Partition and Key</h3>

<p>这里说下partition和key的关系。注意到，在kafka的api中会有一个key的概念，而实际上kafka只是一个消息的订阅和发布系统，和key应该扯不上一点关系，那么这个key有什么用呢，不用key会有什么影响。</p>

<p>看下没有指定key或者key为null的时候kafka是怎么处理的。首先kafka会随机选择一个partition，然后在一个默认的时间（10min）内所有的数据都会写到这个partition内。这会造成数据不均衡的分布在各个partition中。这时可以通过减少metadata refresh interval 缩短这个默认时间来减轻数据不均衡的现象。</p>

<p>不过更实际的还是指定一个key，因为kafka默认的是使用hashing-based partitioner，可能还会造成数据不均衡。这时候就需要使用自定义的partition，并指定<code>partitioner.class</code>。</p>

<h2 id="authorize">Authorize</h2>

<h3 id="authorizer">Authorizer接口</h3>

<p>Kafka带有Authorizer接口，这个是所有实现授权的插件都必须要实现的接口。启动的时候会读<code>authorizer.class</code>配置，<code>authorizer.class</code>就是实现授权的具体类。</p>

<p>Kafka的授权逻辑是<code>Principal P is [Allowed/Denied] Operation O From Host H On Resource R</code>，P是用户，O是上文提到的各种Operation，Host就是client地址，R是Kafka资源，包括cluster、topic、consumer-group。</p>

<p>Kafka本身自带一个<code>SimpleAclAuthorizer</code>，用它来实现一些简单资源的访问，例如</p>

<blockquote>
  <p>Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from IP 198.51.100.0 and IP 198.51.100.1</p>
</blockquote>

<h3 id="ranger">Ranger</h3>

<p>Ranger的Kafka plugin也是实现了Authorizer接口。Ranger的实现机制简单的介绍下，Ranger整体分为Admin和Plugin：</p>

<ol>
  <li>Ranger Plugin运行在服务进程内，在Kafka中，Ranger plugin代码就运行在broker内。</li>
  <li>Policy通过Ranger Admin存储在database中，plugin轮询地向admin请求最新的policy；policy存储在本地的一个文件中。</li>
  <li>在service请求到来的时候，ranger plugin中的policy engine会evaluate request，判断是否合法。</li>
</ol>

<p>Ranger的policy engine分为role based和tag based，kafka中使用的是tag based，evaluae的流程图如下：</p>

<p><img src="https://cwiki.apache.org/confluence/download/attachments/61322361/Ranger-Policy-Evaluation-Flow-with-Tags.png?version=2&amp;modificationDate=1444869949000&amp;api=v2" width="600px" /></p>

<p>Ranger Kafka 目前支持的功能还是比较少的，如下图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/11458053.jpg" width="600px" /></p>

<p>这些功能在Kafka自带的<code>SimpleAclAuthorizer</code>都是可以实现的。</p>

<h2 id="saslkerberos-and-ssl-implementation">Sasl/Kerberos and SSL implementation</h2>

<p>sasl 是broker的认证机制，ssl是数据传输的加密和认证机制。从协议的角度来说，kafka支持以下四种：</p>

<ul>
  <li>
    <p>PLAINTEXT (non-authenticated, non-encrypted)</p>

    <p>This channel will provide exact behavior for communication channels as previous releases</p>
  </li>
  <li>
    <p>SSL</p>

    <p>SSL  implementation. Authenticated principal in the session will be from the certificate presented or the peer host. </p>
  </li>
  <li>
    <p>SASL+PLAINTEXT</p>

    <p>SASL authentication will be used over plaintext channel. Once the sasl authentication established between client and server . Session will have client’s principal as authenticated user. There won’t be any wire encryption in this case as all the channel communication will be over plain text .</p>
  </li>
  <li>
    <p>SASL+SSL</p>

    <p>SSL will be established initially and  SASL authentication will be done over SSL. Once SASL authentication is established users principal will be used as authenticated user .  This option is useful if users want to use SASL authentication ( for example kerberos ) with wire encryption.</p>
  </li>
</ul>

<p>实现SSL需要做如下配置：</p>

<ol>
  <li>Generate SSL key and certificate for each Kafka broker
    <ul>
      <li>Ensure that common name (CN) matches exactly with the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not the malicious one.</li>
    </ul>
  </li>
  <li>Creating your own CA</li>
  <li>Signing the certificate</li>
  <li>Configuring Kafka Brokers</li>
  <li>Configuring Kafka Clients
    <ul>
      <li>需要将生成的<code>kafka.client.truststore.jks</code>拷贝到client</li>
      <li>如果进行双向认证则还需要生成和配置<code>kafka.client.keystore.jks</code></li>
    </ul>
  </li>
</ol>

<p>实现SASL需要：</p>

<ol>
  <li>Kerberos
    <ul>
      <li>客户端需要安装 kerberos client</li>
    </ul>
  </li>
  <li>Create Kerberos Principals
    <ul>
      <li>需要对应的用户principal</li>
    </ul>
  </li>
  <li>Make sure all hosts can be reachable using hostnames</li>
</ol>

<p>具体过程参考<a href="http://kafka.apache.org/documentation.html#security">http://kafka.apache.org/documentation.html#security</a></p>

<p><strong>zookeeper安全性</strong></p>

<ol>
  <li>不开通ibgw（端口），bcc无法直接访问</li>
  <li>zookeeper限制ip段，</li>
  <li>增加zookeeper authentication</li>
</ol>

<p>对每个resource都应该能够实现管理和控制。</p>

<h2 id="references">REFERENCES:</h2>

<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin">https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin</a></p>

<p><a href="https://kafka.apache.org/090/configuration.html">https://kafka.apache.org/090/configuration.html</a></p>

<p><a href="https://kafka.apache.org/090/ops.html">https://kafka.apache.org/090/ops.html</a></p>

<p><a href="https://kafka.apache.org/090/security.html">https://kafka.apache.org/090/security.html</a></p>

<p><a href="http://kafka.apache.org/documentation.html">http://kafka.apache.org/documentation.html</a></p>

<p><a href="http://people.csail.mit.edu/matei/courses/2015/6.S897/readings/kafka.pdf">Kafka: A distributed messaging system for log processing</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Table Join Semantics]]></title>
    <link href="http://billowkiller.github.io/blog/2016/03/19/table-join/"/>
    <updated>2016-03-19T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/03/19/table-join</id>
    <content type="html"><![CDATA[<p>这篇文章主要是记录下各种不同 <code>join</code> 的语义。</p>

<p>在最高层面上，<code>join</code> 分为三种：</p>

<ul>
  <li>INNER</li>
  <li>OUTER</li>
  <li>CROSS</li>
</ul>

<!--more-->

<hr />

<ol>
  <li>
    <p><code>INNER JOIN</code> - fetches data if present in both the tables.</p>
  </li>
  <li>
    <p><code>OUTER JOIN</code> are of 3 types:</p>

    <ul>
      <li><code>LEFT OUTER JOIN</code> - fetches data if present in the left table.</li>
      <li><code>RIGHT OUTER JOIN</code> - fetches data if present in the right table.</li>
      <li><code>FULL OUTER JOIN</code> - fetches data if present in either of the two tables.</li>
    </ul>
  </li>
  <li>
    <p><code>CROSS JOIN</code>, as the name suggests, does [n X m] that joins everything to everything. Similar to scenario where we simply lists the tables for joining (in the FROM clause of the SELECT statement), using commas to separate them.</p>
  </li>
</ol>

<p>重点关注前两种，因为在语法中它们包含 <code>join</code> 这个关键词。语法如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&lt;join_type&gt; ::= 
</span><span class="line">    [ { INNER | { { LEFT | RIGHT | FULL } [ OUTER ] } } [ &lt;join_hint&gt; ] ]
</span><span class="line">    JOIN</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>OUTER</code>是一个可选词，有没有对于语义并没有什么区别。解释下上面的语法，可以看到以下的用法是等价的：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">A LEFT JOIN B            A LEFT OUTER JOIN B
</span><span class="line">A RIGHT JOIN B           A RIGHT OUTER JOIN B
</span><span class="line">A FULL JOIN B            A FULL OUTER JOIN B
</span><span class="line">A INNER JOIN B           A JOIN B</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><img src="http://i.stack.imgur.com/qje6o.png" width="500px" /></p>

<p>结果可以看到是这样的：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Table A | Table B     Table A | Table B      Table A | Table B      Table A | Table B
</span><span class="line">   1    |   5            1    |   1             1    |   1             1    |   1
</span><span class="line">   2    |   1            2    |   2             2    |   2             2    |   2
</span><span class="line">   3    |   6            3    |  null           3    |  null           -    |   -
</span><span class="line">   4    |   2            4    |  null           4    |  null           -    |   -
</span><span class="line">                        null  |   5             -    |   -            null  |   5
</span><span class="line">                        null  |   6             -    |   -            null  |   6
</span><span class="line">
</span><span class="line">                      OUTER JOIN (FULL)     LEFT OUTER (partial)   RIGHT OUTER (partial)</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/01/spark-tunning/"/>
    <updated>2016-02-01T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/01/spark-tunning</id>
    <content type="html"><![CDATA[<p>记录一些spark的调优参数。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-1/51172915.jpg" width="500px" /></p>

<!--more-->

<ol>
  <li>
    <p>一个集群有N个节点，每个拥有a cores，mGB的memory</p>

    <ul>
      <li>num-executor N*3-1， 每个节点包含3个executor，AM的那个节点两个</li>
      <li>executor-cores min(5, a-1)</li>
      <li>executor-memory   (m-1)/3 *  (1-spark.yarn.executor.memoryOverhead)，默认为0.07。</li>
    </ul>
  </li>
  <li>
    <p>并行化调优</p>

    <pre><code> 一个stage中task的数量，也就是处理数据的并行度。通常的配置为分配的core数目的2-3倍。task的数量过少会影响：
	
 * 在聚合操作上会产生更大的内存压力
 * 垃圾回收
 * 当记录在内存中无法装载时会spill to disk，磁盘IO和排序。

 解决办法：
	
 * 配置INputFormat产生更多的splits。
 * 用更小的block size将输入数据写到HDFS上。
 * repartition transformation, triggering a shuffle: `var rdd2 = rdd2.reduceByKey(_ + _, numPartitions = X)`
</code></pre>

    <ul>
      <li>Spark的RDD的partition个数创建task的个数是对应的</li>
      <li>Partition的个数在spark的RDD中由block的个数决定的</li>
      <li>尽可能地增加task的数量使得每个task的数据可以装入分配的内存中。
 Spark能够非常有效的支持段时间任务（例如200ms)，因为他会对所有的任务复用JVM，这样能减小任务启动的消耗。所以，可以放心的使任务的并行度远大于集群的CPU核数。</li>
    </ul>

    <p>spark.default.parallelism: 控制Spark中的分布式shuffle过程默认使用的task数量，默认为8个。如果不做调整，数据量大时，就容易运行时间很长，甚至是出Exception，因为8个task无法handle那么多的数据。</p>

    <pre><code>     spark.default.parallelism 300
</code></pre>
  </li>
  <li>
    <p>减少Java数据结构的消耗，内存中存储未序列化的java对象，硬盘和网络用序列化的数据。</p>

    <ul>
      <li>使用org.apache.spark.serializer.KryoSerializer</li>
      <li>fastutil库为原始数据类型提供方便的集合类，且兼容Java标准类库。</li>
      <li>内存少于32G，设置JVM参数-XX:+UseCompressedOops以便将8字节指针修改成4字节，设置JVM参数-XX:+UseCompressedStrings以便采用8比特来编码每一个ASCII字符。</li>
    </ul>
  </li>
  <li>
    <p>driver默认内存改成2G（原先1G）；</p>
  </li>
  <li>
    <p>默认开启预测执行，缓解慢节点带来的计算任务慢的问题：</p>

 		spark.speculation true
 		spark.speculation.quantile 0.75
 		spark.speculation.multiplier 1.5
  </li>
  <li>
    <p>调整默认GC策略至G1 GC </p>

    <p>需要测试一些数据来看一下环境里最适合的GC策略，测试结果显示G1没有提升</p>

 		CMS vs parallel GC vs G1 GC
 		G1 GC: InitiatingHeapOccupancyPercent、 ConcGCThreads
  </li>
  <li>
    <p>timeout参数优化</p>

 		spark.core.connection.ack.wait.timeout 
 		spark.core.connection.auth.wait.timeout
 		spark.akka.timeout 
 		spark.akka.askTimeout 
 		spark.shuffle.io.connectionTimeout 
  </li>
  <li>
    <p>frameSize 优化（控制Spark中通信消息的最大容量如 task 的输出结果，默认为10M）</p>

 		spark.akka.frameSize 100
  </li>
  <li>
    <p>codec 优化（Cache和Shuffle数据压缩所采用的算法）</p>

 		spark.io.compression.codec org.apache.spark.io.LZ4CompressionCodec
  </li>
  <li>
    <p>gc log打印</p>

    <pre><code>	-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/var/logs/spark/spark_gc.log   
</code></pre>
  </li>
  <li>
    <p>heap dump</p>
  </li>
</ol>

 			-xx:+HeapDumpOnOutOfMemoryError

<ol>
  <li>调整permsize</li>
</ol>

 			–driver-java-options “-XX:MaxPermSize=512m”

<ol>
  <li>
    <p>native lzo</p>
  </li>
  <li>
    <p>减小数据结构内存</p>

    <p>内存少于32G，设置JVM参数-XX:+UseCompressedOops以便将8字节指针修改成4字节，设置JVM参数-XX:+UseCompressedStrings以便采用8比特来编码每一个ASCII字符</p>
  </li>
</ol>

<h3 id="spark-terasort-by-databricks">Spark Terasort by Databricks</h3>

<p>Spark sorted the same data <strong>3X faster using 10X fewer machines</strong>. All the sorting took place on disk (HDFS), without using Spark’s in-memory cache. </p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-1/9096505.jpg" width="600px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Extract-Transform-Load Application Scenarios]]></title>
    <link href="http://billowkiller.github.io/blog/2016/01/13/etl/"/>
    <updated>2016-01-13T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/01/13/etl</id>
    <content type="html"><![CDATA[<h1 id="etl">实时流ETL应用场景</h1>

<p>现有的企业级数据量在不断增大，用户也在寻求大数据解决方案来处理这些日益增长的数据。那么什么是大数据处理的架构呢。Cloudera总结的很好，大数据架构是建立在一系列开发可靠、可扩张、完整的自动化data pipeline上，下面的一张图给了很好的解释：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/11725472.jpg" width="500px" /></p>

<!--more-->

<p>data pipeline目的在于获取数据并能够发掘其中的价值。数据工程师决定数据从何处来，如何进入数据处理层，以及如何处理，如何存储和进一步展示。当然还需要包括必不可少的集群设计、系统调优等。上图中后端的分析工具通常为BI展示或其他分析工具，中间的处理通常由Spark、Hadoop进行，前端的数据获取包括批量和实时的两种。</p>

<p>在BMR中，我们已经为您处理了底层的集群设计、系统调优，打通数据交互层等工作，您只需要专注于如何在业务上挖掘数据潜在的价值即可。</p>

<h2 id="section">应用场景举例</h2>

<p>百度的IDMapping接入层每天的PV达到上亿，每天产生的日志量达到100GB，日志中的信息包括用户的访问IP、访问时间、响应时间、用户请求、应答内容等。IDMapping由10台nginx服务器构成、分别部署在不同的服务器上。其中的日志格式如下：</p>

<p>数据格式如下：</p>

<pre><code>$remote_addr - [$time_local] "$request" $status $body_bytes_sent "$http_referer"  $http_cookie" $remote_user "$http_user_agent" $request_time  $host $msec
</code></pre>

<p>下面是一条具体日志：</p>

<pre><code>10.81.78.220 - [04/Oct/2015:21:31:22 +0800] "GET /u2bmp.html?dm=37no6.com/003&amp;ac=1510042131161237772&amp;v=y88j6-1.0&amp;rnd=1510042131161237772&amp;ext_y88j6_tid=003&amp;ext_y88j6_uid=1510042131161237772 HTTP/1.1" 200 54 "-" "-" 9CA13069CB4D7B836DC0B8F8FD06F8AF "ImgoTV-iphone/4.5.3.150815 CFNetwork/672.1.13 Darwin/14.0.0" 0.004 test.com.org 1443965482.737
</code></pre>

<p>负责人希望能够通过这些日志信息实时地获取服务的PV、UV等统计信息以及访问用户IP的所在地信息等，并且希望可以查询任意时间的用户访问信息，以此满足日常运营的需求，后续还可能添加告警和日运营报表等功能。</p>

<h2 id="section-1">解决方案</h2>

<p>在BMR中我们集成了Flume、Kafka、Spark、Hbase组件，可以很好的满足应用场景中IDMapping负责人的集群需求。我们设计了如下的大数据处理的pipeline。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-26/97268179.jpg" width="750px" /></p>

<p>每台机器上的日志数据通过flume实时地推送到Kafka集群中，在spark集群中订阅这些日志数据，经过ETL处理后存储到Hbase中。前端展示系统可以通过Hbase的Restful接口实时的获取数据。同时，可以提交新的spark streaming application修改原有的数据处理模型，也可以在后端也可以对数据进一步加工：通过集群内部的mahout、spark mllib或对接其他的BI系统，例如Palo、Saiku。</p>

<p>以下是前端获取数据后的展示效果图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-24/88317070.jpg" width="350px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-23/77070404.jpg" width="400px" /></p>

<p>接下来通过三步骤对这个解决方案在BMR中的实现进行详细的阐述。</p>

<h3 id="step-1-">Step 1 创建集群</h3>

<p>创建包括Spark和Streaming组件的集群。在生产环境中建议分别创建Kafka和Spark集群。可以参考<a href="https://bce.baidu.com/doc/BMR/GettingStarted.html#.E5.88.9B.E5.BB.BA.E9.9B.86.E7.BE.A4">文档</a>。</p>

<h3 id="step-2-">Step 2 数据准备</h3>

<p>数据获取表示如何将nginx产生的日志通过flume导入到BMR集群中。我们执行下面的命令：</p>

<pre><code>wget http://bmr.bj.bcebos.com/tools/flume/flume-1.6.0.tar.gz
vim $FLUME_HOME/conf/flume-conf.properties
$FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/flume-conf.properties --name agent
</code></pre>

<p>以上的命令分别表示获取flume、编辑配置文件、运行flume agent。其中配置文件参考<a href="http://wiki.baidu.com/pages/viewpage.action?pageId=158727265">http://wiki.baidu.com/pages/viewpage.action?pageId=158727265</a>，将<code>agent.sources.s.command</code>改为<code>tail $NGINX_HOME/logs/access.log</code>。</p>

<h3 id="step-3-">Step 3 数据处理</h3>
<p>建立新的spark集群，当然在测试阶段您也可以直接使用kafka集群中的spark进行处理，在实际应用中推荐使用新的spark集群。</p>

<ol>
  <li>下载spark streaming代码，进行编译，将编译结果<code>bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</code>放到bos中。</li>
  <li>
    <p>从console页面进去到对应集群的作业列表页面，然后点击“添加作业”，如果使用系统提供的输入数据和jar包，可以按照如下方式填写参数：</p>

    <blockquote>
      <p>作业类型：Spark</p>
    </blockquote>

    <blockquote>
      <p>名称：FKSTest</p>
    </blockquote>

    <blockquote>
      <p>bos输入地址： bos://${PATH}/bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</p>
    </blockquote>

    <blockquote>
      <p>失败后操作：继续</p>
    </blockquote>

    <blockquote>
      <p>Spark-submit: –class com.baidubce.bmr.sample.DirectFKSTest</p>
    </blockquote>

    <blockquote>
      <p>应用程序参数：ng1889b62-master-instance-f5lvbago topic</p>
    </blockquote>

    <p>其中应用程序参数分别代表集群master的hostname和kafka topic。 </p>
  </li>
  <li>
    <p>您可以通过集群页面的<code>Resource Manager Web UI</code>查看spark UI查看作业运行的状态。（进入页面所需要的用户名密码会通过短信形式发送到您手机上）
 <img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/49196148.jpg" alt="" /></p>
  </li>
  <li>
    <p>查看hbase中的数据：</p>

    <pre><code> hbase(main):001:0&gt; list
 hbase(main):002:0&gt; scan 'PVUV', {COLUMN=&gt;['statistics:PV:toInt', 'statistics:UV:toInt']}
</code></pre>
  </li>
</ol>

<h2 id="spark-streaming">关于Spark Streaming中实时流的说明建议</h2>

<p>在Spark Streaming中有两种API用于处理与kafka之间的交互。</p>

<ul>
  <li>一种是spark1.2.0引进的<code>KafkaUtils.createStream</code>，这种方式可以将kafka或其他流式输入先写入磁盘再分片处理，防止重启driver造成数据丢失。换句话说，可以保证At least Once语义，前提是开启<code>Write Ahead Logs</code>，方法如下
    <ul>
      <li>在代码中通过<code>streamingContext.checkpoint</code>配置checkpoint目录</li>
      <li>配置<code>spark.streaming.receiver.writeAheadLog.enable</code>为<code>true</code></li>
    </ul>
  </li>
  <li>另外一种则是spark1.3.0引进的Direct API。这种方式保证的是<code>Exactly Once</code>语义，解决上种方式中<code>consumer offset</code>和数据Logs存储不一致性造成的数据重复计算。这种方式通过将<code>offset</code>存入
checkpoints中，来保证接收数据的一致性。</li>
</ul>

<p>使用<code>KafkaUtils.createStream</code>需要有一下两种考虑：</p>

<ul>
  <li>提高streaming的吞吐量，我们通常会使用多个consumer来并行的获取数据，每个consumer分配到一个executor的单核上，最后将所有得到的Stream进行<code>Union</code>操作。
如果不进行<code>Union</code>则会导致<code>Transformation</code>数量增多<code>#consumer</code>倍。</li>
  <li>另外也要考虑RDD中partition的数量，减少partition数量有助于减少task个数以及调度时间。partition的数量是由batchInterval和spark.streaming.blockInterval共同决定的，根据spark官方指导，通常partition数目
为cores的2到3倍比较合适，所以可以调整适当的参数控制partition的个数。</li>
</ul>

<p>而在DirectAPI中会自动定期的根据kafka的topic+partition查询最新的offset，定义需要处理的offset范围。所以不需要考虑创建多少receivers，也不需要考虑partition的数量。在API中每个kafka partition都是自动地并行读取，并且对应每个RDD partition，从而简化Streaming处理的并行模式。</p>

<p>但是DirectAPI并不会在zookeeper中更新offset，所以基于zookeeper的kafka监控工具无法查看日志处理的进度。但您也可以查询checkpoint，将offset写入zookeeper中。</p>

<p>这两种使用方式在Sample中都有详细的例子可以参考，分别是<code>com.baidubce.bmr.sample.FKSTest</code>和<code>com.baidubce.bmr.sample.DirectFKSTest</code>。</p>

<h2 id="section-2">总结</h2>

<p>虽然针对不同的目标和业务案例使用流式处理的方式也不同，但其主要场景包括：</p>

<ul>
  <li>流ETL——将数据推入存储系统之前对其进行清洗和聚合。</li>
  <li>触发器——实时检测异常行为并触发相关的处理逻辑。</li>
  <li>数据浓缩——将实时数据与静态数据浓缩成更为精炼的数据以用于实时分析。</li>
  <li>复杂会话和持续学习——将与实时会话相关的事件组合起来进行分析。</li>
</ul>

<p>在上述例子中我们介绍了BMR中流ETL的场景。
在BMR中，我们提供了Hadoop生态圈中的全栈组件包括Hadoop、Spark、Hbase、Hive、Pig、Kafka、Mahout等，
您可以根据自己的业务场景灵活地选择不同的组件。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/20/hadoop-intro/"/>
    <updated>2015-12-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/20/hadoop-intro</id>
    <content type="html"><![CDATA[<p>看图说话，用一些图表来记录Hadoop。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/22389512.jpg" width="500px" /></p>

<!--more-->

<h3 id="mapreduce">MapReduce</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/71525619.jpg" width="500px" /></p>

<p>schema-on-read表示hadoop更容易处理非结构化或半结构化的数据，因为可以在数据处理的时候进行解析，因此在加载数据的时候更加轻量化。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/99564088.jpg" width="500px" /></p>

<h3 id="hdfs">HDFS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/60582298.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/98566811.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/1953976.jpg" width="500px" /></p>

<p>NameNode维护整个文件系统的文件目录树，文件目录的元信息和文件的数据块索引。这些信息以 FSImage 和 EditLog 方式存储在本地文件系统中。 FSImage 是某一时刻的镜像，后续修改都是放在 EditLog 中。 Second NameNode 会间隔将 FSImage 和 EditLog 进行合并后放在 NameNode 上。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/94295030.jpg" width="500px" /></p>

<h3 id="yarn">Yarn</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/80769446.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/31902359.jpg" width="400px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37369292.jpg" width="500px" /></p>

<ol>
  <li>
    <p>Fair Scheduler
 Facebook开发的适合共享环境的调度器，支持多用户多分组管理，每个分组可以配置资源量，也可限制每个用户和每个分组中的并发运行作业数量；每个用户的作业有优先级，优先级越高分配的资源越多。</p>
  </li>
  <li>
    <p>Capacity Scheduler
 Yahoo开发的适合共享环境的调度器，支持多用户多队列管理，每个队列可以配置资源量，也可限制每个用户和每个队列的并发运行作业数量，也可限制每个作业使用的内存量；每个用户的作业有优先级，在单个队列中，作业按照先来先服务（实际上是先按照优先级，优先级相同的再按照作业提交时间）的原则进行调度。</p>
  </li>
</ol>

<p><strong>Fair Scheduler vs Capacity Scheduler</strong></p>

<ul>
  <li>
    <p>相同点</p>

    <ul>
      <li>均支持多用户多队列，即：适用于多用户共享集群的应用环境</li>
      <li>单个队列均支持优先级和FIFO调度方式</li>
      <li>均支持资源共享，即某个queue中的资源有剩余时，可共享给其他缺资源的queue</li>
    </ul>
  </li>
  <li>
    <p>不同点</p>

    <ul>
      <li>
        <p>核心调度策略不同。 计算能力调度器的调度策略是，先选择资源利用率低的queue，然后在queue中同时考虑FIFO和memory constraint因素；而公平调度器仅考虑公平，而公平是通过作业缺额体现的，调度器每次选择缺额最大的job（queue的资源量，job优先级等仅用于计算作业缺额）。</p>
      </li>
      <li>
        <p>内存约束。计算能力调度器调度job时会考虑作业的内存限制，为了满足某些特殊job的特殊内存需求，可能会为该job分配多个slot；而公平调度器对这种特殊的job无能为力，只能杀掉这种task。</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/75633723.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/56706884.jpg" width="450px" /></p>

<h3 id="hadoop-io">HADOOP IO</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/15159126.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/90632065.jpg" width="500px" /></p>

<p>其他流行的兼容Hadoop的序列化框架还有：Avro、Thrift、google protobuf</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/11927837.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/70087429.jpg" width="500px" /></p>

<h3 id="hadoop-jobs">HADOOP JOBS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/38335644.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37931472.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/10857296.jpg" width="500px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning/"/>
    <updated>2015-12-01T09:50:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning</id>
    <content type="html"><![CDATA[<p>Hadoop的调优涉及到整个MapReduce的各个过程，并且要对每个参数的意义和Counter信息有一定的了解。也就是说，根据Counter的信息推测哪些MapReduce阶段可能存在性能瓶颈，并且根据这个瓶颈理解对应的Hadoop 框架中的处理逻辑，进而可以调整相关参数的大小或程序的行为。</p>

<p>从大面上来看，MapReduce的瓶颈可能存在以下图中的各个部分。</p>

<p><img src="http://ww2.sinaimg.cn/large/74311666jw1eyjx8zpfz2j20rq0cegna.jpg" width="500px" /></p>

<!--more-->

<p>在开始调优之旅前，先来个开胃小菜。进行调优的过程中，我们首先要知道整个job的运行情况。</p>

<p>Hadoop 2中的JobHistory是能够提供作业运行时各个参数指标的展示工具，可以通过这个UI界面查看所有的Counter。另一方面如果不方便通过界面的方式查看，则可以利用Hadoop自带的命令行工具查看，方法是<code>hadoop job -history &lt;history file&gt;</code>, 这个history file的位置通常是在mapreduce.jobhistory.done-dir目录下，可以用如下方式查找<code>hadoop fs -lsr &lt;done-dir&gt; | grep job_1398974791337_0037</code>。</p>

<h2 id="map-optimizations">Map Optimizations</h2>

<p>在Map端的优化通常会涉及到输入的数据和它的处理过程，以及你的应用程序代码。Mapper需要读取作业的输入，输入文件的不同也会影响到作业运行的效率，例如文件是否是splittable，数据的本地性和input split的数量等等。</p>

<h3 id="data-locality">Data Locality</h3>

<p>在分布式计算中有条著名的准则<strong>Pushing compute to the data</strong>，map的task应该尽可能的被安排在数据存放的节点上。可以用Counter来判断作业是否符合这条准则:</p>

<ul>
  <li>HDFS_BYTES_READ：这个值应当不大于input file的block size</li>
  <li>DATA_LOCAL_MAPS：这个值应当为1</li>
  <li>RACK_LOCAL_MAPS：这个值应当为0</li>
</ul>

<p>以下几种情况可能会发生non-local read:</p>

<ul>
  <li>不能分割的大文件，这样mapper就必须从不同的节点中读取blocks。</li>
  <li>文件格式支持split，但是用的input format不对。典型的情况是LZOP格式，需要先建立索引后再进行读取。</li>
  <li>Yarn的调度器不能在某个节点上产生map container，通常是由于集群under load。</li>
</ul>

<p>解决方法是：</p>

<ul>
  <li>尽量保持unsplittable文件的大小接近一个block的大小。</li>
  <li>设置yarn的<code>scheduler.capacity.node-locality-delay</code>，引入跳过的调度次数，来增加map task分配到数据节点上的概率。</li>
  <li>使用Twitter提供的LZO Input Format来处理lzop数据，或者使用bzip2格式的文件。</li>
</ul>

<h3 id="map-number-overwhelm">Map Number Overwhelm</h3>

<p>当输入有很多的input split的时候，每个input split都需要一个mapper来执行，而每个mapper都是一个单独的进程。这样会给调度器和集群带来极大的压力。原因通常有两个：</p>

<ul>
  <li>input data由很多的小文件组成，Hadoop会为每个小文件生成一个mapper，最终时间会大量消耗在启动进程上。</li>
  <li>每个文件并不是很小（和block size相当），但总体的数据量很大，横跨上千个HDFS的blocks。这样每个block也会分配给单独的mapper。</li>
</ul>

<p>如果是第一种情况，可以先聚合这个小文件，或者使用类似avro的文件格式来存储。或者直接用<code>CombineFileInputFormat</code>来处理以上这两种情况，它可以在一个mapper里处理多个HDFS block的数据。<code>CombineFileInputFormat</code>会首先检查input files的所有blocks，简历每个block到data nodes的映射关系，接着将同一个节点上的blocks聚合到一个input split中以保持data locality。它有三个配置项来调整：</p>

<ul>
  <li>mapreduce.input.fileinputformat.split.minsize.per.node</li>
  <li>mapreduce.input.fileinputformat.split.minsize.per.rac</li>
  <li>mapreduce.input.fileinputformat.split.maxsize</li>
</ul>

<p>以上的默认配置会造成每个节点上尽量形成一个最大的input split，影响作业的并行性，可以用以上几个配置来调整。<code>CombineFileInputFormat</code>还包括两个具体的类：</p>

<ul>
  <li><code>CombineTextInputFormat</code></li>
  <li><code>CombineSequenceFileInputFormata</code></li>
</ul>

<h3 id="input-split-computation">Input Split Computation</h3>

<p>如果提交作业的client是在集群局域网之外，那么input split的计算可能带来高成本。</p>

<p>当输入的数据源是HDFS时，client需要做如下事情，包括file listing, file status retrieving，input files数量比较多的时候，整个过程带来数据传输的延迟是比较可观的。</p>

<p>可以通过设置<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>将input split的计算交给AppMaster处理，这是在集群内部进行的。</p>

<h2 id="shuffle-optimizations">Shuffle Optimizations</h2>

<h3 id="using-the-combiner">Using the Combiner</h3>

<p>combiner可以有效的减少mapper和reducer之间通信的数据量。</p>

<h3 id="using-binary-comparators">Using Binary Comparators</h3>

<p>MapReduce在做sorting或者merging的时候，使用<code>RawComparator</code>比较map output key。内置的<code>Writable</code> classes（<code>Text</code>、<code>IntWritable</code>）有byte-level的比较器，无需将二进制的数据重新组装成实际的对象，所以能够快速进行序列化对象的比较。</p>

<p>用户可以在自己构造的<code>Writable</code>对象里面实现<code>WritableComparable</code>接口，这处理起来会比较容易，但是另一方面要注意MapReduce中map output data是以byte的形式存储的，这会导致在shuffle和sort的阶段需要从byte到object的转化才可以完成对象的比较。</p>

<p>可以看到在Hadoop的内置<code>Writable</code>对象不仅实现了<code>WritableComparable</code>接口，还自定义继承自<code>WritableComparator</code>的比较器。<code>WritableComparator</code>有什么作用呢，可以看下它的一些方法申明。</p>

<pre><code>public class WritableComparator implements RawComparator {
    public int compare(byte[] b1, int s1, int l1,
                       byte[] b2, int s2, int l2); 
}
</code></pre>

<p>可以看到这是byte-level的Comparator，<code>Writable</code>对象正是覆盖了这里面的compare方法。在内置的<code>Writable</code>对象中都实现了<code>WritableComparator</code>，所以无需担心内置对象的效率。但是自己所构造的对象也可以实现<code>WritableComparator</code>方法来提高效率。</p>

<p>例如一个拥有firstName和lastName的Person对象：</p>

<pre><code>private String firstName;
private String lastName;

@Override
public void write(DataOutput out) throws IOException {
    out.writeUTF(lastName);
    out.writeUTF(firstName);
}
</code></pre>

<p><img src="http://i5.tietuku.com/e5eba32886b5773d.png" width="600px" /></p>

<pre><code>public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,
                     int l2) {
    int lastNameResult = compare(b1, s1, b2, s2);
    if (lastNameResult != 0) {
        return lastNameResult;
    }
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compare(b1, s1 + b1l1 + 2, b2, s2 + b2l1 + 2);
}

public static int compare(byte[] b1, int s1, byte[] b2, int s2) {
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compareBytes(b1, s1 + 2, b1l1, b2, s2 + 2, b2l1);
}

public static int readUnsignedShort(byte[] b, int offset) {
    int ch1 = b[offset];
    int ch2 = b[offset + 1];
    return (ch1 &lt;&lt; 8) + (ch2);
}
</code></pre>

<h3 id="tunning-the-shuffle-internals">Tunning the shuffle internals</h3>

<p>在mapper中，output record首先被存储在一个内存buffer中，当这个buffer增长到一定大小的时候，数据被spill到磁盘中的一个文件。整个过程持续到mapper完成所有的output record生成。过程如下：</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>在整个阶段中，I/O相关的splling和merging是最耗时的，所以理想状况应该是所有的output数据都可以装入buffer中，这样只有一个文件被spill到磁盘。这对所有的作业来说自然是不大可能，但是如果mapper可以通过filter或者project的方法减少input data，那么可以好好调整下<code>mapreduce.task.io.sort.mb</code>的大小，因为这个数据直接关系buffer的大小。可以通过检查下面的Counters来调整map端的shuffle：</p>

<ul>
  <li>MAP_OUTPUT_BYTES  用这个数据来估计是否可以调整<code>mapreduce.task.io.sort.mb</code>来装入map的output record</li>
  <li>SPILLED_RECORDS/MAP_OUTPUT_RECORDS  这两个数据的理想情况是一致的，表示只有一个spill发生。</li>
  <li>FILE_BYTES_READ/FILE_BYTES_WRITTEN  比较这两个数据和MAP_OUTPUT_BYTES可以理解在splling和merging阶段发生的读写副作用</li>
</ul>

<p>在reduce方面，map的output通过每个节点上运行的shuffle service进程被发送到对应的reducer。在reducer中，map output是被写入到一个内存buffer中，在数据接收的过程中buffer中的数据被排好序，并在到达一定数据量的时候写入磁盘。同时有一个后台进程负责不断merge这个小的spllied file到merged files中，当所有的fetcher获取了所有的outputs，会有一个最终的merging发生，这时候数据也就从merged files到reducer了。也就是如下图的这个过程：</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<p>通map端的调优一样，reduce端也是尽量将数据存入内存中，减少splling和merging发生的次数，但是这个过程并不如map端一样明显，因为数据是在边接收边merging的。默认情况下，无论数据是否可以装入内存中，splling总会发生，所以可以调整<code>mapreduce.reduce.merge.memtomem.enabled</code>为true启动memory-to-memory的merge。map端的Counter同样适用于reduce。</p>

<p>以下的参数可以用来调整Hadoop的shuffle行为：</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Default</th>
      <th>Map or Reduce</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mapreduce.task.io.sort.mb</td>
      <td>100 (MB)</td>
      <td>Map</td>
      <td>The total amount of buffer memory in megabytes to use when buffering map outputs. This should be approximately 70% of the map task’s heap size.</td>
    </tr>
    <tr>
      <td>mapreduce.map.sort.spill.percent</td>
      <td>0.8</td>
      <td>Map</td>
      <td>Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than 0.5.</td>
    </tr>
    <tr>
      <td>mapreduce.task.io.sort.factor</td>
      <td>10</td>
      <td>Map and Reduce</td>
      <td>The number of streams to merge at once while sorting files. This determines the number of open file handles. Larger clusters with 1,000 or more nodes can bump this up to 100.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.parallelcopies</td>
      <td>5</td>
      <td>Reduce</td>
      <td>The default number of parallel transfers run on the reduce side during the copy (shuffle) phase. Larger clusters with 1,000 or more nodes can bump this up to 20.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.input.buffer.percent</td>
      <td>0.7</td>
      <td>Reduce</td>
      <td>The percentage of memory to be allocated from the maximum heap size to store map outputs during the shuffle.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.merge.percent</td>
      <td>0.66</td>
      <td>Reduce</td>
      <td>The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.merge.memtomem.enabled</td>
      <td>false</td>
      <td>Reduce</td>
      <td>If all the map outputs for each reducer can be stored in memory, then set this property to true.</td>
    </tr>
  </tbody>
</table>

<p>Shuffle的原则是使用filter和project减少数据量，使用combiner以及压缩map的output，尽可能的减少mapper和reducer质检传递的数据，减低IO带来的开销。这样之后再利用上述提到的参数来调整Shuffle的过程。</p>

<h2 id="recuder-optimizations">Recuder Optimizations</h2>

<h3 id="the-number-of-reducers">The Number of Reducers</h3>

<p>大多数情况下map端的并行度是由框架根据input files和input format来自动决定的，但在Reduce端，reducer的数量是由用户自行决定的。太少的reducers意味着没能充分利用集群的资源，太多的reducer则会让调度器疲于奔命，如果没有太多的资源供所有reducer运行则会拖累reducer的执行效率。在一些使用场景中，不能避免的需要使用少量的reducer来运行作业，例如数据写入DB系统中。另外一些场景中需要确认数据是否会发生倾斜，以及如何partition的，数据量是否会让reducer发生OOM的情况。</p>

<h2 id="general-tunning-tips">General tunning tips</h2>

<ul>
  <li>压缩</li>
  <li>使用类似于Avro或Parquet的数据格式存储数据，带来的好处是空间利用率，序列化和反序列化更加有效。
    <ul>
      <li>在Hadoop中，text并不是一个有效的数据格式，空间利用率低，解析成本高，特别是用到正则的时候。</li>
      <li>尽可能考虑使用二进制的文件存储格式。</li>
    </ul>
  </li>
</ul>

<h2 id="tunning-tools">Tunning tools</h2>

<h3 id="stack-dumps">stack dumps</h3>

<p>ssh到task运行的机器上，执行下面的命令：</p>

<pre><code>ps aux | grep container_1393242034820_0001_01_000002
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
</code></pre>

<p><code>SIGQUIT</code>信号的发送应该要间隔几秒，当JVM收到这个信号的时候会执行stack dump，这样可以了解到程序在这段时间内的运行情况。最后可以在task 的output file中查看dump出来的栈信息。</p>

<h3 id="profiling-map-and-reduce-task">Profiling Map and Reduce Task</h3>

<p>可以使用HPROF结合一些Mapreduce job method来进行Profiling。HPROF是JVM内置的java profiling工具，Hadoop内置了对HPROF的支持。可以在driver中加入如下的代码：</p>

<pre><code>job.setProfileEnabled(true);
job.setProfileParams(
    "-agentlib:hprof=depth=8,cpu=samples,heap=sites,force=n," +
        "thread=y,verbose=n,file=%s");
job.setProfileTaskRange(true, "0,1,5-10");
job.setProfileTaskRange(false, "");
</code></pre>

<p>在<code>setProfileParams</code>方法中设置的参数会在每个container中建立一个名为profile.out的文件，这个文件可以很容易被解析。可以通过ssh到目标机器查看或者通过JobHistory UI界面查看。</p>

<p>profile.out包括一些stack traces，还包括内存和CPU时间的信息。以下是一个profile.out文件：</p>

<p><img src="http://i5.tietuku.com/ba1dcbebf426b1a8.png" width="600px" /></p>

<p>可以看出来第一个问题是在<code>String.split</code>这个方法的使用上，它采用正则表达式来分割字符串，这个是相当耗时的一个举措，可以用Apache Commons Lang library的<code>StringUtils.split</code>来替换。另外一个是发生在Text的构造上，可以只构造一个Text实例，采用<code>set</code>方法进行设置，这样会更加有效率。</p>

<p>需要注意，使用HPROF会给程序的执行带来额外的负担，需要持续的收集profiling的信息，所以在正常的运行过程中不应该加上。</p>

<h2 id="conclusion">Conclusion</h2>

<p>一些在工作中实际用到的调优方法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-30/1330399.jpg" width="900px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Secondary Sorting]]></title>
    <link href="http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting/"/>
    <updated>2015-11-22T17:27:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting</id>
    <content type="html"><![CDATA[<p>Hadoop MapReduce的神奇之处发生在mapper和reducer之间，将所有相同key的map输出记录聚集在一块，使得用户可以方便的处理聚合在一起的数据。Hadoop内部使用了partition、sort和merge（shuffle的一部分），在每个reducer中流式地得到排序后的key和value集合。在MapReduce Sorting中有个特别的部分是secondary sort，也就是对value进行排序。</p>

<!--more-->

<p>Secondary sort在两种情况下特别有用：</p>

<ul>
  <li>需要某一部分的数据比其他数据更快的到达reducer。</li>
  <li>希望job的输出按照两个key进行排序。</li>
</ul>

<p>实现Secondary sort需要对MapReduce中的数据流和处理有一定的了解，下图展示了对reducer中出现的数据有影响的三个部分。</p>

<p><img src="http://i5.tietuku.com/7ad3ad872415c4b6.png" width="600px" /></p>

<p><code>partitioner</code>决定哪个reducer接收该mapper数据记录；<code>sorting RawComparator</code>用于在各自的分片中排序输出的结果，map和reduce阶段都有它，其中map阶段的sorting是对reduce阶段sorting的一个优化，让reducer的sorting更高效；最后，<code>grouping RawComparator</code>用于决定reducer处理排序后记录的边界，发生在reducer从本地磁盘读取数据的时候，也就是说，你可以用这个方法决定数据记录是如何聚集起来调用一个reduce方法的。MapReduce默认把这个三个方法作用于map方法输出的key上。</p>

<p>要实现Secondary sorting，我们需要重写partitioner、sort comparator和grouping comparator。</p>

<p>下面，通过对人名的排序来说明如何使用Secondary sorting。使用primary sort排序last name，secondary sort排序first name。</p>

<p>我们需要构建一个由map方法输出的Composite key，这个key由两部分组成：</p>

<ul>
  <li>Natural Key</li>
  <li>Secondary Key</li>
</ul>

<p><img src="http://i5.tietuku.com/25eedf0319e92775.png" width="430px" /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Person</span> <span class="kd">implements</span> <span class="n">WritableComparable</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">firstName</span><span class="o">;</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">lastName</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">readFields</span><span class="o">(</span><span class="n">DataInput</span> <span class="n">in</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">firstName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">lastName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">write</span><span class="o">(</span><span class="n">DataOutput</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">lastName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>下图说明hadoop框架配置中用于设置partitioning、sorting和grouping类的名字和方法。</p>

<p><img src="http://i5.tietuku.com/520e7242cd6ecc43.png" width="530px" /></p>

<h3 id="partitioner">Partitioner</h3>

<p>默认的partitioner使用对key进行hash后取reducer个数的模。但是默认的partitioner使用整个key，会导致相同的natural key发往不同的reducer。所以需要实现自己的partitioner。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNamePartitioner</span> <span class="kd">extends</span>
</span><span class="line">    <span class="n">Partitioner</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numPartitions</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="k">return</span> <span class="n">Math</span><span class="o">.</span><span class="na">abs</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">*</span> <span class="mi">127</span><span class="o">)</span> <span class="o">%</span>
</span><span class="line">        <span class="n">numPartitions</span><span class="o">;</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="sorting">Sorting</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">w1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">w2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">
</span><span class="line">    <span class="kt">int</span> <span class="n">cmp</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">if</span> <span class="o">(</span><span class="n">cmp</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="k">return</span> <span class="n">cmp</span><span class="o">;</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">());</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="grouping">grouping</h3>

<p>grouping阶段所有的数据记录已经是secondary sort了，grouping comparator需要将相同的last name聚合在一起。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNameComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonNameComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">o1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">o2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="mapreduce">MapReduce</h3>

<p>最后在driver中，需要设置上文提到的三个类：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setPartitionerClass</span><span class="o">(</span><span class="n">PersonNamePartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setSortComparatorClass</span><span class="o">(</span><span class="n">PersonComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setGroupingComparatorClass</span><span class="o">(</span><span class="n">PersonNameComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Map</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">Person</span> <span class="n">outputKey</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Person</span><span class="o">();</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">Text</span> <span class="n">lastName</span><span class="o">,</span> <span class="n">Text</span> <span class="n">firstName</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">outputKey</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">lastName</span><span class="o">.</span><span class="na">toString</span><span class="o">(),</span> <span class="n">firstName</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class="line">    <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">outputKey</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Reduce</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="n">Text</span> <span class="n">lastName</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span>
</span><span class="line">                     <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">lastName</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">for</span> <span class="o">(</span><span class="n">Text</span> <span class="n">firstName</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">lastName</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Secondary sort涉及到的自定义的partitioner、sorter和grouper，还是比较复杂的。可以考虑<a href="http://htuple.org">htuple</a>对简单类型进行secondary sort。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming]]></title>
    <link href="http://billowkiller.github.io/blog/2015/10/27/spark-streaming/"/>
    <updated>2015-10-27T21:04:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/10/27/spark-streaming</id>
    <content type="html"><![CDATA[<p>流式计算通常是为了满足日益增长的数据的实时获取和低延时计算的需求。通常来说，一个优秀的流式计算引擎需要满足一下的一些需求：</p>

<ol>
  <li>不重不丢的保证（在节点或计算失败的时候，内存中的计算状态能被正确的恢复）</li>
  <li>低延迟</li>
  <li>高吞吐量</li>
  <li>强大的计算模型</li>
  <li>容错机制的低开销</li>
  <li>流控</li>
</ol>

<p><img src="https://spark.apache.org/images/spark-logo.png" width="200px" /></p>

<!--more-->

<p>介绍完了spark后就可以来说说spark streaming，毕竟spark streaming是完全构建在spark之上，熟悉了spark的RDD原理之后就比较容易理解spark streaming。说白了，spark streaming中的流式计算是<code>伪实时</code>的，之所以是伪实时的是因为它将实时的处理变成时间跨度较小的批量处理。没错，就是<code>将一段时间间隔中的数据变成RDD</code>，然后利用spark原有的架构处理这段时间内的数据，接着在时间维度上对这些处理后的RDD进行迭代计算。也就是将流式计算分解为一系列微小的、原子的批量作业，每个微批量作业如果失败则可以重新计算。这种分解的思想可以应用在批量计算框架、也可以应用在流式计算框架上，例如Storm Trident。</p>

<p>这样的处理带来了几个明显的好处：</p>

<ul>
  <li>高吞吐量</li>
  <li>不从不丢的保证</li>
  <li>快速falut recovery</li>
  <li>更方便处理慢节点</li>
  <li>实时和批量统一的编程接口</li>
</ul>

<p>下面介绍下spark streaming中的具体实现来理解这几点。</p>

<h3 id="section">计算模型</h3>

<p><img src="http://blog.selfup.cn/wp-content/uploads/2014/08/streaming-flow.png" alt="image" /></p>

<p>首先，Spark Streaming把实时输入数据流以时间片（如1秒）为单位切分成块。Spark Streaming会把每块数据作为一个RDD，并使用RDD操作处理每一小块数据。每个块都会生成一个Spark Job处理，最终结果也返回多块。</p>

<p>举个栗子来说明下这个过程：</p>

<pre><code>pageViews = readStream("http://..."， “ls”)
ones = pageViews.map(event =&gt; (event.url, 1))
counts = ones.runningReduce((a, b) =&gt; a+b)
</code></pre>

<p>上述代码的作用是根据URL的数量计算访问次数。处理的过程为首先通过HTTP获取到一个pageview事件的RDD，经过<em>transformation</em>操作变成(URL, 1), 最后的操作计算相同的URL数目。整个streaming过程可以用下图来表示：</p>

<p><img src="http://img-storage.qiniudn.com/15-10-18/95269436.jpg" width="500px" /></p>

<p>整个的处理过程可以看出，输入流被分成每个都是1秒的batch，经过处理后生成resultRDD，这个resultRDD在每个时间间隔中都会产生一个，并经过reduce迭代计算。在上图中最终的reduce的输入参数就包括上一个时间间隔的resultRDD和这个时间间隔中map操作的结果。上图也可以看出这些RDD的lineage graph，在节点失败的时候，可以根据这个lineage graph以partition的粒度为单位重新执行任务计算丢失的分片，可以看出这些计算的任务都是可以并行执行的。同时，对于慢节点来说，因为<code>计算是无状态的</code>，且每个job的结果是可以确定的，spark streaming可以执行类似于hadoop中的预测模型——在其他节点上计算同样的任务。另外，spark streaming也会有些checkpoint来防止无限制的恢复计算。</p>

<p>对比于其他流式处理的方式，spark streaming在处理失败任务和慢节点上无疑更有效率。它可以从<code>时间和partition</code>两个维度上并行地计算数据来加快恢复速度。而对于像Strom的流式处理来说，往往是通过<strong>上游数据backup</strong>或者<strong>同时复制执行相同的作业</strong>来保证数据处理的可靠性。这两种处理方式对于资源的消耗无疑都是巨大的，且恢复的时间也比较长。</p>

<ul>
  <li>其中对于前者来说，每个节点都需要保存上一个checkpoing后所发送数据的拷贝，在节点失败时由standby机器重新计算上游节点发送过来的数据，因为这些计算都是有状态的，所以恢复的时间比较长，Storm就只保证“at least once”语义来提高处理速度。Trident之所以能够保证不重不丢是使用了数据库来复制计算状态。</li>
  <li>对于后者无疑更加消耗资源，并且需要保证两个数据处理操作接受到的上游数据的顺序是一样的。</li>
</ul>

<p>同spark一样，只有当某个Output Operations原语被调用时，stream才会开始真正的计算过程。现阶段支持的Output方式有以下几种：</p>

<ul>
  <li>print()</li>
  <li>foreachRDD(func)</li>
  <li>saveAsObjectFiles(prefix, [suffix])</li>
  <li>saveAsTextFiles(prefix, [suffix])</li>
  <li>saveAsHadoopFiles(prefix, [suffix])</li>
</ul>

<h3 id="section-1">流式处理中的几点难点</h3>

<p>在流式处理中通常都会有几个难点需要考虑。</p>

<ul>
  <li>时间窗口问题</li>
  <li>数据一致性问题</li>
  <li>内存状态管理问题</li>
</ul>

<p>我们来看下spark streaming是如何解决的。</p>

<ol>
  <li>
    <p><strong>时间窗口问题。</strong>spark streaming是根据数据到达系统的时间将记录放到对应的RDD中，这种时间窗口划分是基于墙上时间的，好处可以保证系统及时产生一个新的batch运行job，并且可以让程序运行在数据生成的地方，不必再进行分发。</p>

    <p>这种基于墙上时间的统计有一个非常严重的问题是不能回放数据流。当数据流是实时产生的时候，“墙上时间”的一分钟也就只会有一分钟的event被产生出来。但是如果统计的数据流是基于历史event的，那么一分钟可以产生消费的event数量只受限于数据处理速度。另外event在分布式采集的时候也遇到有快有慢的问题，一分钟内产生的event未必可以在一分钟内精确到达统计端，这样就会因为采集的延迟波动影响统计数据的准确性。所以产生了另外一种时间窗口划分的方法。</p>

    <p>另一种时间窗口划分的方法是基于外部时间的，例如日志时间。spark streaming提供两种方法来处理这种情况：</p>

    <ul>
      <li>延迟处理，等待一定时间来处理每个batch。</li>
      <li>用户的应用程序中保证乱序事件的正确处理。</li>
    </ul>

    <p>以上也说明在<code>批处理的流式计算模型是受限的</code>，很多情况下只能依靠用户的应用程序来做处理，例如实时的统计5s内的pv；其次这种方式也没有很好的流控技术手段，如果有突发的大量数据产生，会导致结果产生的时间更长，甚至是将系统的JVM撑爆。最后实时性也是受限的，只能达到次秒级的处理延迟，毕竟是要等待一个时间batch的处理完成。</p>
  </li>
  <li>
    <p><strong>数据一致性问题。</strong>什么是数据一致性，举个栗子，要统计网站中来自各个国家的page view，把不同国家的pv统计放在不同的节点上处理。但是现在统计英国的节点处理速度要慢于法国的，这将导致两个节点上数据的时间状态不一致。在流式处理中，数据的一致性的保证同时意味着资源的消耗。流式处理的数据一致性有三种解决思路，在上文中也有提到，这里概括下：</p>

    <ul>
      <li>上游备份策略：重启的时候重放kafka的历史数据，恢复内存状态</li>
      <li>中间状态持久化：把统计的状态放到外部的持久的数据库里，不放内存里</li>
      <li>同时跑两份：同时有两个完全一样的统计任务，重启一个，另外一个还能正常运行。</li>
    </ul>

    <p>而在spark streaming中，数据一致性天然得到保证的。因为记录根据时间来分片，所以中间的resultRDD反应的是当前时间和之前时间所计算出来的结果，无论计算和结果被分配到哪个节点上都不会有节点间数据不一致的情况。也就是数据的不重不丢可以得到保证。</p>
  </li>
  <li>
    <p><strong>内存状态管理问题。</strong>
做流式统计的有两种做法：</p>

    <ul>
      <li>依赖于外部存储管理状态：比如没收到一个event，就往redis里发incr增1</li>
      <li>纯内存统计：在内存里设置一个counter，每收到一个event就+1</li>
    </ul>

    <p>第一种会把整个压力全部压到数据库上，造成处理速度下降；第二种的状态相对来说容易管理一些，计算直接是基于这个内存状态做的。如果重启丢失了，重放一段历史数据就可以重建出来。内存的问题是它总是不够用的，解决的方法是input分割和把存储移到外边去。在内存计算中把窗口统计的中间状态落地的好处是显而易见的：重启之后不用通过重算来恢复内存状态。但是这种对外部数据库使用不小心就会导致两个问题：</p>

    <ul>
      <li>处理速度慢。不用一些批量的操作，数据库操作很快就会变成瓶颈</li>
      <li>数据库的状态不一致。内存的状态重启了就丢失了，外部的状态重启之后不丢失。重放数据流就可能导致数据的重复统计</li>
    </ul>

    <p>在spark streaming中支持传统批量计算中的无状态<em>transformation</em>操作，例如<code>map</code>、<code>reduce</code>、<code>groupBy</code>和<code>join</code>。这就避免了普通流式计算中麻烦的状态保存问题。但spark streaming中也支持多个时间间隔中有状态的<em>transformation</em>操作，包括：</p>

    <ol>
      <li>Windowing: 生成滑动窗口RDD。<code>words.window("5s")</code>将产生一个RDD包含[0,5),[1,6),[2,7)的时间间隔。</li>
      <li>增量聚合：在滑动窗口的基础上进行RDD的聚合操作，也就是<code>reduceByWindow</code>。在下图的<em>a</em>中对应的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b)</code>，也就是计算5s内的计数之后。图<em>b</em>的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b, (a, b) =&gt; a-b)</code>。其实很简单，第一个lambda表达式为进入滑动窗口的处理函数，第二个表达式为离开滑动窗口的处理函数。这样也就不用重复求和了。
 <img src="http://img-storage.qiniudn.com/15-10-18/97873121.jpg" alt="" /></li>
      <li>状态跟踪：
 如下图所示，就是保存上一个时间间隔的RDD与本次的记录进行groupBy加map计算的到状态的变化情况。
 <img src="http://img-storage.qiniudn.com/15-10-18/91458919.jpg" alt="" /></li>
    </ol>

    <p>在对这些带状态的操作的处理过程中也就用到了上述的所属的利用外存在保存中间的状态。spark streaming中这只发生在intervel之间，所以整个内存的状态管理会比传统的流式处理简单许多，而且高效，不需要对每一步都进行状态同步，状态恢复的成本也比较低，上文中提到的可以在多个节点上并行计算恢复。</p>
  </li>
</ol>

<h3 id="system-architecture">System Architecture</h3>
<p><img src="http://img-storage.qiniudn.com/15-10-18/82954556.jpg" alt="" /></p>

<p>Spark streaming和Spark的系统结构有些许改动，如上图所示主要包括3个部分：</p>

<ul>
  <li><em>master</em> that tracks the D-Stream lineage graph and schedules tasks to compute new RDD partitions.</li>
  <li><em>Worker</em> nodes that receive data, store the partitions of input and computed RDDs, and execute tasks.</li>
  <li>A <em>client</em> library used to send data into the system.</li>
</ul>

<p>从上图中可以看出，Spark Streaming和传统的流式系统最大的区别就是Spark Streaming将计算分成小的，无状态的确定性的任务，这些任务会在集群的任意节点上运行。并且相对于传统流式系统的拓扑结构来说，无需消耗大量时间将将任务进行迁移，Spark Streaming可以很好的对机器上的节点进行负载均衡，处理失败任务并且对慢节点进行预测。</p>

<p>对比于Spark的系统，Spark Streaming做了一下的改进：</p>

<ul>
  <li>网络传输。使用异步I/O获取远端数据。</li>
  <li>TimeStep pipelining。Spark的调度器可以在当前任务还未完成的时候可以提交下一个时间分片的任务。</li>
  <li>任务调度：优化任务调度器，例如调整控制消息的大小，可以在每隔几百毫秒时间内启动几百个并行任务。</li>
  <li>存储层：支持异步的RDD checkpoint，RDD是不可变的，所以异步存储不会阻塞现有的计算。</li>
  <li>Lineage切割：控制RDD linage graph的大小，在checkpoint之前的lineage便可以删除。</li>
</ul>

<p>当master fail的时候可以进行HA，所有的worker重新连接到新的master上，将原来的checkpoint和原始数据重新计算。因为所有的操作都是确定性的，所以RDD是可以重复计算，也就是说在HA的时候丢失一些正在运行的计算任务不会对最终结果造成什么影响。所有的元数据都是存储在HDFS上的，包括：</p>

<ol>
  <li>RDD的lineage graph，代表用户代码的Scala函数对象。</li>
  <li>上一个checkpoint的时间</li>
  <li>RDD的ID。因为HDFS的checkpoint文件会在每个时间片重新命名。</li>
</ol>

<h3 id="faq">FAQ</h3>
<ol>
  <li>
    <p>Dstream与RDD之间的关系</p>

    <p>首先来看下Spark streaming的代码<code>val ssc = new StreamingContext(sc, Seconds(2))</code>。在这句的作用是定义Dstream生成的时间间隔，<code>2s</code>就是这个时间间隔，也叫<code>batch interval</code>。具体说来<strong>一个streaming batch对应一个RDD</strong>，也就是这个batch interval里产生的数据。</p>

    <p>在这个RDD中，有n个partition，n = batch interval / block interval。 <code>block interval</code>是spark steaming内部定义的一个变量<code>spark.streaming.blockInterval</code>，通常是200ms。上述例子就产生10个partitions。</p>

    <p>Blocks由一个receiver产生，receiver就是流式数据的接收端，每个receiver被分配到一个host上，所以上述的10个partitions就由一个node产生，同时被<code>复制到第二个节点上做容错</code>。注意，这里产生了data locality的问题。好的做法是，分配多个receivers接收数据，最后使用union合并数据做processing。当然还可以对Dstream做<code>repartition</code>操作提高并行度。</p>
  </li>
  <li>
    <p>Spark Streaming 的容错性</p>

    <p>对于文件这样的源数据，这个driver恢复机制足以做到零数据丢失，因为所有的数据都保存在了像HDFS或S3这样的容错文件系统中了。但对于像Kafka和Flume等其它数据源，有些接收到的数据还只缓存在内存中，尚未被处理，它们就有可能会丢失。这是由于Spark应用的分布操作方式引起的。当driver进程失败时，所有在standalone/yarn/mesos集群运行的executor，连同它们在内存中的所有数据，也同时被终止。</p>

    <p>首先driver会利用checkpoint来保存灾备需要的数据，有两种各类型的checkpoint，Metadata Checkpoint（保存streaming计算相关数据） 和 Data Checkpoint（保存生成RDD数据）</p>

    <p>对于Spark Streaming来说，从诸如Kafka和Flume的数据源接收到的所有数据，在它们处理完成之前，一直都缓存在executor的内存中。纵然driver重新启动，这些缓存的数据也不能被恢复。为了避免这种数据损失，Spark 1.2发布版本中引进了预写日志（Write Ahead Logs）功能。</p>

    <p>当启用了预写日志以后，所有收到的数据同时还保存到了容错文件系统的日志文件中。因此即使Spark Streaming失败，这些接收到的数据也不会丢失。另外，接收数据的正确性只在数据被预写到日志以后接收器才会确认，已经缓存但还没有保存的数据可以在driver重新启动之后由数据源再发送一次。这两个机制确保了零数据丢失，即所有的数据或者从日志中恢复，或者由数据源重发。</p>

    <p><a href="http://www.csdn.net/article/2015-03-03/2824081">http://www.csdn.net/article/2015-03-03/2824081</a></p>
  </li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/10/27/spark-introduction/"/>
    <updated>2015-10-27T21:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/10/27/spark-introduction</id>
    <content type="html"><![CDATA[<p>简单介绍下Spark。Spark是分布式的<code>内存计算模型</code>，对于两类计算形式能够极大地提升处理的效率：<code>迭代计算</code>，和<code>交互式的数据挖掘工具</code>。迭代计算例如PageRank、K-means聚类、逻辑回归等要求计算结果的重新利用，交互式的数据挖掘例如针对一份的数据集进行多次特定查询也要求原始数据的重复利用。对比于MR作业，<strong>它不需要外部存储的介入，从而提升处理速度</strong>。接下来将会从Spark的数据模型、编程模型和架构来介绍Spark。</p>

<p><img src="https://spark.apache.org/images/spark-logo.png" width="200px" /></p>

<!--more-->

<h3 id="rdd-data-model">RDD Data Model</h3>
<p>那么如何利用多个节点的内存进行分布式的计算呢，这就是Spark的核心RDD（Resilient Distributed Dataset）。<u>RDD是一个个记录的集合，只读和分片的。它只能通过外部存储或者其他RDD生成</u>。RDD的这些变化操作名称为<em>transformation</em>，在Spark编程中还有另外一个操作需要知道，<em>action</em>，意思是返回一个值或者将数据导出到外部存储，像<code>count</code>、<code>collect</code>、<code>save</code>。一个spark程序往往是从外部存储中定义一个或多个RDD开始的，接着经过各种<em>transformation</em>，最后<em>action</em>。同时<em>action</em>也是计算的开始，在spark中计算是延迟的，各种<em>transformation</em>形成了computation pipeline在<em>action</em>中进行计算。</p>

<p>下表是对RDD中的<em>transformation</em>和<em>action</em>操作：
<img src="http://ww2.sinaimg.cn/large/74311666jw1ex310kinr3j210c0fujwg.jpg" alt="" /></p>

<p>RDD还有两个操作：<em>persistence</em>和<em>partitioning</em>。其实顾名思义，<em>persistence</em>即RDD的持久化操作，这是为了避免重复计算。<em>partitioning</em>是重新分区，为了得到更好的执行并发度。</p>

<p><strong>合理的分区可以有效减少shuffle的数据量</strong>，根据特定的应用场景执行不同的<em>partitioning</em>。例如在PageRank中根据域名来对URL进行Hash，因为很多链接都是内部的。下图表示<em>partitioning</em>的效果。</p>

<p><img src="http://img-storage.qiniudn.com/15-10-16/6543188.jpg" alt="" /></p>

<p><strong>持久化操作是也是为了更高的计算效率</strong>。例如在下图中，Spark有两个action，所以产生两个job。两个job各自计算RDD1和RDD2，对于数据量大的RDD无疑会影响性能，所以可以将RDD进行<em>persistence</em>操作，可以存入内存、硬盘或者二者结合。后续缓存的资源可以手动清除或者通过LRU算法自动清除。
<img src="http://ww2.sinaimg.cn/large/74311666jw1ex30g3zpazj20u60f2tb9.jpg" alt="" /></p>

<p>在上文中我们提到RDD是有<code>transformation</code>和<code>lazy compute</code>的特性的。这两个特性使得RDD不需要一直被实例化，只需要保存这个RDD是如何产生的，在延迟计算的时候便可以通过这些dependence信息进行RDD transformation操作。以上就是RDD lineage，一个RDD的血统关系图。RDD的只读特性也是为了更好地描述这个lineage graph（由于RDD是只读的，Spark的RDD计算中一致性不是主要关心的内容，内存相对容易管理）。在上图中两个Output的产生也可以直观地看到RDD的lineage信息。那么这个linage究竟有什么好处呢，<strong>RDD可以根据dependency信息直接追踪到在外存中的数据，在发生错误的时候直接通过外存的原始数据计算丢失或错误的RDD分片信息</strong>。</p>

<p>这里再简单插入下分布式数据集的容错性方式：<strong>数据检查点和记录数据的更新</strong>。这个在HDFS上都可以看到。RDD的lineage就是记录更新的方式，只不过这个粒度比较大，恢复方式又和接下来要说的窄依赖和宽依赖有关。但是对于稍纵即逝的数据，又会采用什么方式，可以参考<a href="http://billowkiller.com/blog/2015/10/27/spark-streaming">spark streaming</a>。</p>

<h3 id="rdd-programming-model">RDD Programming Model</h3>

<p>RDD的编程模型需要通过适当的接口来表示RDD是如何经过一系列的transformations来达到现在的状态。在Spark中，RDD的编程模型暴露了5方面的信息：</p>

<ul>
  <li>dateset中的分片信息</li>
  <li>父RDD的依赖关系</li>
  <li>基于其父RDD的计算RDD方法</li>
  <li>分片的shceme元数据</li>
  <li>数据存放的位置
<img src="http://ww3.sinaimg.cn/large/74311666jw1ex32afwa8gj20qq0ds41v.jpg" alt="" /></li>
</ul>

<p>在第3个方法中，也就是根据父RDD分片计算本RDD的分片有两种情况：每个父RDD分片最多被一个子RDD分片依赖；父RDD分片被多个子RDD分片依赖。这两种情况分别对应<em>narrow dependency</em>和<em>wide dependency</em>。为什么区分这两种依赖关系呢，这和RDD的延迟计算特性有关系。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-1/72855711.jpg" width="550px" /></p>

<ul>
  <li>在<em>narrow dependency</em>中，一个节点上的RDD分片可以通过pipeline的方式从原始数据开始计算，多个分片可以并行的进行。而对于<em>wide dependency</em>需要所有父RDD的分片可用，而且涉及到data shuffle。</li>
  <li><em>narrow dependency</em>在节点失效后的恢复的效率更高，因为可以并行地在不同的节点上计算丢失的分片。而在<em>wide dependency</em>中，一个父RDD分片可能被多个子RDD分片使用，因为父分片数据只有一部分是需要重算子分片的，其余数据重算就造成了冗余计算；另外，对于<em>wide dependency</em>, Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，通过重新计算恢复数据这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试，所以更加消耗资源。</li>
</ul>

<p>总结下：</p>

<ol>
  <li>
    <p>窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。</p>
  </li>
  <li>
    <p>数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。</p>
  </li>
</ol>

<u>通过上述分析可以看出在以下两种情况下，RDD需要加检查点</u>
<p>：</p>

<ul>
  <li>DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</li>
  <li>在宽依赖上做Checkpoint获得的收益更大。</li>
</ul>

<p>不仅是<code>容错</code>，整个spark作业的<code>作业调度、计算方式</code>也是和这两种dependency有关。当一个用户执行一个<em>action</em>操作的时候，spark的调度器检测RDD的lineage graph，建立一个DAG图来执行，DAG图中的每个节点就是一个<em>stage</em>。在每个<em>stage</em>中包含了多个pipeline的<em>transformation</em>操作，这些RDD的关系全都是
<em>narrow dependency</em>。每个<em>stage</em>的边界都是由<em>wide dependency</em>的shuffle操作，或者已经计算好的分片。调度器这时候就可以建立执行任务计算每个stage中缺失的<em>partitions</em>，直到得到最终的RDD。</p>

<p>stage中每个数据分片的窄依赖计算都是一个task，也就是最后RDD有几个分区就是几个task。那么如何合理划分 stage，并确定 task 的类型和个数？
<img src="http://img-storage.qiniudn.com/15-10-16/48846624.jpg" alt="" /></p>

<p>可以看到在上图中，每个stage中的数据都是形成了pipeline计算的，这里的pipeline思想是：<strong>数据用的时候再算，而且数据是流到要计算的位置的</strong>。有两层意思，一是<code>延迟计算</code>，二是<code>计算本地化</code>。比如在第一个 task 中，从 FlatMappedValuesRDD 中的 partition 向前推算，只计算要用的（依赖的） RDDs 及 partitions。在第二个 task 中，从 CoGroupedRDD 到 FlatMappedValuesRDD 计算过程中，不需要存储中间结果（MappedValuesRDD 中 partition 的全部数据）。</p>

<p>在有<em>wide dependency</em>的时候需要Shuffle后无法进行pipeline。那么我们可以<strong>从后往前推算，遇到 <em>wide dependency</em>就断开，遇到<em>narrow dependency</em>就将其加入该stage。每个stage里面task 的数目由该stage最后一个RDD中的partition个数决定</strong>。因此上图中最后一个stage的id是0，stage 1  stage 2都是stage 0的parents。</p>

<u>整个的computing chain也是根据数据依赖关系自后向前建立，遇到*wide dependency*后形成 stage</u>
<p>。computing chain从后到前建立，而实际计算出的数据从前到后流动，那么RDD内部是如何实现计算的呢？在每个stage中，每个RDD中的compute()调用parentRDD.iter()来将parent RDDs中的 records一个个fetch过来。</p>

<blockquote>
  <p>代码实现：每个 RDD 包含的 getDependency() 负责确立 RDD 的数据依赖，compute() 方法负责接收 parent RDDs 或者 data block 流入的 records，进行计算，然后输出 record。经常可以在 RDD 中看到这样的代码firstParent[T].iterator(split, context).map(f)。firstParent 表示该 RDD 依赖的第一个 parent RDD，iterator() 表示 parentRDD 中的 records 是一个一个流入该 RDD 的，map(f) 表示每流入一个 recod 就对其进行 f(record) 操作，输出 record。为了统一接口，这段 compute() 仍然返回一个 iterator，来迭代 map(f) 输出的 records。</p>
</blockquote>

<p>在实现中，每个task是基于数据的本地性进行分配的，任务是在该分片的节点上执行的。而对于<em>wide dependecy</em>，会在拥有父分片的节点上计算中间结果，这是为了减少fault recovery的时间。</p>

<h3 id="spark-architecture">Spark Architecture</h3>
<p>在上一节中我们确定了Spark stage和task的生成和执行方式，那么这些stage和task在整个spark application中又处于什么样的位置呢，上文中我们提到的每个job又是什么意思呢，为什么一个application中又会有好多个job，它们之间的关系又是什么样的呢？</p>

<p><img src="http://spark-internals.books.yourtion.com/markdown/PNGfigures/deploy.png" alt="image" /></p>

<p>从部署图中可以看到</p>

<ul>
  <li>整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。</li>
  <li>Master 节点上常驻 Master 守护进程，负责管理全部的 Worker 节点。</li>
  <li>Worker 节点上常驻 Worker 守护进程，负责与 Master 节点通信并管理 executors。</li>
  <li>Driver 官方解释是 “The process running the main() function of the application and creating the SparkContext”。Application 就是用户自己写的 Spark 程序（driver program），比如 WordCount.scala。</li>
  <li>每个 Worker 上存在一个或者多个 ExecutorBackend 进程。每个进程包含一个 Executor 对象，该对象持有一个线程池，每个线程可以执行一个 task。</li>
  <li>每个 application 包含一个 driver 和多个 executors，每个 executor 里面运行的 tasks 都属于同一个 application。</li>
</ul>

<p>在最开始的时候我们介绍了RDD的<code>action</code>操作，还提到每个<code>action</code>就是一个job，事实上上表中的<code>action</code>操作其实是论文中的，实际上还有更多的<code>action</code>。如下表：</p>

<table>
<thead>
<tr>
<th style="text-align:left">Action</th>
<th style="text-align:left">finalRDD(records) =&gt; result</th>
<th style="text-align:left">compute(results)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">reduce(func)</td>
<td style="text-align:left">(record1, record2) =&gt; result, (result, record i) =&gt; result</td>
<td style="text-align:left">(result1, result 2) =&gt; result, (result, result i) =&gt; result</td>
</tr>
<tr>
<td style="text-align:left">collect()</td>
<td style="text-align:left">Array[records] =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">count()</td>
<td style="text-align:left">count(records) =&gt; result</td>
<td style="text-align:left">sum(result)</td>
</tr>
<tr>
<td style="text-align:left">foreach(f)</td>
<td style="text-align:left">f(records) =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">take(n)</td>
<td style="text-align:left">record (i&lt;=n) =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">first()</td>
<td style="text-align:left">record 1 =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">takeSample()</td>
<td style="text-align:left">selected records =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">takeOrdered(n, [ordering])</td>
<td style="text-align:left">TopN(records) =&gt; result</td>
<td style="text-align:left">TopN(results)</td>
</tr>
<tr>
<td style="text-align:left">saveAsHadoopFile(path)</td>
<td style="text-align:left">records =&gt; write(records)</td>
<td style="text-align:left">null</td>
</tr>
<tr>
<td style="text-align:left">countByKey()</td>
<td style="text-align:left">(K, V) =&gt; Map(K, count(K))</td>
<td style="text-align:left">(Map, Map) =&gt; Map(K, count(K))</td>
</tr>
</tbody>
</table>

<p>用户的 driver 程序中一旦出现 action()，就会生成一个 job，比如 foreach() 会调用sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(f))，向 DAGScheduler 提交 job。如果 driver 程序后面还有 action()，那么其他 action() 也会生成 job 提交。所以，driver 有多少个 action()，就会生成多少个 job。这就是 Spark 称 driver 程序为 application（可能包含多个 job）而不是 job 的原因。</p>

<p>每一个 job 包含 n 个 stage，最后一个 stage 产生 result。在提交 job 过程中，DAGScheduler 会首先划分 stage，然后先提交无 parent stage 的 stages，并在提交过程中确定该 stage 的 task 个数及类型，并提交具体的 task。无 parent stage 的 stage 提交完后，依赖该 stage 的 stage 才能够提交。从 stage 和 task 的执行角度来讲，一个 stage 的 parent stages 执行完后，该 stage 才能执行。</p>

<p>Spark就先介绍到这里，没有涉及到spark的具体内部实现，包括job的调度，shuffle的过程、文件的管理、cache和broadcast机制等。可以参考(http://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html)[http://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html].</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Compression Format Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/09/18/compression-format-introduction/"/>
    <updated>2015-09-18T13:34:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/09/18/compression-format-introduction</id>
    <content type="html"><![CDATA[<h1 id="section">压缩格式</h1>
<p>最近在做一个日志的压缩策略，在此就记录下几种不同的压缩算法。本文不涉及每种算法的具体实现，只是用于介绍概念。</p>

<!--more-->

<h2 id="gzip">Gzip</h2>
<p>说到gzip，其实我是不想写的，满满的都是痛苦的回忆啊。在研究生时期曾经写过一个程序，在网关出对用户访问的网页进行hack，在网页的最后加上一个网页跳转的代码。由于网页传输的压缩编码就是gzip，具体做法是获取gzip的头文件后，构造压缩后的数据添加到原始的网页数据上。这里面很有意思的是，gzip是bit-oriented的，所以需要定位去除EOF marker后的数据最后的bit位置，再插入构造好的压缩数据。这里面涉及到的东西比较多，包括对gzip头文件的分析，找到每个单词对应的huffman编码；对压缩数据的反解找到对应的bit位；还有HTTP传输编码替换，所有的编码都改为CHUNKED编码方式。言归正传，那么什么是Gzip压缩编码？</p>

<p>Gzip是基于DEFLATE压缩算法的，它是由LZ77和Huffman编码的组成。LZ77其实很好理解，就是将重复的字符串用数字表示，标记为(length, distance)，相当于建立一个索引，后续出现的数据都可以通过这个索引找到原始的数据。举个简单的例子A A A A A A B A B A A A A A 经过LZ77编码后转化为A A A A A B &lt;2,2&gt; &lt;5,8&gt;, 可以看到A B和A A A A A是重复的，用长度和距离的组合来表示。Huffman编码是一种可变字长编码，依据字符出现概率来构造字符的二进制表示，用于保证了按字符出现概率分配码长，使平均码长最短。</p>

<p>通常我们说的gzip是指gzip文件格式，它的构成如下：</p>

<ul>
  <li>10字节的头，包括magic number，版本号和时间戳</li>
  <li>可选的附加字段，例如源文件名</li>
  <li>正文，也就是经过DEFLATED压缩后的数据</li>
  <li>8字节的EOF marker，包括CRC-32校验和和原始未压缩数据的长度。</li>
</ul>

<p>详细的文件格式如下：</p>

<p><img src="http://img-storage.qiniudn.com/15-9-18/30592957.jpg" alt="" /></p>

<h2 id="lzo">lzo</h2>

<p>lzo令人郁闷的一点是压缩算法是定义好的，但是传输和存储的格式并没有定义好。这就导致不同的厂商在lzo文件压缩格式上有不同的解决方案，在查找lzo资料的时候我就曾见过三种不同的传输或存储格式。那么为什么会有这种情况发生呢，原因就在于lzo lib包中只是定义了压缩的算法，且算法有好几十种。lib包关心的是如何将一串文本转成压缩好的字符串数据，而不关心客户端和服务端是否沟通协调好具体的算法，在传输或存储时候是否会有数据损坏的情况，所以在对lzo压缩进行产品化的时候我们需要考虑这些情况。</p>

<p>在hadoop中，lzo所采用的和<em>lzop</em>相同的压缩格式，lzop是神马呢，官网简介如下：
&gt;lzop is a file compressor which is very similar to gzip. lzop uses the LZO data compression library for compression services, and its main advantages over gzip are much higher compression and decompression speed (at the cost of some compression ratio).</p>

<p>lzop定的的头文件包含的信息如下：</p>

<pre><code>typedef struct {
    unsigned char magic[9]
    uint16_t version;
    uint16_t lib_version;
    uint16_t version_needed_to_extract;
    unsigned char method;
    unsigned char level;                // ignore
    uint32_t flags;                     
    uint32_t filter;                    // filter is not supported
    uint32_t mode;                      // ignore
    uint32_t mtime_low;                 // ignore
    uint32_t mtime_high;                // ignore
    unsigned char filename_len;
    uint32_t checksum;
} header_t;
</code></pre>

<p>在lzop的头文件格式中中还有定义一些extra fields，filename，这些在hadoop的解压算法中就直接给忽略了，所以这里也就没有添加上去，<code>header_t</code>中都是一些必要的占位字段，有些无意义，有些有意义。其中在<code>flags</code>中定义了一些是压缩格式实现的细节，例如压缩前数据的校验和算法，压缩后数据的校验和算法，是否有filter，是否是多文件合并等等。<code>method</code>字段定义了具体的压缩算法，使用的比较多的是M_LZO1X<em>1、M_LZO1X</em>1<em>15、M_LZO1X</em>999. 在官方的lib包中又对各种压缩算法的一个比较，通常情况下使用M_LZO1X_1就足够了。</p>

<p>lzo的压缩是面向块的，所以正文部分也就是由一块一块的压缩块构成，每块的压缩格式如下：</p>

<pre><code>/*
 * uncompressed block size
 * compressed block size
 * uncompressed block checksum
 * compressed block checksum
 * compressed block
 */
</code></pre>

<p>最后写一个EOF marker，也就是4bytes的0.</p>

<h2 id="snappy">snappy</h2>

<p>Google开发的一个 C++ 的用来压缩和解压缩的开发包，旨在提供高速压缩速度和合理的压缩率，Snappy 比 zlib 更快，但文件相对要大 20% 到 100%。Snappy特地为64位x86处理器做了优化，在单个Intel Core i7处理器内核上能够达到至少每秒250MB的压缩速率和每秒500MB的解压速率。Snappy的压缩是属于LZ77 体系，不同于gzip，它是byte-oriented，也就是说数据块之间是有严格的字节区分的；而gzip由于最后加入了Huffman编码，所以它是bit-oriented。byte-oriented的好处是处理起来比较简单，不需要记录字节中的位占用情况和加入数据的位迁移。</p>

<p>Snappy的压缩方式有一个frame的概念，这个帧并不是Snappy压缩所必须的，这个帧其实也就是Snappy的压缩文本加上一些必要的其他信息，例如checksum。Snappy中帧与帧之间是相互独立的，也就是说在压缩过程中你可以不关心其他帧的情况，只需要压缩好自己的数据然后发送或存储，解压的过程中会根据帧的粒度进行解压。这就意味着在分布式的环境中，你可以很容易的合并各个节点上的压缩数据，无需和其他节点进行同步，这个在其他算法中是无法办到的。</p>

<p>Snappy文件只由一个个连续的chunk构成，每个chunk的构成包括：1字节的chunk标识符，3字节的chunk长度，接着就是数据。chunk长度可以为0，这就表示该chunk的数据不存在。Snappy文件的第一个chunk是stream chunk，固定的6字节长度，其中数据部分为”sNaPpY”。这个chunk可以理解为Snappy的头文件，但是这个头文件可以出现多次，这也就是每个Snappy frame可以独立的原因之一；另外一个原因是Snappy并没有EOF marker。其他chunk的类型还包括：</p>

<ul>
  <li>Compressed data</li>
  <li>Uncompressed data</li>
  <li>padding</li>
  <li>Reserved unskippable chunks</li>
  <li>Reserved skippable chunks</li>
</ul>

<h2 id="section-1">对比</h2>
<p>以下是在论文中找到的一些资料。</p>

<p><img src="http://img-storage.qiniudn.com/15-9-17/82781797.jpg" alt="" /></p>

<p><img src="http://ww1.sinaimg.cn/large/74311666jw1ew5aeta9loj20w10hfdi2.jpg" alt="" /></p>

<h2 id="section-2">其他</h2>
<p><a href="https://github.com/billowkiller/Codec">以上三种压缩格式的c++方法</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCMC Methods]]></title>
    <link href="http://billowkiller.github.io/blog/2015/09/01/mcmc/"/>
    <updated>2015-09-01T16:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/09/01/mcmc</id>
    <content type="html"><![CDATA[<p>解释下题目中的MCMC，全名是Markov Chain Monte Carlo，这里面有两个词，Markov Chain、Monte Carlo。第一个词是马尔可夫链，可以参考<a href="http://billowkiller.com/blog/2016/05/09/random-process/">http://billowkiller.com/blog/2016/05/09/random-process/</a>。第二个是蒙特卡罗，是一种模拟方法，可以从一个分布中模拟出点来估计我们感兴趣的参数，例如用来估计密度函数的定积分，称为 Monte Carlo Integration。</p>

<!--more-->

<h2 id="monte-carlo-integration">Monte Carlo Integration</h2>

<p>譬如现在有个分布 $p(\theta)$，我们想知道一下积分：</p>

<script type="math/tex; mode=display"> I = \int_{\Theta} g(\theta)p(\theta) d\theta </script>

<p>通过从 $p(\theta)$ 中模拟出 $M$ 个值，我们可以估计 $I$，公式如下：</p>

<script type="math/tex; mode=display"> \hat{I}_M = \frac{1}{M} \sum_{i=1}^M g(\theta^{(i)}) </script>

<p>于是统计 $Beta(3,3)$ 的期望可以通过一下代码得到：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">M <span class="o">&lt;-</span> <span class="m">10000</span>
</span><span class="line">beta.sims <span class="o">&lt;-</span> rbeta<span class="p">(</span>M<span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">3</span><span class="p">)</span>
</span><span class="line">sum<span class="p">(</span>beta.sims<span class="p">)</span><span class="o">/</span>M
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>还有一个经典的例子是计算 $\pi$:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">M <span class="o">&lt;-</span> <span class="m">1000000</span>
</span><span class="line">x <span class="o">&lt;-</span> runif<span class="p">(</span>N<span class="p">,</span> min<span class="o">=</span> <span class="m">-1</span><span class="p">,</span> max<span class="o">=</span> <span class="m">1</span><span class="p">)</span>
</span><span class="line">y <span class="o">&lt;-</span> runif<span class="p">(</span>N<span class="p">,</span> min<span class="o">=</span> <span class="m">-1</span><span class="p">,</span> max<span class="o">=</span> <span class="m">1</span><span class="p">)</span>
</span><span class="line">is.inside <span class="o">&lt;-</span> <span class="p">(</span>x<span class="o">^</span><span class="m">2</span> <span class="o">+</span> y<span class="o">^</span><span class="m">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="m">1</span>
</span><span class="line">pi.estimate <span class="o">&lt;-</span> <span class="m">4</span> <span class="o">*</span> sum<span class="p">(</span>is.inside<span class="p">)</span> <span class="o">/</span> M
</span><span class="line">pi.estimate
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>通过大数定律我们其实是可以知道，当 $M \to \infty$，$\hat{I}_M \to I$。</p>

<p>大数定律要求的是独立同分布的随机变量，上述的例子都是从同一分布中得到独立的变量，但是如果我们不能产生独立的变量又该怎么办呢？</p>

<p>例如，我们想要从后验分布 $p(\theta \vert y)$ 中抽样，但是我们不能产生独立的变量，因为我们通常不知道 normalizing constant。我们却可以产生稍微有点依赖的变量，这时可以应用 Markov chain 得到我们感兴趣的值。</p>

<p>一旦马尔可夫链收敛到了平稳分布，这时候对马尔可夫链的抽样就类似于对 $p(\theta \vert y)$ 的抽样。但是还是没有解决抽样不是独立。</p>

<p>这时候我们祭出大神 <code>Ergodic Theorem</code>，可以让马尔可夫链模拟大数定律，定义如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/86848502.jpg" width="600px" /></p>

<p>下面分别解释下 aperiodic, irreducible, positive recurrent。</p>

<p><strong>aperiodic</strong></p>

<p>马尔可夫链的周期性可以用下图表示</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/33403945.jpg" width="400px" /></p>

<p>只要整个链不是重复一个完整的圆圈，则为 aperiodic。</p>

<p><strong>irreducibility</strong></p>

<p>马尔可夫链的不可约表示为可以从任意一个状态到另外一个状态。</p>

<p>以下的链表示的是可约的：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/75995726.jpg" width="400px" /></p>

<p><strong>positive recurrent</strong></p>

<p>positive recurrent表示经过有限次数步骤，可以从任意给定的状态最终返回这个状态。</p>

<p>如果马尔可夫链满足以上这三个条件，则可以忽略抽样之间的依赖关系，进行 Monte Carlo Integration。</p>

<p>知道了上述的知识，我们可以得到 MCMC 的定义：</p>

<blockquote>
  <p>MCMC is a class of methods in which we can <strong>simulate draws</strong> that are <strong>slightly dependent</strong> and are approximately from a (posterior) distribution.</p>
</blockquote>

<p>在贝叶斯统计中，我们通常会使用两种 MCMC 算法：the Gibbs Sampler 和 Metropolis-Hastings algorithm。</p>

<h2 id="gibbs-sampling">Gibbs Sampling</h2>

<p>假设我们想从联合概率分布 $p(\theta_1,…\theta_k)$ 中得到抽样，那么我们需要知道的是<strong>每个参数的条件分布</strong> $p(\theta_j \vert \theta_{-j},y)$。</p>

<p>从条件概率是如何得到联合概率的呢？具体是根据 <code>The Hammersley-Clifford Theorem</code>。有个例子如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/41685068.jpg" width="600px" /></p>

<p>于是右边就可以得到 $f(y \vert x) f(x) = f(x,y)$。</p>

<p>接下来我们要确认的是如何得到每个参数的条件概率分布，full conditional probability.</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/33580254.jpg" width="600px" /></p>

<p>得到了 full conditional probability 就可以进行 Gibbs Sampling.</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/5179470.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/18003635.jpg" width="600px" /></p>

<p>通过对后验概率的近似抽样我们其实是在模拟马尔可夫链。所以也就可以通过这些抽样得到估计值。</p>

<u>Example:</u>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/34647596.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/6225347.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/68667610.jpg" width="600px" /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">gibbs <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>n.sims<span class="p">,</span> beta.start<span class="p">,</span> alpha<span class="p">,</span> gamma<span class="p">,</span> delta<span class="p">,</span>
</span><span class="line">        y<span class="p">,</span> t<span class="p">,</span> burnin<span class="o">=</span><span class="m">0</span><span class="p">,</span> thin<span class="o">=</span><span class="m">1</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        beta.draws <span class="o">&lt;-</span> c<span class="p">()</span>
</span><span class="line">        lambda.draws <span class="o">&lt;-</span> matrix<span class="p">(</span><span class="kc">NA</span><span class="p">,</span> nrow <span class="o">=</span> n.sims<span class="p">,</span> ncol <span class="o">=</span> length<span class="p">(</span>y<span class="p">))</span>
</span><span class="line">        beta.cur <span class="o">&lt;-</span> beta.start
</span><span class="line">        lambda.update <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>alpha<span class="p">,</span> beta<span class="p">,</span> y<span class="p">,</span> t<span class="p">)</span> <span class="p">{</span>
</span><span class="line">           rgamma<span class="p">(</span>length<span class="p">(</span>y<span class="p">),</span> y <span class="o">+</span> alpha<span class="p">,</span> t <span class="o">+</span> beta<span class="p">)</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">        beta.update <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>alpha<span class="p">,</span> gamma<span class="p">,</span> delta<span class="p">,</span> lambda<span class="p">,</span> y<span class="p">)</span> <span class="p">{</span>
</span><span class="line">           rgamma<span class="p">(</span><span class="m">1</span><span class="p">,</span> length<span class="p">(</span>y<span class="p">)</span> <span class="o">*</span> alpha <span class="o">+</span> gamma<span class="p">,</span> delta <span class="o">+</span> sum<span class="p">(</span>lambda<span class="p">))</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">        <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>n.sims<span class="p">)</span> <span class="p">{</span>
</span><span class="line">            lambda.cur <span class="o">&lt;-</span> lambda.update<span class="p">(</span>alpha <span class="o">=</span> alpha<span class="p">,</span> beta <span class="o">=</span> beta.cur<span class="p">,</span>
</span><span class="line">                y <span class="o">=</span> y<span class="p">,</span> t <span class="o">=</span> t<span class="p">)</span>
</span><span class="line">            beta.cur <span class="o">&lt;-</span> beta.update<span class="p">(</span>alpha <span class="o">=</span> alpha<span class="p">,</span> gamma <span class="o">=</span> gamma<span class="p">,</span>
</span><span class="line">                delta <span class="o">=</span> delta<span class="p">,</span> lambda <span class="o">=</span> lambda.cur<span class="p">,</span> y <span class="o">=</span> y<span class="p">)</span>
</span><span class="line">            <span class="kr">if</span> <span class="p">(</span>i <span class="o">&gt;</span> burnin <span class="o">&amp;</span> <span class="p">(</span>i <span class="o">-</span> burnin<span class="p">)</span><span class="o">%%</span>thin <span class="o">==</span> <span class="m">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                lambda.draws<span class="p">[(</span>i <span class="o">-</span> burnin<span class="p">)</span><span class="o">/</span>thin<span class="p">,</span> <span class="p">]</span> <span class="o">&lt;-</span> lambda.cur
</span><span class="line">                beta.draws<span class="p">[(</span>i <span class="o">-</span> burnin<span class="p">)</span><span class="o">/</span>thin<span class="p">]</span> <span class="o">&lt;-</span> beta.cur
</span><span class="line">            <span class="p">}</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">        <span class="kr">return</span><span class="p">(</span>list<span class="p">(</span>lambda.draws <span class="o">=</span> lambda.draws<span class="p">,</span> beta.draws <span class="o">=</span> beta.draws<span class="p">))</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>下面实验得到上述例子的参数估计</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">y <span class="o">&lt;-</span> c<span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">14</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">19</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">22</span><span class="p">)</span>
</span><span class="line">t <span class="o">&lt;-</span> c<span class="p">(</span><span class="m">94</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">63</span><span class="p">,</span> <span class="m">126</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">31</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
</span><span class="line">posterior <span class="o">&lt;-</span> gibbs<span class="p">(</span>n.sims <span class="o">=</span> <span class="m">10000</span><span class="p">,</span> beta.start <span class="o">=</span> <span class="m">1</span><span class="p">,</span> alpha <span class="o">=</span> <span class="m">1.8</span><span class="p">,</span> gamma <span class="o">=</span> <span class="m">0.01</span><span class="p">,</span> delta <span class="o">=</span> <span class="m">1</span><span class="p">,</span> y <span class="o">=</span> y<span class="p">,</span> t <span class="o">=</span> t<span class="p">)</span>
</span><span class="line"><span class="c1"># 大数定律</span>
</span><span class="line">colMeans<span class="p">(</span>posterior<span class="o">$</span>lambda.draws<span class="p">)</span>
</span><span class="line">mean<span class="p">(</span>posterior<span class="o">$</span>beta.draws<span class="p">)</span>
</span><span class="line">apply<span class="p">(</span>posterior<span class="o">$</span>lambda.draws<span class="p">,</span> <span class="m">2</span><span class="p">,</span> sd<span class="p">)</span>
</span><span class="line">sd<span class="p">(</span>posterior<span class="o">$</span>beta.draws<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h2>

<p>Metropolis-Hastings 算法适用于这种情况：</p>

<ul>
  <li>后验概率并不像任何我们知晓的分布（没有共轭分布）</li>
  <li>参数的条件概率并不像任何我们知晓的分布（没法用Gibbs sampling）</li>
  <li>后验概率拥有三个以上的参数（grid approximations 不可解）</li>
</ul>

<p>具体的算法有以下步骤：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/92275724.jpg" width="600px" /> </p>

<p>细分下每个步骤：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/86212406.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/28170626.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/32187111.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/28671748.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/28650748.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/73904716.jpg" width="600px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-1/8969349.jpg" width="600px" /></p>

<u>Example:</u>

<p>使用随机游走Metropolis算法从Gamma(1.7, 4.4)分布抽样，jumping distribution是一个标准差为2的正太分布。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">mh.gamma <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>n.sims<span class="p">,</span> start<span class="p">,</span> burnin<span class="p">,</span> cand.sd<span class="p">,</span> shape<span class="p">,</span> rate<span class="p">)</span> <span class="p">{</span>
</span><span class="line">    theta.cur <span class="o">&lt;-</span> start
</span><span class="line">    draws <span class="o">&lt;-</span> c<span class="p">()</span>
</span><span class="line">    theta.update <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>theta.cur<span class="p">,</span> shape<span class="p">,</span> rate<span class="p">)</span> <span class="p">{</span>
</span><span class="line">        theta.can <span class="o">&lt;-</span> rnorm<span class="p">(</span><span class="m">1</span><span class="p">,</span> mean <span class="o">=</span> theta.cur<span class="p">,</span> sd <span class="o">=</span> cand.sd<span class="p">)</span>
</span><span class="line">        accept.prob <span class="o">&lt;-</span> dgamma<span class="p">(</span>theta.can<span class="p">,</span> shape <span class="o">=</span> shape<span class="p">,</span> rate <span class="o">=</span> rate<span class="p">)</span> <span class="o">/</span>
</span><span class="line">            dgamma<span class="p">(</span>theta.cur<span class="p">,</span> shape <span class="o">=</span> shape<span class="p">,</span> rate <span class="o">=</span> rate<span class="p">)</span>
</span><span class="line">        <span class="kr">if</span> <span class="p">(</span>runif<span class="p">(</span><span class="m">1</span><span class="p">)</span> <span class="o">&lt;=</span> accept.prob<span class="p">)</span> theta.can <span class="kr">else</span> theta.cur
</span><span class="line">    <span class="p">}</span>
</span><span class="line">    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>n.sims<span class="p">)</span> <span class="p">{</span>
</span><span class="line">        draws<span class="p">[</span>i<span class="p">]</span> <span class="o">&lt;-</span> theta.cur <span class="o">&lt;-</span> theta.update<span class="p">(</span>theta.cur<span class="p">,</span> shape <span class="o">=</span> shape<span class="p">,</span> rate <span class="o">=</span> rate<span class="p">)</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line">    <span class="kr">return</span><span class="p">(</span>draws<span class="p">[(</span>burnin <span class="o">+</span> <span class="m">1</span><span class="p">)</span><span class="o">:</span>n.sims<span class="p">])</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">mh.draws <span class="o">&lt;-</span> mh.gamma<span class="p">(</span><span class="m">10000</span><span class="p">,</span> start <span class="o">=</span> <span class="m">1</span><span class="p">,</span> burnin <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> cand.sd <span class="o">=</span> <span class="m">2</span><span class="p">,</span> shape <span class="o">=</span> <span class="m">1.7</span><span class="p">,</span> rate <span class="o">=</span> <span class="m">4.4</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Design Patterns for MapReduce Algorithms]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/mr-design-pattern/"/>
    <updated>2015-07-26T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/mr-design-pattern</id>
    <content type="html"><![CDATA[<p>参考《MapReduce设计模式》，做个总结。</p>

<h2 id="section">概要模式</h2>

<h3 id="section-1">数值概要</h3>

<p><strong>问题描述：</strong>求一堆数据的一个高层次展示，平均值，中位数，最大，最小，标准差这类的特征值。</p>

<p>假设θ是我们想要执行的聚合方法，要计算的值是列表values（v1, v2, v3,…, vn)），要想求出聚合值λ， 令：λ=θ(v1, v2, v3, …, vn) 。</p>

<p><strong>解决方案：</strong></p>

<ul>
  <li>
    <p>使用combiner</p>

    <p>如果函数θ满足结合律和交换律，那么就可以使用combiner来减少中间数据的传输量。</p>

    <p>为了得到最终要的那个统计值，是否一定要把所有的数据都传递给reduce，是不是可以先小范围的做些汇总。比如求最大值。 得出每一part的最大值后，再比较得出最大值。那么任意打乱顺序（交换律），任意将数据组成part（结合律），都不会影响最终结果</p>

    <p>再比如算中位数，那么可能就无法使用combiner优化了。中位数不满足交换律和结合律。比如θ函数表示求中位数，那么显然 （1,1,1,1,1,2,2,3,3）这组数据不同的组合结果不同：</p>

    <pre><code>  θ( θ(1,1,1), θ(1,2,2), θ(1,3,3) ) = 2
  θ( θ(1,1,1), θ(1,1,2), θ(2,3,3) ) = 1
</code></pre>
  </li>
  <li>
    <p>定制partition</p>

    <p>数据倾斜严重的会造成reduce处理的数据量差异很大。建议采样分析或者根据之前的经验重新实现partition。</p>

    <p>该策略也适用于其他任何mapreduce程序，线上也时常能碰到partition分桶严重不均的情况，曾经发现一个任务其余reduce都是在秒级跑完，而其中一个reduce跑了10几个小时，查看该任务的partition分布，发现这个reduce处理的量比其他的多好几个数量级。</p>
  </li>
  <li>
    <p>内存优化版的中位数和标准差程序</p>

    <p>前面提到，求中位数的函数不满足交换律和结合律。 因此无法简单的使用combiner。因此就设计了一种新的存储方式。</p>

    <p>(1,1,1,1,2,2,3,4,5,5,5)这样的数据集合可以存储为：(1→4, 2→2, 3→1, 4→1, 5→3)</p>

    <p>实际情况下，这种优化在数据重复率高的情况下，收益明显。另外，考虑到reduce端需要排序，而combiner内排序无意义，因此重新实现下combiner用hashmap来存储上面的数据结构。（reduce端用SortedMapWritable）</p>
  </li>
</ul>

<h2 id="section-2">过滤模式</h2>

<h3 id="section-3">过滤</h3>

<p><strong>问题描述：</strong>针对每一条记录，基于某个条件作出判断，决定这条记录是应该保留还是放弃。</p>

<p><strong>解决方案：</strong></p>

<ul>
  <li>
    <p>可以不使用reduce</p>

    <p>如果只是为了过滤，那么可以不加reduce。结果会输出多个part-m-文件。</p>

    <p>但是，如果保留下的数据量很小，就会造成大量小文件。如果是永久存储的，为了节省inode以及后续处理这些文件时不至于浪费大量map去处理。那么可以考虑用一定数量的reduce收集结果。</p>
  </li>
  <li>
    <p>布隆过滤</p>

    <p>利用布隆过滤器来替代字典，每条记录查询布隆过滤器来决定去留。</p>

    <p>其优点是省内存。缺点或者说特点是：只能判断no或者maybe，但是不能判断yes。(有一定概率的fase-positive)</p>

    <p>而对于线上实际，可以采用分层的架构，在布隆过滤器下游再加一个精确的过滤来处理。如果需求仅仅是要判断不存在或可以容忍fase-positive，那就可以直接使用。</p>

    <p>布隆过滤器训练的结果如果不是经常变更，那么可以保存下来每次复用。然后将该结果作为分布式cache分发到各个map端。</p>
  </li>
</ul>

<h3 id="top10">Top10</h3>

<p><strong>问题描述：</strong>求一个大数据集合的的top 10的数据。</p>

<p><strong>解决方案：</strong></p>

<ul>
  <li>
    <p>key设置为null，每个map数据拼接到一行。</p>

    <p>mapper遍历数据拿到top 10之后，将这10个数据拼接，组成一个value, 然后设置key为null。这样，key都是null，节省了shuffle排序的开销。 </p>
  </li>
</ul>

<p><strong>性能分析：</strong></p>

<ul>
  <li>由于reduce只能有一个，因此这个单点在数据不断增长的情况下迟早会成为瓶颈。</li>
  <li>对于 Top K的模型，K值很大时，reduce会更快的达到瓶颈。</li>
  <li>可以多跑几轮map的逻辑。比如reduce设置多个，每个也是产出top 10，然后再起一轮mapreduce。</li>
  <li>ChainMapper/ChainReducer</li>
  <li>使用全排序，运行成本要看具体情况</li>
</ul>

<h3 id="section-4">去重</h3>

<p><strong>解决方案：</strong></p>

<pre><code>	map(key,record):
	emit record,null
	reduce(key,records):
	emit key
</code></pre>

<p>这种去重方法是无法保持数据原来的顺序的</p>

<h2 id="section-5">连接模式</h2>

<h3 id="reduce">reduce端连接</h3>
<p>就是在reduce端执行各种连接操作，map会将所有数据传给reduce。该操作耗带宽，io等资源。</p>

<p>但是，这种方式的可伸缩性强，数据集大的情况下，通过增加reduce个数就可以处理。甚至，在所有的数据集都非常大的情况下，这种方式可能是唯一的选择。</p>

<p>在map端使用布隆过滤器来减少传递给reduce的量.</p>

<p><img src="http://img.my.csdn.net/uploads/201301/09/1357709896_1263.jpg" width="500px" /></p>

<h3 id="replicated-join">Replicated Join</h3>

<p>复制join是一种特殊的join，用于一个大数据和许多小数据集map端执行的情况。能够消除reduce阶段的shuffle。</p>

<p>如果其中一个数据集很小，可以全部放入内存，那么只要将这个文件作为分布式cache分发即可。因此，这种情况值适用于内连接和大数据集在左边的左外连接。</p>

<p><img src="http://img.my.csdn.net/uploads/201301/10/1357790552_4334.jpg" width="300px" /></p>

<h3 id="composite-join">Composite Join</h3>

<p>复合join是一种特殊的join操作用来执行map端的很多大数据集的join。该模式对数据有一定的要求。就是需要连接的两份数据已经按照相同的标尺切分好了，并且是排好序的.</p>

<p><img src="http://img.my.csdn.net/uploads/201301/10/1357790574_9671.jpg" width="350px" /></p>

<h3 id="section-6">笛卡尔积</h3>

<p>数据集中的每条记录都与另个数据集的每条记录两两匹配。这是一个非常重型的操作。假设两个数据集都是1万条数据，那么将匹配1亿次。随着数据量的增长，算法复杂度指数级增长。</p>

<p><img src="http://img.my.csdn.net/uploads/201301/10/1357790607_3097.jpg" width="500px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mapreduce framework]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/mapreduce-framework/"/>
    <updated>2015-07-26T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/mapreduce-framework</id>
    <content type="html"><![CDATA[<h2 id="mapreduce">MapReduce框架</h2>

<p>mapReduce 的输入是hdfs上存储的一系列文件集。在hadoop中，这些文件被一种定义了如何分割一个文件成分片的input format来分割，一个分片是一个文件基于字节的可以被一个map任务加载的一个块。</p>

<ul>
  <li>每个map任务被分为以下阶段：<code>record reader</code>，<code>mapper</code>，<code>combiner</code>，<code>partitioner</code>。Map任务的输出叫中间数据，包括keys和values，发送到reduce端。</li>
  <li>Reduce任务分为以下阶段：<code>shuffle</code>，<code>sort</code>，<code>reduce</code>，<code>output format</code>。运行map任务的节点会尽量选择数据所在的节点。这种情况下，不会出现网络传输，在本地节点就可以完成计算。</li>
</ul>

<p>过程如图所示，接下来的章节会一一介绍。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-3/75294350.jpg" alt="" /></p>

<!--more-->

<h3 id="input-format">Input Format</h3>

<p>Record reader会把根据input fromat生成输入分片翻译成records。Record reader的目的是把数据解析成记录，而不是解析数据本身。它把数据以键值对的形式传递给mapper。通常情况下键是偏移量，值是这条记录的整个字节块。从Input file到map的中间过程如下图所示</p>

<p><img src="http://i12.tietuku.com/691b0fd1648b0497.png" width="450px" /></p>

<p>InputFormat其实做了三件事：</p>

<ul>
  <li>校验job的input configuration（比如，查看数据是否存在）。</li>
  <li>split输入的数据文件为逻辑上分片InputSplit，每个InputSplit给接下来的map task处理。</li>
  <li>创建RecordReader从InputSplit中解析出一个个键值对，这个键值对就是Record。</li>
</ul>

<p>如果要自定义InputFormat则最重要的是两个方法：</p>

<ul>
  <li><code>public List&lt;InputSplit&gt; getSplits(JobContext context)</code></li>
  <li><code>public RecordReader createRecordReader(InputSplit split, TaskAttemptContext context)</code></li>
</ul>

<p>通常在处理文本文件的时候，为了保证记录的完整性，RecorderReader会读取超过InputSplit边界的数据。</p>

<p><img src="http://i5.tietuku.com/392e9f9f99737b4d.jpg" alt="" /></p>

<p>在上图中共有三个InputSplit，在hadoop中默认的InputSplit大小为HDFS中每个Block的大小，所以共产生三个map task，它们读取数据的情况如下：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Start</th>
      <th>Actual Start</th>
      <th>End</th>
      <th>Line(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mapper1</td>
      <td>B1:0</td>
      <td>B1:0</td>
      <td>B2:150</td>
      <td>L1, L2, L3</td>
    </tr>
    <tr>
      <td>Mapper2</td>
      <td>B2:128</td>
      <td>B2:150</td>
      <td>B3:300</td>
      <td>L4, L5, L6</td>
    </tr>
    <tr>
      <td>Mapper3</td>
      <td>B3:256</td>
      <td>B3:300</td>
      <td>B3:300</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<p>现有的InputFormat包括：</p>

<ul>
  <li>TextInputFormat，Hadoop中默认的InputFormat，每行都是一个record，以偏移量为key</li>
  <li>KeyValueTextInputFormat， 可以指定key value分割符的TextInputFormat</li>
  <li>NLineInputFormat, mapper接受固定行数的记录</li>
  <li>SequenceFileInputFormat，二进制的KV
    <ul>
      <li>SequenceFileAsTextInputFormat，文本形式的KV</li>
      <li>SequenceFileAsBinaryInputFormat</li>
      <li>FixedLengthInputFormat</li>
    </ul>
  </li>
  <li>MultipleInputs</li>
  <li>DBInputFormat</li>
</ul>

<h3 id="map">Map</h3>

<p>Map阶段，会对每个从RecordReader读取的Record键值对执行用户代码，这些键值对又叫中间键值对。键和值的选择不是任意的，并且对MapReduce job的成功非常重要。键会用来分组，值是reducer端用来分析的数据。。</p>

<h3 id="combiner">Combiner</h3>
<p>Combiner 是一个map阶段分组数据，可选的，局部reducer。它根据用户提供的方法在一个mapper范围内根据中间键去聚合值。例如：数的总和是各个部分数量的和，你可以先计算中间的数目，最后再把所有中间数目加起来。很多情况下，这样能减少数据的网络传输量。发送（hello world，1）三次很显然要比发送（hello world，3）需要更多的网络传输字节量。</p>

<p><img src="http://i12.tietuku.com/009804b71667c5b9.png" width="500px" /></p>

<p>上图所示就是combiner发生的时机，它发生在map side写入磁盘的时候，可能会发生两次，一次是在Spill的时候，一次是在Merge的时候。在这两个阶段之前，是有一个sort的过程，这是为了最大化地提高combiner效率。其实从结构和作用来看，combiner函数和reducer函数的作用是相同的，很多情况下，Combiner也是直接用Reducer。</p>

<h3 id="partitioner">Partitioner</h3>

<p>Partitioner很简单，它决定哪个reducer接收该mapper数据记录，默认的是一个hash函数，对key进行hash之后取reducer个数的模。Partitioner会获取从mapper（或combiner）来的键值对，并分割成分片，每个reducer一个分片。默认用哈希值，典型使用md5sum。然后partitioner根据reduce的个数执行取余运算：key.hashCode() % (number of reducers)。这样能随即均匀的根据key分发数据到reduce，但仍然要保证不同mapper的相同key要到同一个reduce。Partitioner也可以自定义，使用更高级的样式，例如排序。然而，更改partitioner很少用。Partitioner的每个map的数据会写到本地磁盘，并等待对应的reducer检测，拿走数据。</p>

<h3 id="shuffle-and-sort">Shuffle and sort</h3>
<p>虽然说这个阶段主要是在Reduce端，但是Map端的一些行为也会影响到Reduce端。</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>上图是Map端的Shuffle过程，从整个过程来看，最重的部分在于Spill和Merge这两个I/O相关的操作，所有的数据需要从磁盘中读取后又重新写入到磁盘中，所以理想情况下是能够将所有mapper的output都装入到内存中。</p>

<p>Reduce任务开始于shuffle和sort阶段，Reduce将会开启多个fetcher从每个节点上的shuffle service中获取数据流。这一阶段获取partitioner的输出文件，并下载到reduce运行的本地机器。在下载的过程中，map的output首先会写入到内存中并进行排序，在数据达到一定量之后spill到磁盘，会有一个后台程序不断merge这些spilled file。最终，所有的output会合并成一个大的文件。这一阶段不能自定义，由框架自动处理。需要做的只是key的选择和可以自定义个用于分组的比较器。整个过程如下图所示。</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<h3 id="reduce">Reduce</h3>
<p>Reduce 任务会把分组的数据作为输入并对每个key组执行reduce方法代码。方法会传递key和可以相关的所有值得迭代集合。很多的处理会在这个方法里执行，也就会有很多的模式。一旦reduce方法完成，会发送0或多个键值对到output format。跟map一样，不同的reduce依据不同的逻辑情形而不同。</p>

<h3 id="output-format">Output format</h3>
<p>Output Format会把reduce阶段的输出键值对根据record writer写到文件里。默认用tab分割键值对，用换行分割不同行。这里也可以自定义为更丰富的输出格式，最后，数据被写到hdfs。整个过程类似于InputFormat。</p>

<p><img src="http://i12.tietuku.com/cbdb549a3e19f898.png" width="500px" /></p>

<p>LazyOutputFormat 用来保证output (part-r-nnnnn) files有数据，不会存在空文件。</p>

<h3 id="output-commiter">Output Commiter</h3>

<p>Hadoop使用<code>OutputCommitter</code>来保证作业和任务的事务性。在旧的API中需要显示的使用<code>setOutputCommitter</code>或者设置<code>mapred.output.committer.class</code>。
而在新的API中，<code>OutputCommitter</code>是由<code>OutputFormat</code>通过<code>getOutputCommitter()</code>方法决定的。默认的是<code>FileOutputCommitter</code>，它适用于所有的基于文件的MapReduce。</p>

<p><code>OutputCommitter</code>API如下：</p>

<pre><code>public abstract class OutputCommitter {
	public abstract void setupJob(JobContext jobContext) throws IOException; 
	public void commitJob(JobContext jobContext) throws IOException { } 
	public void abortJob(JobContext jobContext, JobStatus.State state) throws IOException { }
	public abstract void setupTask(TaskAttemptContext taskContext) throws IOException;
	public abstract boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException;
	public abstract void commitTask(TaskAttemptContext taskContext) throws IOException;
	public abstract void abortTask(TaskAttemptContext taskContext) throws IOException;
}
</code></pre>

<p><code>setupJob</code>方法在job运行前就被调用，用来服务于作业的初始化，类似创建作业和task的临时目录。</p>

<p>job成功后会调用<code>commitJob()</code>方法，用于删除临时目录和创建<em>_SUCCESS</em>文件。如果job失败，则调用<code>abortJob()</code>方法
表示作业失败或者被kill，默认情况下会删除临时目录。</p>

<p>在task级别的操作类似。Hadoop框架会保证在一个task中的多个task attempt中，只有一个会commit，其他abort。有两种情况：</p>

<ul>
  <li>第一个attempt失败的时候会abort，第二个attempt如果成功则会commit。</li>
  <li>对于预测任务，只要有一个首先成功的话会commit，另外一个则abort。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PID File Analysis]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/pid-file-analysis/"/>
    <updated>2015-07-26T16:57:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/pid-file-analysis</id>
    <content type="html"><![CDATA[<h1 id="pid-">pid 文件浅析</h1>

<p>在Linux系统的目录/var/run下面一般我们都会看到很多的*.pid文件。而且往往新安装的程序在运行后也会在/var/run目录下面产生自己的pid文件。那么这些pid文件有什么作用呢？它的内容又是什么呢？</p>

<!--more-->

<h3 id="pid">pid文件的内容</h3>

<p>pid文件为文本文件，内容只有一行, 记录了该进程的ID。
用cat命令可以看到。</p>

<h3 id="pid-1">pid文件的作用</h3>

<p>防止进程启动多个副本。只有获得pid文件固定路径固定文件名)写入权限(F_WRLCK)的进程才能正常启动并把自身的PID写入该文件中。其它同一个程序的多余进程则自动退出。</p>

<h3 id="section">编程技巧</h3>

<p>调用fcntl设置pid文件的锁定F_SETLK状态，其中锁定的标志位F_WRLCK。
如果成功锁定，则写入进程当前PID，进程继续往下执行。
如果锁定不成功，说明已经有同样的进程在运行了，当前进程结束退出。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>pid usage</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="n">lock</span><span class="p">.</span><span class="n">l_type</span> <span class="o">=</span> <span class="n">F_WRLCK</span><span class="p">;</span>
</span><span class="line"><span class="n">lock</span><span class="p">.</span><span class="n">l_whence</span> <span class="o">=</span> <span class="n">SEEK_SET</span><span class="p">;</span>
</span><span class="line"><span class="k">if</span> <span class="p">(</span><span class="n">fcntl</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">F_SETLK</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">lock</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">){</span>
</span><span class="line">    <span class="c1">//锁定不成功, 退出......</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="n">sprintf</span> <span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="s">&quot;%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">pid</span><span class="p">);</span>
</span><span class="line"><span class="n">pidsize</span> <span class="o">=</span> <span class="n">strlen</span><span class="p">(</span><span class="n">buf</span><span class="p">);</span>
</span><span class="line"><span class="k">if</span> <span class="p">((</span><span class="n">tmp</span> <span class="o">=</span> <span class="n">write</span> <span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="n">pidsize</span><span class="p">))</span> <span class="o">!=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">pidsize</span><span class="p">){</span>
</span><span class="line">    <span class="c1">//写入不成功, 退出......</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">一些注意事项：</h3>

<ol>
  <li>如果进程退出，则该进程加的锁自动失效。</li>
  <li>如果进程关闭了该文件描述符fd， 则加的锁失效。(整个进程运行期间不能关闭此文件描述符)</li>
  <li>锁的状态不会被子进程继承。如果进程关闭则锁失效而不管子进程是否在运行。
 (Locks are associated with processes. A process can only have one kind of lock set for each byte of a given file. When any file descriptor for that file is closed by the process, all of the locks that process holds on that file are released, even if the locks were made using other descripors that remain open. Likewise, locks are released when a process exits, and are not inherited by child processes created using fork.)</li>
</ol>

<h3 id="section-2">其他</h3>

<p><strong>C的fcntl函数：</strong></p>

<pre><code>int fcntl(int fd, int cmd, struct flock *lock);
</code></pre>

<p>参数cmd代表欲垄断的号召</p>

<ul>
  <li>F_DUPFD 复制参数fd的文件描写符，厉行获胜则归来新复制的文件描写符，</li>
  <li>F_GETFD 获得close-on-exec符号，若些符号的FD_CLOEXEC位为0，代表在调用exec()相干函数时文件将不会关闭</li>
  <li>F_SETFD 设置close-on-exec符号，该符号以参数arg的 FD_CLOEXEC位定夺</li>
  <li>F_GETFL 获得open()设置的符号</li>
  <li>F_SETFL 改换open()设置的符号</li>
  <li>F_GETLK 获得文件锁定的事态，依据lock的描写，定夺是否上文件锁</li>
  <li>F_SETLK 设置文件锁定的事态，此刻flcok，构造的l_tpye值定然是F_RDLCK、F_WRLCK或F_UNLCK，
万一无法发生锁定，则归来-1</li>
  <li>F_SETLKW 是F_SETLK的阻塞版本，在无法获得锁时会进去睡眠事态，万一能够获得锁可能捉拿到信号则归来</li>
</ul>

<p>参数lock指针为flock构造指针定义如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>flock structure</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="k">struct</span> <span class="n">flock</span> <span class="p">{</span>
</span><span class="line">	<span class="p">...</span>
</span><span class="line">	<span class="kt">short</span> <span class="n">l_type</span><span class="p">;</span>
</span><span class="line">	<span class="kt">short</span> <span class="n">l_whence</span><span class="p">;</span>
</span><span class="line">	<span class="kt">off_t</span> <span class="n">l_start</span><span class="p">;</span> <span class="err">锁定区域的开关位置</span>
</span><span class="line">	<span class="kt">off_t</span> <span class="n">l_len</span><span class="p">;</span> <span class="err">锁定区域的大小</span>
</span><span class="line">	<span class="n">pid_t</span> <span class="n">l_pid</span><span class="p">;</span> <span class="err">锁定动作的历程</span>
</span><span class="line">	<span class="p">...</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>1_type有三种事态：</p>

<ul>
  <li>F_RDLCK读取锁（分享锁）</li>
  <li>F_WRLCK写入锁（排斥锁）</li>
  <li>F_UNLCK解锁</li>
</ul>

<p>l_whence也有三种措施</p>

<ul>
  <li>SEEK_SET以文件开始为锁定的起始位置</li>
  <li>SEEK_CUR以现在文件读写位置为锁定的起始位置</li>
  <li>SEEK_END以文件尾为锁定的起始位置</li>
</ul>

<p><strong>Python用法:</strong></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_fcntl.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">signal</span><span class="o">,</span> <span class="nn">fcntl</span><span class="o">,</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">time</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">signal_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
</span><span class="line">    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">pid_file</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_UN</span><span class="p">)</span>
</span><span class="line">    <span class="n">pid_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="n">pid_path</span><span class="p">)</span>
</span><span class="line">    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
</span><span class="line">    <span class="n">pid_path</span> <span class="o">=</span> <span class="s">&#39;test.pid&#39;</span>
</span><span class="line">    <span class="n">pid_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">pid_path</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>
</span><span class="line">    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">pid_file</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_EX</span> <span class="o">|</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_NB</span><span class="p">)</span>
</span><span class="line">    <span class="n">pid_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%d</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
</span><span class="line">    <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">signal_handler</span><span class="p">)</span>
</span><span class="line">    <span class="k">while</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;Hello World&quot;</span>
</span><span class="line">        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java Log4j]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/java-log4j/"/>
    <updated>2015-07-26T15:28:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/java-log4j</id>
    <content type="html"><![CDATA[<h2 id="java-log4j-">Java Log4j 使用记录</h2>

<p>Log4j即为Java的日志记录框架，除了Java语言外，它还支持其他的语言接口：C、C++、.Net和PL/SQL。说道日志框架，其他使用较多的日志框架还包括Logback、SLF4J Simple Logging、Java Util Logging，这些日志框架都是大同小异，目的都是用来记录程序运行的状态。它们的区别主要是在用法和性能上，由于日志通常涉及到IO读写磁盘（或者是阻塞或者是异步），这需要耗费时间，假设应用系统的日志比较庞大，对性能要求比较高，那么就需要好好斟酌下使用的框架。这里有个图可以直观感受下几个框架的区别：</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/log%20comparision_zpstrg3kc1m.png" alt="image" /></p>

<p>上图中的数字是每秒的日志写入行数，<a href="https://docs.google.com/spreadsheet/ccc?key=0Alceaf46X4GPdHBoLTdYQ29nRDh6V1dRY00zT1FwWWc&amp;usp=sharing">点击更详细的信息</a>。</p>

<!--more-->

<h2 id="hello-word-example">Hello Word Example</h2>

<p>示例是使用maven来组织的。maven用的越多，就越觉得它的方便，不用关心jar包的管理，开发、测试和发布都只需要一行命令，还能还IDE完美的融合。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>pom.xml</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="xml"><span class="line"><span class="nt">&lt;dependency&gt;</span>
</span><span class="line">    <span class="nt">&lt;groupId&gt;</span>log4j<span class="nt">&lt;/groupId&gt;</span>
</span><span class="line">    <span class="nt">&lt;artifactId&gt;</span>log4j<span class="nt">&lt;/artifactId&gt;</span>
</span><span class="line">    <span class="nt">&lt;version&gt;</span>1.2.17<span class="nt">&lt;/version&gt;</span>
</span><span class="line"><span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>创建log4j.properties文件，放在resources文件夹下，这个文件夹的路径是src/main/resources。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>log4j.properties</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="properties"><span class="line"><span class="c"># Root logger option</span>
</span><span class="line"><span class="na">log4j.rootLogger</span><span class="o">=</span><span class="s">DEBUG, stdout, file</span>
</span><span class="line">
</span><span class="line"><span class="c"># Redirect log messages to console</span>
</span><span class="line"><span class="na">log4j.appender.stdout</span><span class="o">=</span><span class="s">org.apache.log4j.ConsoleAppender</span>
</span><span class="line"><span class="na">log4j.appender.stdout.Target</span><span class="o">=</span><span class="s">System.out</span>
</span><span class="line"><span class="na">log4j.appender.stdout.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
</span><span class="line"><span class="na">log4j.appender.stdout.layout.ConversionPattern</span><span class="o">=</span><span class="s">%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n</span>
</span><span class="line">
</span><span class="line"><span class="c"># Redirect log messages to a log file, support file rolling.</span>
</span><span class="line"><span class="na">log4j.appender.file</span><span class="o">=</span><span class="s">org.apache.log4j.RollingFileAppender</span>
</span><span class="line"><span class="na">log4j.appender.file.File</span><span class="o">=</span><span class="s">C:\\log4j-application.log</span>
</span><span class="line"><span class="na">log4j.appender.file.MaxFileSize</span><span class="o">=</span><span class="s">5MB</span>
</span><span class="line"><span class="na">log4j.appender.file.MaxBackupIndex</span><span class="o">=</span><span class="s">10</span>
</span><span class="line"><span class="na">log4j.appender.file.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
</span><span class="line"><span class="na">log4j.appender.file.layout.ConversionPattern</span><span class="o">=</span><span class="s">%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>说明下ConversionPattern中的符号和参数：</p>

<ol>
  <li><code>%d{yyyy-MM-dd HH:mm:ss}</code> 日期和时间</li>
  <li><code>%-5p</code> 日志优先级, 输出DEBUG、ERROR等. <code>-5</code>是可选的, 格式化输出宽度，为了更好的输出效果.</li>
  <li><code>%c{1}</code> 是日志名称，通过getLogger()设置, 可以参考<a href="http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html">log4j PatternLayout guide</a>.</li>
  <li><code>%L</code> 打印日志的句子在源文件中的行数.</li>
  <li><code>%m%n</code> 打印的日志信息和换行符.</li>
</ol>

<p>上述设置的输出效果是：</p>

<pre><code>2014-07-02 20:52:39 DEBUG className:200 - This is debug message
2014-07-02 20:52:39 DEBUG className:201 - This is debug message2
</code></pre>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>HelloExample.java</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kn">import</span> <span class="nn">org.apache.log4j.Logger</span><span class="o">;</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">HelloExample</span><span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="kd">final</span> <span class="kd">static</span> <span class="n">Logger</span> <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">HelloExample</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line">
</span><span class="line">    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">        <span class="n">HelloExample</span> <span class="n">obj</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HelloExample</span><span class="o">();</span>
</span><span class="line">        <span class="n">obj</span><span class="o">.</span><span class="na">runMe</span><span class="o">(</span><span class="s">&quot;billowkiller&quot;</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="kd">private</span> <span class="kt">void</span> <span class="nf">runMe</span><span class="o">(</span><span class="n">String</span> <span class="n">parameter</span><span class="o">){</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span><span class="o">(</span><span class="n">logger</span><span class="o">.</span><span class="na">isDebugEnabled</span><span class="o">())</span> <span class="o">{</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">debug</span><span class="o">(</span><span class="s">&quot;This is debug : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="k">if</span><span class="o">(</span><span class="n">logger</span><span class="o">.</span><span class="na">isInfoEnabled</span><span class="o">())</span> <span class="o">{</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;This is info : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="s">&quot;This is warn : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;This is error : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">fatal</span><span class="o">(</span><span class="s">&quot;This is fatal : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">try</span><span class="o">{</span>
</span><span class="line">            <span class="mi">1</span><span class="o">/</span><span class="mi">0</span><span class="o">;</span>
</span><span class="line">        <span class="o">}</span><span class="k">catch</span><span class="o">(</span><span class="n">ArithmeticException</span> <span class="n">ex</span><span class="o">){</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Sorry, something wrong!&quot;</span><span class="o">,</span> <span class="n">ex</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>可以调整配置文件中的<code>log4j.rootLogger</code>设置日志输出的级别和<code>appender</code>，级别从低到高有DEBUG、INFO、WARN、ERROR、FATAL，一般使用INFO和ERROR。</p>

<p>顺便说一句，类似<code>logger.isDebugEnabled()</code>这种句子在源码中实在是不太美观。如果debug中间的信息不影响程序的性能还是不要使用这种句子，因为log4j自己会检查需不需要输出debug信息。</p>

<h3 id="tips">Tips</h3>

<ul>
  <li>java -calsspath *.jar 加载多个jar包，而多个jar包中可能包含多个log4j.properties的时候，系统自动加载第一个，忽略后面的。也就是说将你想要使用的log4j.properties所包含的jar文件放在classpath的第一个。</li>
  <li>mvn的package和test可以分别对应两个不同的log4j.properties中，如下：src/main/resources/log4j.properties, src/test/resources/log4j.properties。</li>
  <li><strong>需要注意程序的输出级别，用户可以定义不同的配置文件来输出不同级别的信息</strong>。这一点尤其重要：
    <ul>
      <li>工具是为程序提供服务的，在开发和调试阶段，日志可以帮助我们更好更快地定位bug；在运行维护阶段，日志系统又可以帮我们记录大部分的异常信息，从而帮助我们更好的完善系统。</li>
      <li>定义好级别和输出的日志信息，配合使用<code>grep</code>等命令行工具，可以更好的帮助我们定位错误。</li>
      <li>在设计系统之前需要先确定好日志子系统，在什么情况下程序会发生错误，输出什么样的错误信息，在debug和正常运行的时候日志如何调整，error信息是否需要另外打印。</li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
</feed>
