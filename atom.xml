<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tech Digging and Sharing]]></title>
  <link href="http://billowkiller.github.io/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-11-30T23:48:57+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[BigData Performance Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2016/11/19/bigdata-perf-tunning/"/>
    <updated>2016-11-19T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/11/19/bigdata-perf-tunning</id>
    <content type="html"><![CDATA[<h2 id="section">单节点优化</h2>

<h3 id="section-1">增加并发吞吐</h3>
<ul>
  <li>IO密集：多进程，多线程，异步，协程</li>
  <li>CPU密集：多进程，多线程</li>
</ul>

<p>异步：回调机制、事件驱动</p>

<p>协程(同步模型开发的方式达到异步效果)：Ucontext(glibc)  Bthread（baidu rpc）</p>

<p>除非资源达到瓶颈，否则不排队</p>

<ul>
  <li>合适并发模型</li>
  <li>队伍要均衡</li>
  <li>过长的队伍，及时柔性处理（可丢服务）</li>
</ul>

<p><em>socket队列改成request队列</em></p>

<!--more-->

<h3 id="section-2">去除不必要动作</h3>

<ul>
  <li>减少网络重连（长连接）</li>
  <li>减低连接数（连接池）</li>
  <li>减少线程切换（线程池）</li>
  <li>减少内存分配和释放（内存池）</li>
  <li>减少耗时的操作和运算（memset, 浮点运算，除法，指数，对数运算，慎用stl）</li>
  <li>在线转离线（离线生成词典）</li>
</ul>

<h3 id="section-3">避免冲突</h3>

<ul>
  <li>多线程无锁算法
    <ul>
      <li>无锁共享数据</li>
      <li>copy on write</li>
    </ul>
  </li>
  <li>hash冲突
    <ul>
      <li>桶如何分配</li>
      <li>如何减少hash冲突</li>
    </ul>
  </li>
  <li>合理使用锁
    <ul>
      <li>所得时间尽可能短</li>
      <li>减低冲突概率</li>
      <li>避免死锁   </li>
    </ul>
  </li>
</ul>

<h3 id="io">IO优化</h3>

<p>顺序io 600MB/s VS 随机io 100KB/s</p>

<p>随机修改：</p>

<ul>
  <li>WAL</li>
  <li>LSM-Tree</li>
  <li>批量去重，减低读写次数（后链入库，hold去重）</li>
</ul>

<p>随机读取：</p>

<ul>
  <li>减少IOPS</li>
  <li>优化cache，预热cache（发些虚假的query）</li>
  <li>SSD</li>
</ul>

<p>kafka</p>

<ul>
  <li>顺序写磁盘效率比随机写内存还要高，高吞吐</li>
  <li>重复利用page cache， 直接内存读取直接发送</li>
</ul>

<h2 id="section-4">集群优化</h2>

<h3 id="section-5">减低数据传输量</h3>
<ul>
  <li>数据压缩，cpu和网络io权衡</li>
  <li>减少交互次数（数据量也降低 header）</li>
  <li>打包访问</li>
  <li>减少跨机io</li>
</ul>

<h3 id="section-6">均衡</h3>

<ul>
  <li>负载均衡
    <ul>
      <li>RR, Random， locality-aware，hash（固定后端）</li>
    </ul>
  </li>
  <li>热点+打散
    <ul>
      <li>自动拆分和融合节点</li>
      <li>自动伸缩容量，弹性</li>
    </ul>
  </li>
  <li>消除长尾（quorum，预测模式）</li>
  <li>消峰，限流，缓冲 + 延迟处理（优先级机制）</li>
  <li>丢弃，降级处理 </li>
</ul>

<h3 id="cache">cache</h3>

<p>有结果cache、【无结果cache、超时cache】反着玩
只读、读写cache</p>

<h2 id="section-7">系统优化</h2>

<p>容器+混布</p>

<p>全量模型 增量模型</p>

<p>避免局部瓶颈，每个子系统的扩展性很重要</p>

<p>木桶效应</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Photon: fault-tolerant and scalable joining of continuous data streams]]></title>
    <link href="http://billowkiller.github.io/blog/2016/11/19/photon/"/>
    <updated>2016-11-19T11:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/11/19/photon</id>
    <content type="html"><![CDATA[<h2 id="intro-and-issues">Intro and Issues</h2>

<p>Photon 是 Google 的数据流合并的系统，目的是 “perform continuous stream joining in real-time”，例如 query log 和 click log 合并。为什么需要对数据流合并呢？</p>

<p>先看下用户使用的场景：用户先 query 一个搜索词，这时候的日志可以直接通过 Google 的网关服务器直接落盘，这里的日志称为 query log，也就是展示广告，表示发给用户的广告信息；之后的一段时间，用户浏览网页，可能会点击到某一条广告，这时候再将用户的 click log 发到某个 log datacenter。</p>

<p>这两个日志所附带的内容区别很大，query log 可以附带很多的信息，类似于展示位、广告主出价等，而 click log 的内容是受限的：虽然可以把需要的信息附带到广告的 url 上，但是传输量变大增加延迟降低用户体验，另外 url 的长度也是受限的。因此很有必要进行日志的合并以用于后续的计费、报表、模型训练等等。</p>

<p>那么处理这个问题的难点在哪里？</p>

<!--more-->

<ul>
  <li>Exactly-once semantics: 任意时刻 at-most-once，实时的 near-exact，最终 exactly-once。</li>
  <li>Automatic datacenter-level fault-tolerance</li>
  <li>High scalability</li>
  <li>Low latency</li>
  <li>unordered streams: 特别的，由于网络或者其他原因，query log 可能会落后于 click log。</li>
</ul>

<h2 id="big-ideas">Big Ideas</h2>

<p>从对系统的要求来看，有两个重要的概念：</p>

<ul>
  <li><code>Persistent State Consistency</code></li>
  <li><code>Exactly Once Eventually</code></li>
</ul>

<h3 id="persistent-state-consistency">1. Persistent State Consistency</h3>

<p>高可用，容错的系统的最简单方法就是冗余，Photon 也如此。导致的问题是在不同的数据中心可能会处理同一份数据。这时候需要对 Worker 进行协调，保证它们不对同一份数据处理。方案是对处理好的 click_id 存储下来，各个 Worker 处理的时候查询下，并在输出 joined log 之前先存储 click_id 以保证 at most once 语义。</p>

<p>Photon 使用基于 <code>Multi-Paxos</code> 的容错、强一致的 <code>IdRegisty</code>。Paxos 能够保证在大多数副本之间同步状态，实现一致性。如下图是一个 IdRegistry 的架构：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/63413408.jpg" width="450px" /></p>

<p>不同的数据中心来回的传输时间可能高达100ms，导致 IdRegistry 的 TPS 最高只有 10，对于每秒需要处理上万日志的 photon 来说是不可以接受的。那么如何增加吞吐呢？有两个措施：<code>server-side batching</code>，<code>sharding</code>。这是两个经典的扩大吞吐量的方法，批量和分治。原文如下：</p>

<blockquote>
  <p>Server-side batching combine multiple event-level commits into one bigger commit. The registry thread dequeues multiple requests and batches them into a single PaxosDB transaction.</p>
</blockquote>

<p>但是对于一个批量的 Paxos RPC 怎么区分有冲突的 event_id 原文并没有做过多的说明。</p>

<blockquote>
  <p>To take advantage of the event_id independence, we partition the event id space handled by the IdRegistry into disjoint shards such that event ids from separate shards are managed by separate IdRegistry servers. </p>
</blockquote>

<p>sharding 需要 resharding 是比较麻烦的，需要考虑向后兼容性。对于落后的 log 来说它必须在 resharding 之后还能找到原来的 shard，原文称为 “deterministic mapping”。Photon 通过增加时间窗口实现，在 timestamp 在 [0, now + S] 内使用之前的sharding。这个时间会通过 True Time API 校正，这个 API 是 Google 特有的，通过原子时钟校正，不具有普适性。这里不得不感叹 Google 的基础架构之强大。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/39260007.jpg" width="500px" /></p>

<h3 id="exactly-once-eventually">2. Exactly Once Eventually</h3>

<p>这个语义和架构有关，所以我们先看下 Photon 的架构，了解里面的组件如何实现 Exactly-once，这里说明下印象比较深的是在 Paper 中， Photon 使用两个非常普遍的技术：<code>异步 PRC</code>, <code>throttling</code>，在提高效率和稳定性方面带来很大的帮助：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/70920655.jpg" width="500px" /></p>

<p><strong>Dispatcher</strong></p>

<p>Dispatcher 负责监控日志文件，并将它们及时的传送到系统。需要注意读取文件的状态元数据会保存在GFS上，用于 failover。发送前需要查看 event_id 是否在 IdRegistry 中，这里的 event_id 需要保持唯一，Photon 使用 &lt;ServerIP, ProcessID, Timestamp&gt; 作为 event_id。</p>

<p>Dispatcher 异步的发送日志到 joiner，等待 joiner 的 ack。如果失败，则会将 click log 存入 GFS，稍后使用指数回退算法重试。一旦重试时间超过阈值，则判定日志为 unjoinable。通过这种方式达到 Dispatcher 的 at least once。</p>

<p>另外至少会有两条 Photon 流在运行，即使一个数据中心挂了，另外一个也可以正常工作。并且在恢复之后会执行 backlog，追上 IdRegistry 中已有的信息，需要注意的是追赶时候的 throttling，防止 IdRegistry 压力过大。</p>

<p><strong>joiner</strong></p>

<p>joiner 接收 dispatcher 传来的数据，并且协调 EventStore 和 IdRegistry，执行日志合并的业务逻辑。joiner 解析 click log 的 click_id 和 query_id，异步获取 EventStore 的 query log 进行比较，一旦获取失败或者 dispatcher 发送过多数据，则会导致失败使得dispatcher 进入重试逻辑。这里的 throttling 是用于 joiner 能维持平滑的数据流。</p>

<p>joiner 使用 adaptor 库处理业务逻辑。一旦 join 成功，则异步地注册 click_id 到 IdRegistry，成功注册后才允许下发 joined log 给下游。这两个步骤在 Photon 中并不是事务地进行，所以一旦在下发前宕机或者 ack 丢失就会导致 joined log 丢失。</p>

<p>之所以不进行事务处理，原因应该是想让 joiner <code>无状态</code>，提高可扩展性。导致的风险可以用下面的方法解决。</p>

<p>提交 click_id 给 IdRegistry 时，joiner 会附带自己的 &lt;ServerIP, ProcessID, Timestamp&gt; 信息作为 <code>unique token</code> 一并提交。当 IdRegistry 收到 click_id 的提交信息时，会比较 token，
相同则说明是上一次没有收到 ack 的那个 joiner 发过来的。IdRegistry 此时返回成功，允许 joiner 下发合并日志。</p>

<p>但此种方法并没有解决宕机带来的不一致。Photon 给的方案是使用另外一个 <code>verification system</code> 作为补充。一旦检测到 IdRegistry 中的 event 并不在 output log 中，则删除 IdRegistry 的 click_id，并重新注入 click log。</p>

<p>以上其实是通过内部机制实现尽力而为的 exactly-once，最后通过类似的 lambda 架构实现最终 exactly-once。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MillWheel: Fault-Tolerant Stream Processing at Internet Scale]]></title>
    <link href="http://billowkiller.github.io/blog/2016/11/13/millwheel/"/>
    <updated>2016-11-13T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/11/13/millwheel</id>
    <content type="html"><![CDATA[<h2 id="intro-and-issues">Intro and Issues</h2>
<p>MillWheel 是 Google 开发的一个低延时的流式处理系统。用户指定计算的 DAG 图，以及图中每个节点的计算代码，MillWheel 负责数据流的计算，保证整个计算过程的分布式以及容错性。从宏观上看，MillWheel 提供了数据计算的幂等性，在用户视角提供不重不丢的语义。</p>

<p>在论文中，还存在一个使用场景。Google 的 Zeitgeist 服务用来监测网络搜索的趋势，输入端是持续不断的搜索内容，经过异常检测，异常检测通过模型预测来排除false positive，输出 spike 或者 dip 的搜索。这要求解决：</p>

<ul>
  <li>Persistent storage： 模型预测和峰值判断分别依赖于长期和短期的存储。</li>
  <li>Low Watermarks: 流量低谷的判断需要有一个标志来判断期待时间窗口内的数据都到达。</li>
  <li>Duplicate Prevention: 在各种情况下用户无需考虑重复流量，系统保证数据传输和处理的 exactly-once 语义。</li>
</ul>

<!--more-->

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-22/53383350.jpg" width="600px" /></p>

<p>综合而言，对比于一般的分布式流式系统，存在一些特别的要求：</p>

<ul>
  <li>处理数据乱序的问题。</li>
  <li>处理数据到达延时问题。</li>
  <li>提供 exactly-once 的保证，尤其在 failover 的时候。</li>
  <li>需要有持久存储，读写权限暴露给用户，提供保证一致性。</li>
</ul>

<h2 id="big-ideas">Big Ideas</h2>

<p>从对系统的要求来看，有两个重要的概念：<code>low watermark</code> 和 <code>exactly-once</code>。low watermark 解决前两个问题，exactly-once 解决后两个问题。</p>

<h3 id="low-watermark">low watermark</h3>

<p>low watermark 在原文的作用如下：</p>

<blockquote>
  <p>The low watermark for a computation provides a bound on the timestamps of future 
records arriving at that computation.</p>
</blockquote>

<blockquote>
  <p>Strip away outliers and offer heuristic low watermark values for pipelines that are more interested in speed than accuracy. </p>
</blockquote>

<p>即时间窗口内所有事件的时间下限，目的是为了让这个窗口可以正常的划出，更快的计算出结果。它其实是提供了一个折衷, 这个折衷就是在watermark这个点认为不会有比 watermark 值更老的数据到来了，本质上是一个barrier, 即系统中所有正在流动的数据的 timestamp 都大于或等于该 watermark。</p>

<p>之所有需要有这个概念，是由于网络环境或者故障导致数据流乱序，窗口无法判断何时数据已经全部到达。这里的时间计算一般是基于eventtime的，即时间发生时间，如logtime。文中也给了一个应用的例子：</p>

<blockquote>
  <p>In the case of Zeitgeist, our input would be a continuously arriving set of search queries, and our output would be the set of queries that are spiking or dipping.</p>
</blockquote>

<p>it is important to be able to distinguish whether a flurry of expected Arabic queries at t = 1296167641 is simply delayed on the wire, or actually not there.</p>

<p>波谷由于数据量小，对到达的数据比较敏感，所以需要判断是真的到达波谷，还仅仅是由于延迟导致数据还在传输途中。之前的做法是再设置一个安全时间，延迟几分钟再计算，但安全时间如何设置？过长则影响时效性，过短无法奏效。或者采取右端开放窗口的补偿方法，先产出数据，当延迟数据到来时再予以修改，但需要业务场景适用、下游系统支持修改。</p>

<p>除了aggregation计算对时序性有要求外，一些策略计算可能也会要求数据完整、有序。那么 low watermark 的定义如下：</p>

<pre><code>low watermark of A = min(oldest work of A, low watermark of C: C outputs to A)
If there are no input streams, the low watermark and oldest work values are equivalent.
</code></pre>

<p>A 点的 low watermark 的值等于 A 点所有待处理的消息的 timestamp 和 A 所有上游 C 的 low watermark 的最小值。这是一个递归的定义，终止条件处于流的源头，即由外围注入的injection或者数据订阅端生成；生成之后会驱动 watermark 传递给下游。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-23/12047055.jpg" width="600px" /></p>

<p>由上图可以看出，low watermark 是根据事件到达后的处理时间递增的，反映的是系统中所有 <em>pending work</em>；并且事件其实并不是有序到达和处理的，但是在某个时间可以确保所有的事件都已经到达了，即在文中所强调的：</p>

<blockquote>
  <p>By waiting for the low watermark of a computation to advance past a certain value, the user can determine that they have a complete picture of their data up to that time</p>
</blockquote>

<p>对于处于 low watermark 下的事件，有两种处理方案：抛弃或者更正已有的状态。</p>

<p>low watermark 除了提供系统事件处理的完成情况，还有一个作用，触发trigger（文中用Timer）。trigger 实际是窗口内部累积状态的记录器，根据不同的触发类型来累积状态，达到一定的条件就允许外围触发，将窗口内的数据计算后 emit 给下游。MillWheel 的触发条件有 wall time 和 low watermark value，这个交给用户来定义。</p>

<h3 id="exactly-once">exactly-once</h3>

<p>这个概念涉及两个方面：</p>

<ul>
  <li>Persistent storage available at all nodes in the stream graph</li>
  <li>Exactly once delivery semantics.</li>
</ul>

<p>前者的概念好理解，Persistent state 在 MillWheel 中表现为一个键值对，值是 protobuf 的一个二进制字符串。用户代码可以方便的获取和设置各种状态。常用的状态为窗口数据聚合中保存的 counter，join 要使用的 buffer 等。</p>

<p>MillWheel 的状态存储在 bigtable 或者 Spanner 中，另外它还提供一个 soft state，用于内存 cache 或者 聚合。为了保证可能存在的状态不一致性，对所有的 per-key update 都封装在一个原子的操作中，防止在处理过程中可能出现的各种意外。</p>

<p>但是在 failover 或者 load balancing 的时候，计算会被迁移，这时候可能存在 zombie writer 或者 stale writer。因此，为了保证原子性，需要保证每个 key 只有一个 writer 可以写入。MillWheel 的解决方案是给每个 writer 附带一个 sequencer token，后台存储的协调者在每个写入前会检查 token 的有效性使得新 worker 可以阻止其他的 sequencer 的有效性。这其实就是一个 lease 机制，在一段时间内对于特定的key只有一个worker可以写入。</p>

<p>保证 Exactly Once 语义有个很重要的简化途径是将用户的非幂等操作变成幂等。通过以下措施来保证：</p>

<ul>
  <li>Exactly Once delivery</li>
  <li>Strong Production</li>
</ul>

<p>Exactly Once delivery 通常的方法是保证 at least once 基础上进行去重。MillWheel 也是通过这个方法保证，发送方发送数据后没有接收到 ack 重新发送数据。另外接收方接收到数据会进行 state modificaton，这时候将数据的 unique ID 也一并原子写入到后端存储；接收端为了加速去重，使用 bloom filter，这时候需要考虑 false postive，如果发生 filter miss，需要去后端存储进行比较，如果确认是重复数据，会返回 duplicate ACK 通知发送方。</p>

<p>在发送方发送数据之前可以进行 checkpoint，用于 failover 后恢复原来的状态。在发送的数据被 ACK 后，checkpoint可以删除。这个 <code>Checkpoint-&gt;Delivery-&gt;ACK-&gt;GC</code> 的运行模式在 MillWheel 中称为 <code>Strong Production</code>。即使用户代码不是幂等的，在 Strong Production 中，无论 retry 多少次，用户的运行逻辑都是正确的。</p>

<p>与之对应的是 <code>Weak Production</code>，这时的用户代码是幂等的，无需strong procution，即 checkpointing；无需 exactly once delivery（把 deduplication 逻辑去掉）。这在延迟和资源消耗上无疑更加友好，但是对于取消 checkpointing 有一个 straggler latency 问题。在 strong production中，只要 checkpoint 就可以返回上游表示这个阶段的任务结束，但是现在需要下游返回 ack 才可以保证，并且这是级联的。Millwheel 采用 checkpointing 一部分 straggler 的 pending production 来缓解，如下图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-23/46647429.jpg" width="400px/" /></p>

<h2 id="other-details">Other Details</h2>

<p>MillWheel 集群可以动态扩缩容，根据 key interval 划分计算，并以此通过 master 进行负载分配、均衡。当监控进程检测到机器的负载过高的时候，会进行 key interval 的重分配：split、merge、move。</p>

<p>failover 时，key interval 被分配到新的 owner，它可以从后端存储中获取计算需要的元数据，包括 heap of pending timers 以及 queue of checkpointed productions。一个节点的例子如下，每个计算都是一个pipeline，即输入计算输出。数据的传输通过RPC。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-24/94589760.jpg" width="400px" /></p>

<p>另外关于 low watermark 的管理和流动，Millwheel 是基于一个全局的订阅发布中央系统。</p>

<ul>
  <li>每个 processer 将自己持有的所有work（in fly, stored, pending）计算出最小的 timestamp发给中央系统。</li>
  <li>不同的 processer 在不同的 key interval 下，low watermark 也是根据 key interval 来存储。</li>
  <li>为了保证计算的准确性，订阅发布系统会查询后台存储，以保证每个 key interval 都有 low watermark。感兴趣的节点会订阅每个发送端的 low watermark，计算所有的最小值作为 low watermark。</li>
  <li>之所以不在中央系统算最小值是为了保证一致性：中央系统的 low watermark 不能大于节点上的，否则在 failover 后容易出现 low watermark 的回退。在节点上计算最小值则保证中央系统永远不会领先于节点上的。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cache]]></title>
    <link href="http://billowkiller.github.io/blog/2016/11/10/cache/"/>
    <updated>2016-11-10T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/11/10/cache</id>
    <content type="html"><![CDATA[<h3 id="cache">单机cache</h3>

<p>数据类型</p>

<ul>
  <li>简单KV get key, set key, expire key, del key</li>
  <li>复杂数据机构， MAP， List, Struct</li>
</ul>

<p>数据持久化</p>

<ul>
  <li>全内存， 无持久化 （memcache）</li>
  <li>内存为主，磁盘为辅，半持久化 (redis 定时dump)</li>
  <li>内存为主，磁盘为辅，提供完全持久化（leveldb， redis aof, 内存映射）</li>
  <li>全磁盘，提供完全持久化（ssd db）</li>
</ul>

<!--more-->

<p>数据淘汰</p>

<ul>
  <li>LRU： hash+list、基于Hazard Pointer的无锁实现 适合访问热点场景（浪费内存，时间等）</li>
  <li>FIFO: 基于queue实现（磁盘用顺序文件）， 适合访问热度较均衡</li>
</ul>

<p>数据安全</p>

<ul>
  <li>快照，存储、恢复、备份
    <ul>
      <li>实时全量，实时增量，定时全量</li>
    </ul>
  </li>
</ul>

<p>一致性</p>

<ul>
  <li>主从复制，保证最终一致性。全量同步，增量同步</li>
</ul>

<p>存储量</p>

<ul>
  <li>meta和value隔离存储， 顺序io与随机io
    <ul>
      <li>meta：leveldb rocksdb</li>
      <li>value: index+rawfile</li>
    </ul>
  </li>
</ul>

<p>ssd cache:</p>

<ul>
  <li>leveldb, rocksdb, ssdb，数据容量限制，value（10K）过大容易引起compaction写放大，阻塞请求</li>
</ul>

<h3 id="cache-1">分布式cache</h3>

<p>cap base原则</p>

<p>多副本：一致性，可靠性，可扩展，副本数</p>

<p>多租户</p>

<ul>
  <li>隔离（业务、部署、入口、存储、调度）、审计（成本、使用量、功能）</li>
</ul>

<p>热点</p>

<ul>
  <li>拆分、多副本、合适引擎（存储、数据结构、架构）</li>
</ul>

<p>数据均衡：</p>

<ul>
  <li>hash缺点 偏静态、成倍扩容、容量预先规划</li>
  <li>slot：基于数据分片，一个集群包含固定slot数量，每条数据属于某个slot</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed System Design]]></title>
    <link href="http://billowkiller.github.io/blog/2016/10/27/distribute-design/"/>
    <updated>2016-10-27T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/10/27/distribute-design</id>
    <content type="html"><![CDATA[<p>分布式系统原理的脑图.</p>

<p>TODO: 先mark，以后再添加内容。</p>

<!--more-->

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-30/42661393.jpg" width="900px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper Roles]]></title>
    <link href="http://billowkiller.github.io/blog/2016/10/23/zk/"/>
    <updated>2016-10-23T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/10/23/zk</id>
    <content type="html"><![CDATA[<p>看到一篇不错的zookeeper应用介绍[http://blog.csdn.net/ldds<em>520/article/details/51679071](http://blog.csdn.net/ldds</em>520/article/details/51679071)，记录下。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-30/15982256.jpg" width="600px" /></p>

<!--more-->

<table cellspacing="0" cellpadding="0" style="margin:1em 0px 2em; padding:0px; width:607px; border-collapse:collapse; border-spacing:0px; color:rgb(51,51,51); font-family:'Segoe UI',Calibri,'Myriad Pro',Myriad,'Trebuchet MS',Helvetica,Arial,sans-serif; font-size:13px; border-style:solid">
<tbody>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>数据发布与订阅（配置中心）</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。</li><li style="margin:0px; padding:0px">分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。</li><li style="margin:0px; padding:0px">分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。</li><li style="margin:0px; padding:0px">系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>注意</strong>：在上面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>负载均衡</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
这里说的负载均衡是指软负载均衡。在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。<strong></strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的<a target="_blank" href="http://metaq.taobao.org/" style="color:rgb(45,138,199); text-decoration:none; outline:none">metaq</a>都是通过zookeeper来做到生产者、消费者的负载均衡。这里以metaq为例如讲下：<br />
<strong>生产者负载均衡</strong>：metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>消费负载均衡：</strong></p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。MetaQ的消费策略是：</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">每个分区针对同一个group只挂载一个消费者。</li><li style="margin:0px; padding:0px">如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。</li><li style="margin:0px; padding:0px">如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>命名服务(Naming Service)</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。<strong></strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，<a target="_blank" href="http://code.alibabatech.com/wiki/display/dubbo/Home" style="color:rgb(45,138,199); text-decoration:none; outline:none">点击这里</a>查看Dubbo开源项目。在Dubbo实现中：
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>服务提供者</strong>在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>服务消费者</strong>启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
<strong>注意</strong>，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式通知/协调</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。</li><li style="margin:0px; padding:0px">另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。</li><li style="margin:0px; padding:0px">另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。</li></ul>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>集群管理与Master选举</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题：</li></ul>
<ol style="margin:0px 0px 10px 15px; padding:0px; list-style-type:disc; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">集群中机器有变动的时候，牵连修改的东西比较多。</li><li style="margin:0px; padding:0px">有一定的延时。</li></ol>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：</p>
<ol style="margin:0px 0px 10px 15px; padding:0px; list-style-type:disc; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。</li><li style="margin:0px; padding:0px">创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。</li></ol>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">Master选举则是zookeeper中最为经典的应用场景了。</li></ul>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
另外，这种场景演化一下，就是动态Master选举。这就要用到?EPHEMERAL_SEQUENTIAL类型节点的特性了。</p>
<p align="left" style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
上文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,?/currentMaster/{sessionId}-2 ,?/currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。</p>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#D0D0D0" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。</li><li style="margin:0px; padding:0px">在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。在Hbase实现中，会在ZK上存储一些ROOT表的地址和HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了HMaster的单点问题</li></ul>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式锁</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是<strong>保持独占</strong>，另一个是<strong>控制时序</strong>。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<ul style="margin:0px 0px 10px 15px; padding:0px; list-style-position:outside; line-height:19.5px">
<li style="margin:0px; padding:0px">所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。</li><li style="margin:0px; padding:0px">控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。</li></ul>
</td>
</tr>
<tr>
<td valign="top" bgcolor="#8CEA00" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); font-size:18px; background:rgb(239,239,239)">
<strong>分布式队列</strong></td>
</tr>
<tr>
<td valign="top" bgcolor="#C2C287" width="909" style="margin:0px; padding:5px; border:2px solid rgb(255,255,255); background:rgb(239,239,239)">
队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
</p>
<p style="margin-top:0.25em; margin-bottom:0.75em; padding-top:0px; padding-bottom:0px; line-height:19.5px">
第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当
 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。</p>
<div><br />
</div>
</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paxos and Raft]]></title>
    <link href="http://billowkiller.github.io/blog/2016/09/23/paxos-raft/"/>
    <updated>2016-09-23T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/09/23/paxos-raft</id>
    <content type="html"><![CDATA[<h2 id="distributed-consensus">1. Distributed Consensus</h2>

<p>分布式一致性是指在一组process里对一个value达成的一致。这个value可以是任何操作，例如“修改某个变量的值”，“设置某个节点为primary”等等。为什么需要分布式一致性呢，因为在分布式系统里，组件是不可靠的，无论是网络问题还是组件自身的问题，我们希望在这些不可靠组件构成的系统中得到一个可靠的系统。</p>

<p>一致性协议就是为了解决这些不可靠组件如何对外提供一致的value。具体说来可以认为多个process可以提供不同的value，一致性协议能迫使这些组件互相协作得到一个一致的结论，并且允许有少量的process出现失败或重启。</p>

<p>一致性协议可以应用的场景包括节点中状态机的复制，分布式的键值存储，分布式序号生成等等，任意节点都可以接收消息，但是对外提供的确是一致的value。</p>

<!--more-->

<p>在分布式系统中，基础复制方法有以下几种：</p>

<ul>
  <li>主从异步复制（磁盘在复制前损毁，则造成数据丢失）</li>
  <li>主从同步复制（一个失联节点会造成整个系统不可用）</li>
  <li>主从版同步复制（可能任何从库都不完整，需要多数派读写）</li>
  <li>多数派读写</li>
</ul>

<h2 id="paxos">2. Paxos</h2>

<p>Paxos算法是Lamport于1990年提出的一种基于消息传递的一致性算法，一开始并未引起人们注意，直到06年google的chubby锁服务使用paxos作为chubby cell中的一致性算法，paxos的人气从此一路狂飙。</p>

<p>Paxos有两个原则：</p>

<ul>
  <li>安全原则—保证不能做错的事
    <ul>
      <li>只能有一个值被批准，不能出现第二个值把第一个覆盖的情况</li>
      <li>每个节点只能学习到已经被批准的值，不能学习没有被批准的值</li>
    </ul>
  </li>
  <li>存活原则—只要有多数服务器存活并且彼此间可以通信最终都要做到的事
    <ul>
      <li>最终会批准某个被提议的值</li>
      <li>一个值被批准了，其他服务器最终会学习到这个值 </li>
    </ul>
  </li>
</ul>

<p>Paxos有两个角色：</p>

<ul>
  <li>Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。</li>
  <li>Acceptor：提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值。</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-20/80935469.jpg" width="700px" /></p>

<h3 id="paxos-1">2.1 Paxos过程</h3>

<p>下面介绍Paxos协议过程，以及解决的一些问题。</p>

<p><strong>1、第一阶段 Prepare</strong></p>

<p><strong>P1a：Proposer 发送 Prepare</strong></p>

<p>Proposer 生成全局唯一且递增的提案 ID（Proposalid，以高位时间戳 + 低位机器 IP 可以保证唯一性和递增性），向 Paxos 集群的所有机器发送 PrepareRequest，这里无需携带提案内容，只携带 Proposalid 即可。</p>

<p><strong>P1b：Acceptor 应答 Prepare</strong></p>

<p>Acceptor 收到 PrepareRequest 后，做出“两个承诺，一个应答”。</p>

<p>两个承诺：</p>

<ul>
  <li>第一，不再应答 Proposalid <code>小于等于</code>（注意：这里是 &lt;= ）当前请求的 PrepareRequest；</li>
  <li>第二，不再应答 Proposalid <code>小于</code>（注意：这里是 &lt; ）当前请求的 AcceptRequest</li>
</ul>

<p>一个应答：</p>

<ul>
  <li>返回自己已经 Accept 过的提案中 ProposalID 最大的那个提案的内容，如果没有则返回空值;</li>
</ul>

<p><strong>注意：这“两个承诺”中，蕴含两个要点：</strong></p>

<ul>
  <li>就是应答当前请求前，也要按照“两个承诺”检查是否会违背之前处理 PrepareRequest 时做出的承诺；</li>
  <li>应答前要在本地持久化当前 Propsalid。</li>
</ul>

<p><strong>2、第二阶段 Accept</strong></p>

<p><strong>P2a：Proposer 发送 Accept</strong></p>

<p>“提案生成规则”：Proposer 收集到多数派应答的 PrepareResponse 后，从中选择proposalid最大的提案内容，作为要发起 Accept 的提案，如果这个提案为空值，则可以自己随意决定提案内容。然后携带上当前 Proposalid，向 Paxos 集群的所有机器发送 AccpetRequest。</p>

<p><strong>P2b：Acceptor 应答 Accept</strong></p>

<p>Accpetor 收到 AccpetRequest 后，检查不违背自己之前作出的“两个承诺”情况下，持久化当前 Proposalid 和提案内容。最后 Proposer 收集到多数派应答的 AcceptResponse 后，形成决议。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-20/9823122.jpg" width="600px" /></p>

<h3 id="section">2.2 核心思想</h3>

<ul>
  <li>Optimistic concurrency control. Hold a <code>preemptible lock</code> first, try updating, restart on denial.</li>
  <li>Quorum as a logical unit of acceptor for Choose operation. A value is chosen iff it’s accepted by a quorum, which implies from the proposer’s perspective the Choose operation is <code>atomic</code>, it’s all or nothing, it’s either accepted by a quorum or it isn’t.</li>
</ul>

<h3 id="section-1">2.3 协议推导</h3>

<p>Paxos 协议利用了 Quorum 机制，选择的 W=R=N/2+1。简单而言，协议就是 Proposer 更新 Acceptor 的过程，一旦某个 Proposer 成功更新了超过半数的 Acceptor，则更新成功。Learner 按 Quorum 去读取 Acceptor，一旦某个 value在超过半数的 Acceptor 上被成功读取，则说明这是一个被批准的 value。协议通过引入轮次，使得高轮次的议抢占低轮次的提议来避免死锁。</p>

<p>协议的关键点是如何满足“在一次Paxos算法实例过程中只批准一个Value”，称这个约束为“约束条件 1”，推到过程就是不断推到出“约束条件 1”的充分不必要条件。</p>

<ul>
  <li>“约束条件 2” =&gt; “约束条件 1”：一旦一个 value 获得超过半数的 Acceptor 批准,之后 Paxos 协议实例只能批准这个 value。</li>
  <li>“约束条件 3” =&gt; “约束条件 2”：一旦一个 value 获得超过半数的 Acceptor 批准,之后任何 Acceptor 只能批准这个 value。</li>
  <li>“约束条件 4” &lt;=&gt; “约束条件 3”: 一旦一个 value v 获得超过半数的 Acceptor 批准,之后 Proposer  议的 value 只能是 v。</li>
  <li>“约束条件 5” =&gt; “约束条件 4”：Proposer 提议一个 value v 前,要么之前没有任何一个 value 被批准,要么存在一个大小为 N/2+1 的 Acceptor 集合,这个集合内的各个 Acceptor 批准过的轮数最大的 value 是 v。</li>
</ul>

<p>可以用反证法证明“约束条件 5”.</p>

<h3 id="section-2">2.4 问题</h3>

<ol>
  <li>
    <p>这里可以看到paxos使用了多acceptor，为什么？</p>

    <p>为了解决Acceptor crash的问题，必须要用到一种多数选择的方法。</p>
  </li>
  <li>
    <p>如何保证批准的提案无法改变？</p>

    <p>假设Proposer以更高的序号发提案，但已经批准的提案必然被一半以上的Acceptor接受，那么Acceptor返回应答必然包含这个已经批准的提案，所以此时Proposer发起的Accept消息必然是（高序号，已经批准的提案）这么一个组合。所以一旦一个提案被批准，以后永远只能批准这个提案。</p>
  </li>
  <li>
    <p>paxos中是否存在死锁和活锁？</p>

    <p>虽然paxos协议过程类似于”占坑“，需要value抢占超过半数的”坑“，但是高轮数的提案可以抢占低轮数的提案，所以可以避免死锁的发生。但是这种设计可能导致”活锁“，即Proposer互相不断以更高的轮数提出提案，使得每轮paxos过程都无法完成。一种解决方案是Proposer重新提案之前等待随机时间，让上一个提案有时间进行第二阶段的accept。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/70209540.jpg" width="500px" /></p>
  </li>
  <li>
    <p>如果某个提案之后又另外一个提案跟进会发生什么情况？</p>

    <p>如果前一个提案已经被多数派接受，那新的提案会看到和使用前一个提案作为它的value；如果前一个提案没有被多数派接受，但新提案发现这个提案，那新提案也会使用它作为value；如果前一个提案没有被多数派接受，并且新提案没有发现，则新提案会使用自己的value，旧的提案不会被block。</p>

    <p>&lt;img src=“http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/38064842.jpg” width=”500px”/&gt;
 <img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/41211772.jpg" width="500px" />
<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-9-27/4176188.jpg" width="500px" /></p>
  </li>
  <li>
    <p>server如何知道提案的value？</p>

    <p>只有Proposer自己知道自己选择的提案，别的server如果想要知道必须发起一个paxos instance。</p>
  </li>
  <li>
    <p>在 P2a 阶段，为什么Proposer发起的提案需要使用旧的value和更高的proposalID？</p>

    <p>是受“在一次paxos instance中只批准一个value”的约束。即使读取了N/2+1的acceptor状态，由于没有读取所有acceptor，Proposer也无法判断value是否批准了。这时候选择proposalID更高的旧value，可以保证，要么此时Paxos还没有批准任何一个value，要么只能是旧的value。</p>
  </li>
  <li>
    <p>如果提案被多数派接受后，一个acceptor宕机了，导致多数派不成立，会有什么问题？ </p>

    <p>不会有问题，因为新的Proposer选择的quorum一定包括接受上一次提案的那台机器(接受的机器数至少是N/2)，这时候会使用更高的proposalID提交旧的value。</p>
  </li>
</ol>

<h3 id="zab">2.5 Zab</h3>

<p>Zookeeper 使用一种修改后的 Paxos 协议，称为 Zab。</p>

<p>在 zookeeper 中，始终分为两种场景：</p>

<ul>
  <li>Leader activation：leader 选举，数据同步</li>
  <li>Active messaging：leader 接收 client 更新请求，同步到各个 follower</li>
</ul>

<p>在两种场景中，zk都依赖于一个全局版本号：zxid。zxid 由 (epoch, count) 组成，epoch 是选举编号，每次提议进行leader选举时 epoch 都会增加，count 是 leader 为每个更新操作决定给的序号。从全局看，一个 zxid 代表了一个更新操作的全局序号（版本号）。</p>

<p>每个 zookeeper 节点都有各自最后 commit 的 zxid，表示这个 zookeeper 节点上最近成功执行的更新操作，也代表了这个节点的数据版本。在 Leader activation 阶段，每个 zk 节点都以自己的 zxid 作为 proposalID 发起 paxos instance，设置自己为leader (value)。每个节点既是 Proposer 也是 Acceptor。通过 Paxos 协议，某个超过 quorum 半数的节点中持有最大的 zxid 节点会成为新的leader。实际上 proposalID 会是 (zxid，nodeid)，这是当 zxid 相同时， zk 会选择节点编号较大的成为 leader。成为新的 leader 需要与 follower 进行同步，数据同步过程可能会涉及删除 follower 上的最后一条脏数据。</p>

<p>当与至少半数节点完成数据同步后，leader 更新 epoch，在各个 follower 上以 (epoch + 1, 0) 为 zxid 写一条没有数据的更新操作。这个更新操作称为 NEW_LEADER 消息，是为了在各个节点上更新 leader 信息，当收到超过半数的 follower 对 NEW_LEADER 的确认后，leader 发起对 NEW_LEADER 的 COMMIT 操作，并进入 Active messaging 阶段。</p>

<p>进入 active messaging 状态的 leader 会接收从客户端发来的更新操作，为每个更新操作生成递增的 count，组成递增的 zxid。Leader 将更新操作以 zxid 的顺序发送给各个 follower (包括leader本身, 一个 leader 同时也是 follower，当收到超过半数的 follower 的确认后，Leader 发送针对该更新操作的 COMMIT 消息给各个 follower。这个更新操作的过程很类似两阶段交，只是 leader 永远不会对更新操作做 abort 操作。</p>

<p>如果 leader 不能更新超过半数的 follower，此时可以发起新的 leader 选举。最后一条更新操作处于“中间状态”，其是否生效取决于选举出的新 leader 是否有该条更新操作。</p>

<p>Zookeeper 通过 zxid 将两个场景阶段较好的结合起来，且能保证全局的强一致性。由于同一时刻只有一个 zookeeper 节点能获得超过半数的 follower，所以同一时刻最多只存在唯一的 leader；每个 leader 利用 TCP 带来的消息 FIFO 特点以 zxid 顺序更新各个 follower，只有成功完成前一个更新操作的才会进行下一个更新操作。在同一个 leader 任期内，数据在全局满足 quorum 约束的强一致，即读超过半数的节点一定可以读到最新已提交的数据；每个成功的更新操作都至少被超过半数的节点确认，使得新选举的 leader 一定可以包括最新的已成功提交的数据。</p>

<h2 id="multi-paxos">3. Multi Paxos</h2>

<h2 id="raft">4. Raft</h2>

<h2 id="discussion">5. Discussion</h2>

<h2 id="ref">Ref</h2>

<p><a href="https://docs.google.com/presentation/d/1y2lbLzSmZdd3OVzXpxAvmvWt4Hhn_WljQiU4x30Cst4/edit?usp=sharing">A Beginner’s Guide to Paxos</a></p>

<p><a href="https://ramcloud.stanford.edu/~ongaro/userstudy/">Raft user sutdy</a></p>

<p><a href="https://raft.github.io/">The Raft Consensus</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mesa: Google's near real-time OLAP warehouse]]></title>
    <link href="http://billowkiller.github.io/blog/2016/08/06/mesa/"/>
    <updated>2016-08-06T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/08/06/mesa</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Ashish et al. VLDB 2014</p>
</blockquote>

<p>谷歌Mesa是一个服务于谷歌广告业务的近实时分析型数据仓库，主要用于广告主的报表、内部审计和预测服务。作为底层的存储系统，Mesa能处理PB级的数据，每秒百万的行更新，并且需要应对每日上亿的查询请求，因此它需要满足的设计目标是：</p>

<blockquote>
  <p>near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes.</p>
</blockquote>

<!--more-->

<p>据Google描述，随着他们的广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。为了应付如此持续增长并且重要数据的处理、存储和查询需求，Mesa的具体需求包括：</p>

<ul>
  <li>
    <p><code>原子更新</code>。某一单个的用户行为可能会引起多个关系数据级别的更新，从而影响定义在某个指标集上（例如：点击和成本）跨某个维度集（例如：广告客户和国家）的数千张一致性视图。所以系统状态不会在查询时处于一个只有部分更新生效的状态。</p>
  </li>
  <li>
    <p><code>一致性和正确性</code>。出于业务和法律的原因，该系统必须返回一致和正确的数据。即使某个查询牵涉到多个数据中心，我们仍然需要提供强一致性和可重复的查询结果。</p>
  </li>
  <li>
    <p><code>可用性</code>。系统不允许出现单点故障。不会出现由于计划中或非计划中的维护或故障所造成的停机，即使出现影响整个数据中心或地域性的断电也不能造成停机。</p>
  </li>
  <li>
    <p><code>近实时的更新吞吐率</code>。系统必须支持大约每秒几百万行规模的持续更新，包括添加新数据行和对现有数据行的增量更新。这些更新必须在几分钟内对跨不同视图和数据中心的查询可见。</p>
  </li>
  <li>
    <p><code>查询性能</code>。系统必须对那些对时间延迟敏感的用户提供支持，按照超低延迟的要求为他们提供实时的客户报表，而分批提取用户需要非常高的吞吐率。总的来说，系统必须支持将99%的点查询的延迟控制在数百毫秒之内，并且整体查询控制在每天获取万亿行的吞吐量。</p>
  </li>
  <li>
    <p><code>可伸缩性</code>。系统规模必须可以随着数据规模和查询总量的增长而伸展。举个例子，它必须支持万亿行规模和PB级的数据。但是即使上述参数再出现显著增长，更新和查询的性能必须仍然得以保持。</p>
  </li>
  <li>
    <p><code>在线的数据和元数据转换</code>。为了支持新功能的启用或对现有数据粒度的变更，客户端经常需要对数据模式进行转换或对现有数据的值进行修改。这些变更必须对正常的查询和更新操作没有干扰。</p>
  </li>
</ul>

<p>所有Google现有的大数据技术都无一能满足所有以上的需求。BigTable无法提供<code>原子性和强一致性</code>。而Megastore、Spanner和F1虽然为跨地域复制的数据提供了<code>强一致性</code>的访问，但是他们无法支持Mesa客户端所有需要的<code>峰值更新吞吐率</code>。既有的ROLAP或者MOLAP方法也不能在提供<code>近实时查询</code>同时支持分钟级别的<code>数据更新和聚合</code>。但是Mesa在其不同的基础设施中充分利用了现有的Google技术组件。它使用了BigTable来存储所有<code>持久化的元数据</code>，使用了Colossus (Google的分布式文件系统)来存储数据文件。此外，Mesa还利用了MapReduce来处理连续的数据。</p>

<p>以下有几个技术要点：</p>

<ul>
  <li>为了存储的可伸缩性和可用性，数据是水平分片，并且重复的。</li>
  <li>为了得到一致性和update时候的查询可用性，利用了多版本控制。</li>
  <li>为了更新可伸缩行，数据的更新是批量的和周期性的，并且赋值一个新的版本。</li>
  <li>为了多数据中心数据更新的一致性，Mesa利用基于Paxos的分布式同步协议。</li>
</ul>

<p>Mesa的contributions包括以下一些内容：</p>

<ul>
  <li>高吞吐量的PB级别的数据仓库，同时维持ACID属性以提供事务的处理能力</li>
  <li>新的版本管理方式，利用批量更新，得到低延迟和高吞吐量</li>
  <li>应用数据是通过一个独立和冗余的程序进行异步复制，但是关键的元数据是同步复制。这样可以减少管理副本的同步消耗，并且得到数据更新的高吞吐量</li>
  <li>在线的schema change，并不会影响现有程序的正确性和性能</li>
  <li>软件错误或硬件错误带来的数据损毁的容忍性</li>
</ul>

<h3 id="section">数据模型</h3>

<p>在Mesa的数据是多维度的，定义了细粒度的一个事实表。在这个事实表中分为两种属性，一个是 Key， 一个是 Value。Key是多维的，并且具有层级，例如 date 可以组织成 day, month, year。<strong>单个事实表在这些层级之间的聚合可以被物化，这样就支持数据分析的上卷和下钻。</strong>Value 也就是一些数据 Metrics。</p>

<p>数据是组织成表的，每个表有 schema 用于指定表结构。schema 里面包含了<code>Key空间</code>和<code>Value空间</code>，指明他们的<strong>类型</strong>和<strong>聚合方式</strong>：aggregation function $F: V \times V \to V$。一般来说聚合方法有 SUM、MAX、MIN 和 REPLACE；schema 还指明了table的一个或多个索引方法。</p>

<p>数据按对应的索引序排序分割成限定大小的文件，每个文件内部再按<strong>行分割成行组row blocks进行压缩存储</strong>，每个行组内部的数据在实际存储时，<strong>按列式存储的layout进行转换压缩，提高压缩率</strong>。而<strong>索引文件</strong>由行键取固定长度的前缀组成，映射到对应数据行在行组内部的存储偏移量。因为是一个前缀，所以索引可能不能精确定位一行的位置，而是定位这一行在行组内的偏移范围，在通过二分查找的方式在行组内部定位到具体的行。</p>

<h3 id="section-1">查询和更新</h3>

<p>Mesa中存储的数据是多版本的，更新时版本号是向前叠加的，只有处理完当前的版本才会更新下一个版本。这使得当新的更新正在处理时，Mesa可以向用户提供前置状态的<code>一致性数据</code>，也就是 update 的<code>原子性</code>。这种<strong>严格的版本顺序</strong>还可以保证数据的正确性。</p>

<p>通常，每隔几分钟，上游系统就会执行一次数据更新的批处理，结果为产生数据提交文件。独立的各个<code>无状态</code>的数据提交者实例，负责对跨（Mesa运行所在的）全部数据中心的更新操作进行协调。提交者为每个更新批处理分配一个新的版本号，并基于<code>Paxos一致算法</code>向版本数据库<strong>发布全部与该更新关联的元数据</strong>。当一个更新满足提交的条件时，意味着一个给定的更新已经被全球范围内的大量Mesa实例进行了合并，提交者会将该次更新的版本号声明为新的提交版本号，并将该值存储在版本数据库里。</p>

<p>查询通常都是根据提交版本号来分发的，因为<strong>查询通常都是根据提交版本号来分发的，所以Mesa不需要在更新和查询之间进行任何的锁操作</strong>。更新都是由Mesa实例在批处理中进行异步实施的。这些属性使得Mesa获得了非常高的查询和更新吞吐率，同时也对数据一致性提供了保障。</p>

<h3 id="section-2">版本管理</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-8-7/25645530.jpg" width="500px" /></p>

<p>为了支持查询的高效性，如上图，Mesa中的版本分为Base，Cumulative和Singleton，含义在图中也有了清晰的表达，主要的目的是为了<strong>打平查询时计算的消耗，将数据聚合前置</strong>。Singleton就是上文中的上游产生的数据，每个几分钟产生一个。Base，Cumulative分别是由Base Compaction和Cumulative Compaction产生。二者都会按照一定的规则进行，显然后者的频率会远高于前者。</p>

<p>singleton中因为row都是有序，所以两个singleton归并的时间是线性的。多个文件的归并可以使用<strong>最小堆算法</strong>，达到 nlogn 的时间。如何找到需要版本号的多个文件，可以使用<strong>最短路径算法</strong>。</p>

<p>归并方法与HBase的<code>Memory Store -&gt; flush -&gt; Minor compaction -&gt; major compaction</code>的整体流程概念也很相似。但是与HBase／Bigtable的区别有三点：</p>

<p>第一，批量的写操作，Mesa是直接通过外部系统提交批量数据来实现，HBase则是借助于Memory Store／Flush机制来缓冲，将随机写累积成批量写，Mesa因此需要外部系统配合，不过，也减少了服务自身的复杂性。</p>

<p>第二，底层文件合并，目的都是为了提升检索效率，但是HBase更多的是做简单（或者说通用）的数据归并动作，将无效数据（比如Delte掉的数据，超过历史版本数量的数据）剔除，减少文件数量等，本质上不改变数据的形态，性能的收益是从检索本身的开销上获得的，而对于Mesa来说，其工作的重点是历史数据的聚合（剔除一行的动作是通过负值Value来实现），本质上是将细粒度数据转化为粗粒度数据的过程（当然，和前面说的，实际上，取决于聚合函数的定义，可能可以达到其它目的），性能的改善更多的是从数据的形态变化上获得的。</p>

<p>第三，Mesa中Version版本的概念也与BigTable／HBase相比较，形式类似，但是整体目的用途，还是不同的。HBase系统中版本，尽管理论上可以认为是行／列以外的又一个存储维度，但实际系统的设计导向，基本就是作为区分数据历史版本使用，有不少其它系统（比如Percolator／Trafodion）借助于HBase的Cell版本功能，构建起MVCC的机制来实现例如跨行跨表原子操作，OLTP等特性。但是在Mesa中，除了构建原子操作，从查询方式（返回所有0-n版本聚合后的数据）和预聚合合并Delta文件等系统整体设计思路的角度来看，其版本的功能指向，更接近时间序列的概念，用来罗列相同Key下的可聚合的细粒度数据，当然，实际应用可能性取决于聚合函数的定义（比如假设有一个聚合函数是LastVersion，那就接近HBase的版本概念了）。</p>

<h3 id="section-3">其他工程优化</h3>

<p>delta 剪枝：类似于key range，bloomfilter等，可以快速数据块中是否含有需要的信息。</p>

<p>Scan to seek： 如果查询条件的过滤字段不是索引组合键的第一个字段，对过滤字段的左侧字段进行枚举检索，尽量减少需要进行范围检索的工作</p>

<p>Resume key：海量数据的返回是分批返回的，返回结果中附带一个Resume key用作标识当前进度，便于后续查询可以在此基础上继续进行</p>

<p>Mesa系统内部的日常数据维护工作可能涉及到大量读写工作，采用了MR作业来分布式的进行，而要保证MR作业的时间可控性，正确的进行分区，避免数据倾斜会是一个需要妥善解决的问题，Mesa采用数据采样的方式，对每个Delta文件同步存储一个采样文件，通过预读采样文件，决定MR任务分区的键值</p>

<p>对于表结构Schema的在线变更，Mesa提供两种方式，一是使用新的schema全量拷贝数据到新的版本上，二是在特定的表结构变更场景中（比如增加字段的这种表结构变更操作），在查询的时候动态判断schema版本，对返回数据进行转换，（增加字段的变更为例，Mesa会为老数据补上默认值），这个过程只需要维持一段特定的时间，因为MESA内的数据经过一段时间会进行Delta增量合并操作，在合并过程中Mesa再对涉及到的历史数据进行格式转换操作。</p>

<p>数据校验采用线上和线下两种方式。线上测试在每个update和query操作中进行，针对每个数据文件和index文件，检查checksum，并且查看row key的排序和范围。线下测试会进行全局的checksum校验，这个checksum依赖row的顺序，对于不同粒度（table，version，index）会有不同的checksum粒度。</p>

<h3 id="lessons-learned">Lessons Learned</h3>

<ul>
  <li>分布式并行化，非中心化的思想横贯其中。</li>
  <li>注意模块化和抽象化，以及分层的设计理念</li>
  <li>减少对应用层的假设，需要设计的更加通用化，对现在和未来应用的假设越少越好</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Column-Stores vs. Row-Stores]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores/"/>
    <updated>2016-07-31T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/31/column-stores-row-stores</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Abadi et al. SIGMOD 2008</p>
</blockquote>

<p>在数据分析领域，例如数据仓库、决策支持、BI应用等，面向列的存储结构通常会比面向行的存储结构表现要好一个数量级以上，因为只需要读取需要的属性，列存储对于只读的查询 IO 更高效。本文否定了用列存储的思想优化行存储的方式，提出对两种存储方式都有效的优化，并探讨列存储真正高效的原因。</p>

<!--more-->

<h3 id="row-vs-column">Row vs. column</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/39462037.jpg" width="500px" /></p>

<p>Row store 更加适合添加和修改一条记录，但读取不必要的数据；Column Store 适合读取相关数据，但写的时候需要多次磁盘IO。所以 Column stores 适合读密集型的数据集。对比如下：</p>

<table border="1" cellspacing="0" cellpadding="0"> <tbody><tr>  <td valign="top" style="background:#5B9BD5;"><p align="left"><strong><span style="color:white;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">行式存储</span></strong></p></td>  <td valign="top" style="background:#5B9BD5;"><p align="center"><strong><span style="color:white;">列式存储</span></strong></p></td> </tr> <tr>  <td valign="top" style="background:#DEEAF6;"><p align="center"><strong>优点</strong></p></td>  <td valign="top" style="background:#DEEAF6;"><p align="left">Ø&nbsp; 数据被保存在一起</p>  <p align="left">Ø&nbsp; INSERT/UPDATE容易</p></td>  <td valign="top" style="background:#DEEAF6;"><p>Ø&nbsp; 查询时只有涉及到的列会被读取</p>  <p>Ø&nbsp; 投影(projection)很高效</p>  <p>Ø&nbsp; 任何列都能作为索引</p></td> </tr> <tr>  <td valign="top"><p align="center"><strong>缺点</strong></p></td>  <td valign="top"><p align="left">Ø&nbsp; 选择(Selection)时即使只涉及某几列，所有数据也都会被读取</p></td>  <td valign="top"><p>Ø&nbsp; 选择完成时，被选择的列要重新组装</p>  <p>Ø&nbsp; INSERT/UPDATE比较麻烦</p></td> </tr></tbody></table>

<p>通常来说它比 Row store 在某些应用中更快的原因有：</p>

<ul>
  <li>只读取需要的列</li>
  <li>更好的缓存有效性</li>
  <li>适合压缩</li>
</ul>

<h3 id="row-oriented-execution">Row-oriented Execution</h3>

<p>首先看下在商用的行存储DBMS中如何实现列数据库：</p>

<ol>
  <li>
    <p>Vertical Partitioning</p>

    <blockquote>
      <p>The most straightforward way to emulate a column-store approach in a row-store is to fully vertically partition each relation.</p>
    </blockquote>

    <p>垂直切割表格形成两列的tuple表（table key, attribute）, 查询的时候直接访问必要的列。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/64076524.jpg" width="450px" /></p>

    <p>遇到的问题： 需要添加 Position 列，浪费磁盘和带宽；Tuple 有额外 Header，例如 PostgreSQL 里有 24 bytes。</p>
  </li>
  <li>
    <p>Index-only Plan</p>

    <blockquote>
      <p>base relations are stored using a standard, row-oriented design, but an additional unclustered B+Tree index is added on every column of every table.</p>
    </blockquote>

    <p>构造在query中所需要用到的所有列的数据块的集合，所以查询的时候根本不需要查询底层的（按行存储的）表。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/2612463.jpg" width="450px" /></p>

    <p>遇到的问题：分开的数据块如果需要全表扫表的话，会很慢。</p>
  </li>
  <li>
    <p>Materialized Views</p>

    <blockquote>
      <p>Create optimal set of MVs for given query workload to provide just the required data, avoid overheads and perform better.</p>
    </blockquote>

    <p>使用query中所需要用到的列的物化视图集合，尽管使用很多空间，但是可以得到更好的效率，典型的空间换时间。</p>

    <p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-31/44662651.jpg" width="300px" /></p>

    <p>遇到的问题：适用性比较窄；需要知道查询的先验知识。</p>
  </li>
</ol>

<h3 id="column-oriented-execution">Column-oriented Execution</h3>

<p>下面来看下面向列存储的一些优化方法：</p>

<ol>
  <li>
    <p>Compression</p>

    <blockquote>
      <p>Low information entropy (high data value locality) leads to High compression ratio.</p>
    </blockquote>

    <p>这个比较重要，有结果显示压缩和不压缩性能上会差别一个数量级，具体的好处是：</p>

    <ul>
      <li>节省磁盘空间</li>
      <li>减少磁盘和网络IO</li>
      <li>如果能在压缩数据上面做计算，那 CPU 消耗也可以降低</li>
    </ul>

    <p>几个注意点：</p>

    <ul>
      <li>注意压缩率和解压效率的权衡，使用轻量级的压缩算法，牺牲一些压缩率。</li>
      <li>使用类似run-length encoding (1,1,1,2,2 -&gt; 1<em>3,2</em>2)，可以直接在压缩数据上做计算。</li>
      <li>特别是对于排好序的数据，压缩效率会更高。</li>
    </ul>
  </li>
  <li>
    <p>Late Materialization</p>

    <p>通常来说，查询结果是要得到一个 entity，而不是一个 column，所以需要对多个列进行 join。比较朴素的做法是，数据按列存储在磁盘上，当一个query需要多个 column 的时候，会从这些属性中构造 tuples，接着进行一些其他操作（select，aggregate，join…）。虽然这样做仍然会优于 row-oriented 的存储，但是会有许多优化空间，使用延迟物化就是一个。</p>

    <p>举一个例子，<code>SELECT R.a FROM R WHERE R.c = 5 AND R.b = 10</code>。首先，<code>R.c</code> 和 <code>R.b</code> 的输出是一个 bit string，这个其实就是中间结果 position lists，对着两个结果进行 <code>bitwise and</code>，最后这个 final position list 提取 <code>R.a</code>。</p>

    <p>带来的好处有：</p>

    <ul>
      <li>非必要的 tuple 构建省略了（selection 和 aggregation带来的）</li>
      <li>可以直接对着压缩数据进行操作</li>
      <li>更好的 cache performance，没有其他属性的数据污染 cache</li>
      <li>可以结合下一个优化点</li>
    </ul>
  </li>
  <li>
    <p>Block Iteration</p>

    <blockquote>
      <p>Operators operate on blocks of tuples at once like batch processing.</p>
    </blockquote>

    <p>对 Block 而不是 Tuple 进行操作，这个同样可以应用在 Row-oriented stores 上，带来的好处有：</p>

    <ul>
      <li>如果列是固定宽度的，则可以变成对数组的操作</li>
      <li>减少 tuple 的 overhead</li>
      <li>有效利用并行化</li>
    </ul>
  </li>
</ol>

<h3 id="section">结论</h3>

<p>Vertical portioning 和 Index only plan 并不能起到很好的效果，Materialized view 表现的最好。</p>

<ul>
  <li>Block processing improves the performance by a factor of 5% to 50%</li>
  <li>Compression improves the performance by almost a factor of two on avg</li>
  <li>Late materialization improves performance by almost a factor of three</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hbase Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/20/hbase/"/>
    <updated>2016-07-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/20/hbase</id>
    <content type="html"><![CDATA[<p>犯懒了，留两篇好文章，基本的东西也都在这文章里面。得空再补补吧。</p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426877.html">深入HBase架构解析（一）</a></p>

<p><a href="http://www.blogjava.net/DLevin/archive/2015/08/22/426950.html">深入HBase架构解析（二）</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Realtime Data Processing at Facebook]]></title>
    <link href="http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook/"/>
    <updated>2016-07-13T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/07/13/realtime-data-processing-at-facebook</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Chen et al. SIGMOD 2016</p>
</blockquote>

<p>论文说明Facebook是如何建立实时系统的。有段话说明batch和stream的关系：</p>

<blockquote>
  <p>Streaming versus batch processing is not an either/or decision. Originally, all data warehouse processing at Facebook was batch processing. We began developing [some of our streaming systems] about five years ago… using a mix of streaming and batch processing can speed up long pipelines by hours. Furthermore, streaming-only systems can be authoritative. We do not need to treat realtime system results as an approximation and batch results as “the truth.”</p>
</blockquote>

<!--more-->

<p>典型的实时系统有Twitter的<code>Storm</code>和<code>Heron</code>、Google的<code>Millwheel</code>，LinkedIn的<code>Samza</code>。Facebook有<code>Puma</code>、<code>Swift</code>、<code>Stylus</code>，在实时系统中他们有以下几方面的考虑：</p>

<ol>
  <li>易用性：处理要求有多复杂，sql是否足够，还是需要通用语言？用户对新应用的编写，测试、调试和部署有多快，如何对应用进行监控？</li>
  <li>性能：延迟，吞吐量，每台机器的以及聚合时候的？</li>
  <li>容错性：容忍错误种类，数据处理和输出时的语义保证，系统如何存储恢复内存状态？</li>
  <li>扩展性：数据分片以及重新分片的并行处理，数据量增加和减小时候系统如何处理，对于老数据的重放？</li>
  <li>正确性：ACID保证？</li>
</ol>

<p>数据处理需求允许秒级的延迟，所以fb中的数据传输是通过一个持久存储的；分离传输和处理带来的好处是易用、容错和可扩展以及一部分正确性保证。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/63234135.jpg" width="500px" /></p>

<ul>
  <li>
    <p>scribe</p>

    <p>Scribe从各种数据源上收集数据，放到一个共享队列上，然后push到后端的中央存储系统上。当中央存储系统出现故障时，scribe可以暂时把日志写到本地文件中，待中央存储系统恢复性能后，scribe把本地日志续传到中央存储系统上。
需要注意的是，各个数据源须通过thrift向scribe传输数据（每条数据记录包含一个category和一个message）。可以在scribe配置用于监听端口的thrift线程数。在后端，scribe可以将不同category的数据存放到不同目录中，以便于进行分别处理。后端的日志存储方式可以是各种各样的store，包括file（文件），buffer（双层存储，一个主储存，一个副存储），network（另一个scribe服务器），bucket（包含多个store，通过hash的将数据存到不同store中），null(忽略数据)，thriftfile（写到一个Thrift TFileTransport文件中）和multi（把数据同时存放到不同store中）</p>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-14/99114695.jpg" width="500px" /></p>

<ul>
  <li>puma 使用支持Java UDF的类sql语言，目的在于快速开发部署</li>
  <li>swift 使用脚本语言支持流式处理，简单的API和checkpointing。</li>
  <li>stylus C++编写，类似Strom的流处理引擎。</li>
  <li>Laser 高吞吐低延迟的内存KV存储，基于RocksDB（基于Google的LevelDB）<a href="https://influxdata.com/blog/benchmarking-leveldb-vs-rocksdb-vs-hyperleveldb-vs-lmdb-performance-for-influxdb/">benchmark</a></li>
  <li>Scuba fast slice-and-dice analysis data store，支ad-hoc query和可视化</li>
  <li>Hive  数据仓库</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-13/55531065.jpg" width="600px" /></p>

<ul>
  <li>Language paradigm
    <ul>
      <li>声明语言，SQL，编写作为最简单和快速，群众基础好。缺点是表达能力不够。</li>
      <li>函数语言，使用预制的算子串联处理过程，简单并且对算子的组合增加表达能力。</li>
      <li>过程语言，C++，JAVA，Python，灵活，性能最高，对数据结构和执行过程有完全掌控力，但是要求也最高。S4/Storm/Heron/Samza。</li>
    </ul>
  </li>
  <li>Data transfer
    <ul>
      <li>直接传输，RPC或者内存消息队列。延迟少，毫秒级。</li>
      <li>Broker，另外一个broker进程处理数据传输，增加overhead但是扩展性好。支持多对多的传输，支持反压。典型的是Heron。</li>
      <li>持久存储，persistent message bus。多路复用，输入输出不同的速度，不同的时间点，重复读。</li>
    </ul>

    <p>邻近节点处于DAG中，两种依赖关系，窄依赖和宽依赖，实现在数据传输中。</p>
  </li>
  <li>
    <p>Processing semantics</p>

    <p>stream processor有三种行为：</p>

    <ul>
      <li>处理input event，反序列化输入event，查询外部系统，更新内存状态，并没有side-effect</li>
      <li>输出产生，根据input event和内存状态，向下游产生输出。可以在检查点之前或之后。</li>
      <li>保存检查点，内存状态、input stream offset、output value</li>
    </ul>

    <p>根据这些行为的实现方式，有两种语义信息：</p>

    <ul>
      <li>状态语义，input event是at-least once, at-most once, exactly</li>
      <li>输出语义，output value….</li>
    </ul>

    <p>无状态的处理器只有输出语义，有状态的有两种语义。状态语义只依赖于保存offset和内存状态的顺序。</p>

    <ul>
      <li>at-least-once，先内存状态后offset</li>
      <li>at-most-once，先offset，再内存状态</li>
      <li>exactly，原子的</li>
    </ul>

    <p>输出语义依赖数据保存到checkpint的顺序：</p>

    <ul>
      <li>at-least-once，先emit output再offset和内存状态</li>
      <li>at-most-once，先offset和内存状态，再emit output</li>
      <li>exactly，原子的事务性</li>
    </ul>

    <p>可以做一些side-effect-free的操作加速吞吐量，例如保存checkpoint的同时进行反序列化。</p>
  </li>
  <li>
    <p>State-saving mechanism </p>

    <ul>
      <li>Replication</li>
      <li>local database persistence</li>
      <li>remote database persistence</li>
      <li>upstream backup</li>
      <li>global consistent snapshot</li>
    </ul>
  </li>
  <li>
    <p>Backfill processing</p>

    <ul>
      <li>stream only.</li>
      <li>保存两个系统，一个用于batch，一个用于流处理</li>
      <li>开发流处理系统可以运行在batch环境中，spark streaming 和 flink. </li>
    </ul>
  </li>
</ul>

<h3 id="lessons-learned">Lessons learned</h3>

<ol>
  <li>拥有多个不同的实时处理系统是有好处的. “Writing a simple application lets our users deploy something quickly and prove its value first, then invest the time in building a more complex and robust application.”</li>
  <li>回放对于debug来说很重要.</li>
  <li>“Writing a new streaming application requires more than writing the application code. The ease or hassle of deploying and maintaining the application is equally important. Laser and Puma apps are deployed as a service. Stylus apps are owned by the individual teams who write them, but we provide a standard framework for monitoring them.” Making Puma apps self-service was key to scaling to the hundreds of data pipelines using Puma today.</li>
  <li>使用告警来检查应用的处理速度低于输入速度，未来考虑自动缩放。</li>
  <li>Streaming versus batch processing does not need to be an either/or decision.</li>
  <li>易用性很重要.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Funny Facts about Rating]]></title>
    <link href="http://billowkiller.github.io/blog/2016/04/09/rating/"/>
    <updated>2016-04-09T16:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/04/09/rating</id>
    <content type="html"><![CDATA[<p>本文分析Wilson score confidence interval，它在Reddit评论评分中的应用，另外分析了Hacker News、Reddit的评分算法。先来看下错误的评分。</p>

<p>Case 1: 网站的内容排名，得分高的排前面，得分低的排后面。如何计算得分。</p>

<p><strong>Solution 1:  Score = (Positive ratings) - (Negative ratings)</strong></p>

<u>未考虑内容的受欢迎程度。</u>

<!--more-->

<blockquote>
  <p>Suppose one item has 600 positive ratings and 400 negative ratings: 60% positive. Suppose item two has 5,500 positive ratings and 4,500 negative ratings: 55% positive. This algorithm puts item two above item one.</p>
</blockquote>

<p><strong>Solution 2: Score = Average rating = (Positive ratings) / (Total ratings)</strong></p>

<u>数据较多的时候比较适合，但是数据量少的时候会有比较大的Bias。</u>

<blockquote>
  <p>Average rating works fine if you always have a ton of ratings, but suppose item 1 has 2 positive ratings and 0 negative ratings. Suppose item 2 has 100 positive ratings and 1 negative rating. This algorithm puts item two below item one.</p>
</blockquote>

<p><strong>Correct Solution: Score = Lower bound of <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval">Wilson score confidence interval</a> for a Bernoulli parameter</strong></p>

<u>We need to balance the proportion of positive ratings with the uncertainty of a small number of observations. </u>

<p><img src="http://www.evanmiller.org/images/rating-equation.png" alt="" /></p>

<p>Here $\hat{p}$ is the observed fraction of positive ratings, $z_{\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution, and $n$ is the total number of ratings. </p>

<p>上式适合于二元评分机制，并不适用于例如5-star评分。</p>

<p>Wilson score confidence interval除了排序问题，还可以应用在如下地方：</p>

<ul>
  <li>Detect spam/abuse: What percentage of people who see this item will mark it as spam?</li>
  <li>Create a “best of” list: What percentage of people who see this item will mark it as “best of”?</li>
  <li>Create a “Most emailed” list: What percentage of people who see this page will click “Email”?</li>
</ul>

<h3 id="hacker-news-ranking-algorithm">Hacker News ranking algorithm</h3>

<p>Hacker News由Lisp的一种方言Arc实现，代码现在是开源的。它实现的内容得分公式为：</p>

<script type="math/tex; mode=display">Score = \frac{P-1}{(T+2)^G}</script>

<ul>
  <li>P = points of an item (and -1 is to negate submitters vote)</li>
  <li>T = time since submission (in hours)</li>
  <li>G = Gravity, defaults to 1.8</li>
</ul>

<p>Gravity和Time对内容得分有很大影响：</p>

<ul>
  <li>T增加导致得分降低，这是新闻老化的效果</li>
  <li>当Gravity增加的时候，上述降低会来的更快</li>
</ul>

<p>查看Time和Gravity的影响，在<a href="http://www.wolframalpha.com">Wolfram Alpha</a>的输入框中输入：</p>

<pre><code>plot(
    (30 - 1) / (t + 2)^1.8, 
    (60 - 1) / (t + 2)^1.8,
    (200 - 1) / (t + 2)^1.8
) where t=0..24
</code></pre>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/75012257.jpg" alt="" /></p>

<pre><code>plot(
    (p - 1) / (t + 2)^1.8, 
    (p - 1) / (t + 2)^0.5,
    (p - 1) / (t + 2)^2.0
) where t=0..24, p=10
</code></pre>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/43469961.jpg" alt="" /></p>

<h3 id="reddit-ranking-algorithm">Reddit ranking algorithm</h3>

<p>在Hacker News中新闻和评论都是采用同样的评分算法，而在Reddit中，它们采用不同的算法。</p>

<p>Reddit 同样是开源的，并且使用Python实现，为了加速计算，评分算法使用 Pyrex 实现。先来看下新闻的评分算法，hot algorithm：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/36257254.jpg" alt="" /></p>

<ul>
  <li>同样，submission time对评分的影响很大，人们总是喜欢新鲜的事情</li>
  <li>和Hacker News不同，新闻评分并不会随着时间流逝而降低，虽然它会比较新的新闻得分低。</li>
</ul>

<p>对于同样的votes，但是submission time不同的新闻，它们的得分可以参考：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/83965192.jpg" alt="" /></p>

<p>公式中的logarithm表示，最开始的votes拥有比较大的权重：开始的10 upvotes和接下的100 upvotes具有相同的权重，接下是 1000 upvotes。这就是upvotes的平滑效果。</p>

<p>比较有意思的是Reddit中的downvotes，它认为具有比较多upvotes和downvotes的新闻代表争议，它们的得分应该会比只有upvotes的新闻得分低。看看downvotes的影响：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/95155391.jpg" alt="" /></p>

<p>下面看下Reddit的评论的评分算法：</p>

<p>Reddit认为hot algorithm并不适用于comment，原因如下：</p>

<ul>
  <li>In a comment system you want to rank the best comments highest regardless of their submission time</li>
</ul>

<p>Reddit用到我们上面提到过的Wilson score interval，由这个interval得到the confidence sort，解释如下：</p>

<ul>
  <li>The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone</li>
  <li>The confidence sort gives a comment a provisional ranking that it is 85% sure it will get to</li>
  <li>The more votes, the closer the 85% confidence score gets to the actual score</li>
  <li>Wilson’s interval has good properties for a small number of trials and/or an extreme probability</li>
</ul>

<blockquote>
  <p>If a comment has one upvote and zero downvotes, it has a 100% upvote rate, but since there’s not very much data, the system will keep it near the bottom. But if it has 10 upvotes and only 1 downvote, the system might have enough confidence to place it above something with 40 upvotes and 20 downvotes — figuring that by the time it’s also gotten 40 upvotes, it’s almost certain it will have fewer than 20 downvotes. And the best part is that if it’s wrong (which it is 15% of the time), it will quickly get more data, since the comment with less data is near the top.</p>
</blockquote>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-5-26/41802353.jpg" alt="" /></p>

<h3 id="references">REFERENCES</h3>

<p><a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html">How Not To Sort By Average Rating</a></p>

<p><a href="https://medium.com/hacking-and-gonzo/how-hacker-news-ranking-algorithm-works-1d9b0cf2c08d#.utg0mohjf">How Hacker News ranking algorithm works</a></p>

<p><a href="https://medium.com/hacking-and-gonzo/how-reddit-ranking-algorithms-work-ef111e33d0d9#.ub7osc36n">How Reddit ranking algorithms work</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2016/04/06/kafka/"/>
    <updated>2016-04-06T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/04/06/kafka</id>
    <content type="html"><![CDATA[<p>Kafka最早是由LinkedIn开发的一个分布式发布-订阅消息系统，现在已经是Apache的一个开源项目。它具有以下的一些特点：</p>

<ul>
  <li>作为分布式系统，很容易 scale out</li>
  <li>消息以时间复杂度为O(1)的方式持久化到磁盘，支持离线消费和实时消费</li>
  <li>发布和订阅都支持高吞吐量，单机支持每秒100K条以上消息的传输</li>
  <li>支持多个订阅端，并且可以在异常情况下自动对这些消费者进行负载均衡</li>
</ul>

<p>消息系统的好处包括：</p>

<ul>
  <li>解耦生产者和消费者</li>
  <li>持久化直到消息已经被完全处理</li>
  <li>扩展性</li>
  <li>灵活性 &amp; 峰值处理能力</li>
  <li>可恢复性，系统的一部分组件失效时，不会影响到整个系统。</li>
  <li>顺序保证</li>
  <li>缓冲，有助于控制和优化数据流经过系统的速度。</li>
  <li>异步通信</li>
</ul>

<!--more-->

<h2 id="section">名词解释</h2>

<ul>
  <li>
    <p>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</p>
  </li>
  <li>
    <p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>
  </li>
  <li>
    <p>Partition：Parition是物理上的概念，每个Topic包含一个或多个Partition.</p>
  </li>
  <li>
    <p>Producer：负责发布消息到Kafka broker</p>
  </li>
  <li>
    <p>Consumer：消息消费者，向Kafka broker读取消息的客户端。</p>
  </li>
  <li>
    <p>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>
  </li>
</ul>

<h2 id="section-1">架构图</h2>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310020.png" width="500px" /></p>

<p>如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p>

<p><img src="http://cdn.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-1/zh/resources/0310025.png" width="500px" /></p>

<p>上图示意消费者和生产者的工作方式。同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>

<p><img src="http://sookocheff.com/img/kafka/kafka-in-a-nutshell/log-anatomy.png" width="500px" /></p>

<p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。每条消息都被append到某个Partition中，具体存储到哪一个Partition是根据Partition机制。如果Partition机制比较合理，不同的消息可以并行写入不同broker的不同Partition里，能极大的提高了吞吐率。另因为磁盘限制，Kafka提供两种策略删除旧数据：一是基于时间，二是基于Partition文件大小。</p>

<h3 id="kafka-delivery-guarantee">Kafka delivery guarantee</h3>

<p>Producer向broker发送消息时，默认情况下一条消息从Producer到broker是确保了 <code>At least once</code>，但如果设置Producer为异步发送则实现 <code>At most once</code>。<code>Exactly once</code>还未实现（生成一种消息主键，幂等重试）。</p>

<p>Consumer在从broker读取消息后，可以选择<code>autocommit</code>或<code>手动commit</code>。区别在于一个是读完消息先commit再处理消息，一个是读完消息先处理再commit。前者实现 <code>At most once</code>，后者实现 <code>At least once</code>，但如果消息的处理有幂等性，则可以理解为<code>Exactly once</code>。如果要做到严格<code>Exactly once</code>，则让offset和操作输入存在同一个地方，<strong>保证数据的输出和offset的更新要么都完成，要么都不完成</strong>，参考Spark Kafka的<code>DirectAPI</code>的实现以及<code>Druid</code>。</p>

<p>Kafka允许的<code>Exactly once</code>语义其实和它的特性有很大关系：
* 每条消息有序和不可变的放在partition中，同时分配一个递增的offset。这样通过&lt;partition, offset&gt;就可以确定一条消息，并且它的前辈和后辈都是不变的。
* 消费者拉数据，从而由消费者自身进行流控。
* 消费者可以根据&lt;partition, offset&gt;寻找消息，允许message rewind，并且根据消息的metadata可以保证消息接收的不重不丢。</p>

<h2 id="high-available">High Available</h2>

<p>Kafka的HA包括Data Replication和Leader Election两方面。</p>

<h3 id="data-replication">Data Replication</h3>

<p><img src="http://cdn4.infoqstatic.com/statics_s2_20160405-0343u1/resource/articles/kafka-analysis-part-2/zh/resources/0416000.png" width="500px" /></p>

<p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。Kafka分配Replica的算法如下：</p>

<ol>
  <li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>
  <li>将第i个Partition分配到第（i mod n）个Broker上</li>
  <li>将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</li>
</ol>

<p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR(in-sync replica)中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW(high watermark)并且向Producer发送ACK。HW会从leader持续发送到follower并被保存到每个broker的磁盘中。</p>

<p>对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p>

<p>Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。</p>

<h3 id="leader-election">Leader Election</h3>

<p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。对比Majority Vote则需要2f+1个Replica。</p>

<p>Kafka中，如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除，包括两种情况：长时间未向leader发送fetch request，消息lag超过阈值。
为了防止ISR里面的慢节点，Producer选择是否被commit阻塞。</p>

<p>选举时候，Kafka会在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。这种方式改善每个follower都使用zk watch的方法进行选举的问题：</p>

<ul>
  <li>brain split</li>
  <li>herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</li>
  <li>ZooKeeper负载过重 </li>
</ul>

<h2 id="kafka-">Kafka 网络模型</h2>

<p>Kafka使用的网络模型是典型的reactor模式，一个acceptor处理新来的连接请求，分配给N个processor处理，每个processor都有selector从socket中读取数据，生成request对象放到requestChannel中。requestChannel包含一个requestQueue和一个responseQueues，requestQueue是一个blocking queue，它的大小为<code>queued.max.requests</code>；responseQueues 包含N个blocking queue，对应每个processor。Kafka会有M个Handler threads用于处理responseQueues中的request，并且生成response放到对应的response队列中，处理过程如下：KafkaRequestHandler循环从RequestChannel中取Request并交给kafka.server.KafkaApis处理具体的业务逻辑。。</p>

<p>这里面涉及的数目在配置文件中都有体现，上述的每个acceptor包括processor在Kafka中称为NIO socket server，数量在<code>listeners</code>中定义，例如<code>PLAINTEXT://myhost:9092, SSL://:9091 </code>。N 取值 <code>background.threads</code>，M 取值 <code>num.io.threads</code>。</p>

<p>在 NIO socket server 中会给每个processor的responseQueue都注册一个ResponseListener，一旦有Response产生就会通知对应的processor发送Response到客户端。</p>

<h2 id="kafka-clients-operations">Kafka Clients’ Operations</h2>

<p>这一章节介绍一个Kafka client对于Kafka Resources可能的操作类型。</p>

<p>Operation包括以下几种：Read, Write, Create, Delete, Alter, Describe, ClusterAction。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/80246194.jpg" width="450px" /></p>

<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-4+-+Command+line+and+centralized+administrative+operations">KIP-4</a></p>

<h3 id="createdelete">Create/Delete</h3>

<p>Create/Delete就是创建和删除Topics。</p>

<p>具体过程如下：</p>

<ol>
  <li>broker发送<code>TopicMetadataRequest</code>到controller。</li>
  <li>Controller在ZooKeeper的/brokers/topics节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。</li>
  <li>对于删除Topic操作，Topic工具会将该Topic名字存于/admin/delete_topics。若delete.topic.enable为true，则Controller注册在/admin/delete_topics上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest，若为false则Controller不会在/admin/delete_topics上注册Watch，也就不会对该事件作出反应。</li>
  <li>对于创建Topic操作，Controller从/brokers/ids读取当前所有可用的Broker列表，对于set_p中的每一个Partition：
    <ul>
      <li>从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）</li>
      <li>将新的Leader和ISR写入/brokers/topics/[topic]/partitions/[partition]</li>
    </ul>
  </li>
  <li>直接通过RPC向相关的Broker发送LeaderAndISRRequest。</li>
</ol>

<p>创建Topic顺序图如下所示。</p>

<p><img src="http://cdn4.infoqstatic.com/statics_s1_20160405-0343u1/resource/articles/kafka-analysis-part-3/zh/resources/0606003.png" width="500px" /></p>

<p>有两点说明：</p>

<ul>
  <li>对于<code>auto.create.topics.enable=false</code>的Kafka，如果对未存在的topic进行produce，则会导致producer <code>org.apache.kafka.common.errors.TimeoutException</code>错误。</li>
  <li>除了使用broker进行创建Topic，还可以通过Kafka的AdminUtils直接指定zk、topic、partitions，replicationFactor，把相关信息写入zk来创建Topic。</li>
</ul>

<h3 id="alterdescribe">Alter/Describe</h3>

<p>alter/describe 是对配置修改或查看的操作，包括Topic和Client两个部分。改变方法也是通过Kafka的AdminUtils和ConfigCommand，指定zk、topic或要修改的properties。</p>

<h3 id="clusteraction">ClusterAction</h3>

<p>包括LeaderAndIsrRequest，StopReplicaRequest，UpdateMetadataRequest，ControlledShutdownRequest</p>

<h3 id="read">Read</h3>

<p>consumer 订阅Topic数据。consumer根据consumer group的分配算法如下</p>

<pre><code>rebalance process for consumer C_i in group G
For each topic T that C_i subscribes to {
 remove partitions owned by C_i from the ownership registry
 read the broker and the consumer registries from Zookeeper
 compute P_T = partitions available in all brokers under topic T
 compute C_T = all consumers in G that subscribe to topic T
 sort P_T and C_T
 let j be the index position of C_i in C_T and let N = |P_T|/|C_T|
 assign partitions from j*N to (j+1)*N - 1 in P_T to consumer C_i
 for each assigned partition p {
 set the owner of p to C_i in the ownership registry
 let O_p = the offset of partition p stored in the offset registry
 invoke a thread to pull data in partition p from offset O_p
 }
}
</code></pre>

<p>当订阅topic的时候，kafka会注册一个<code>ConsumerRebalanceListener</code>，当发生以下任何一个事件的时候，会引发上述的rebalance算法：</p>

<ul>
  <li>Number of partitions change for any of the subscribed list of topics</li>
  <li>Topic is created or deleted</li>
  <li>An existing member of the consumer group dies</li>
  <li>A new member is added to an existing consumer group via the join API</li>
</ul>

<p>Kafka的Consumer API可以透明地处理上述情况，另外还包括server的fail，partition的聚合。</p>

<p>读取数据发生在<code>poll</code>函数中，每次调用时，consumer都会使用上次消费的offset作为这次的起始offset，并且顺序的读取记录。当然上次消费的offset也可以由<code>seek</code>函数直接指定。</p>

<p>读取数据在Kafka内部是一个FetchRequest，处理的过程为
* authorize
* FetchResponse
* recordAndMaybeThrottle(quota控制)</p>

<h3 id="write">Write</h3>

<p>producer 发布数据的动作。produce的api接口可以选择同步或异步，默认情况下<code>send</code>接口是异步的，它将数据放到缓冲区后就立即返回，等到数据到达一定带下后再发送出去，节约IO开销。可以直接在send()后调用get()，这样可以达到同步的效果。另外一个重要的参数是acks，用来设置produce 请求完成的标准，也就是数据要写到几个broker中才算是完成。</p>

<p>发布数据在Kafka内部是一个ProducerRequest，处理过程为：</p>

<ul>
  <li>authorize</li>
  <li>produceResponse</li>
  <li>recordAndMaybeThrottle(quota控制)</li>
</ul>

<h3 id="partition-and-key">Partition and Key</h3>

<p>这里说下partition和key的关系。注意到，在kafka的api中会有一个key的概念，而实际上kafka只是一个消息的订阅和发布系统，和key应该扯不上一点关系，那么这个key有什么用呢，不用key会有什么影响。</p>

<p>看下没有指定key或者key为null的时候kafka是怎么处理的。首先kafka会随机选择一个partition，然后在一个默认的时间（10min）内所有的数据都会写到这个partition内。这会造成数据不均衡的分布在各个partition中。这时可以通过减少metadata refresh interval 缩短这个默认时间来减轻数据不均衡的现象。</p>

<p>不过更实际的还是指定一个key，因为kafka默认的是使用hashing-based partitioner，可能还会造成数据不均衡。这时候就需要使用自定义的partition，并指定<code>partitioner.class</code>。</p>

<h2 id="authorize">Authorize</h2>

<h3 id="authorizer">Authorizer接口</h3>

<p>Kafka带有Authorizer接口，这个是所有实现授权的插件都必须要实现的接口。启动的时候会读<code>authorizer.class</code>配置，<code>authorizer.class</code>就是实现授权的具体类。</p>

<p>Kafka的授权逻辑是<code>Principal P is [Allowed/Denied] Operation O From Host H On Resource R</code>，P是用户，O是上文提到的各种Operation，Host就是client地址，R是Kafka资源，包括cluster、topic、consumer-group。</p>

<p>Kafka本身自带一个<code>SimpleAclAuthorizer</code>，用它来实现一些简单资源的访问，例如</p>

<blockquote>
  <p>Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from IP 198.51.100.0 and IP 198.51.100.1</p>
</blockquote>

<h3 id="ranger">Ranger</h3>

<p>Ranger的Kafka plugin也是实现了Authorizer接口。Ranger的实现机制简单的介绍下，Ranger整体分为Admin和Plugin：</p>

<ol>
  <li>Ranger Plugin运行在服务进程内，在Kafka中，Ranger plugin代码就运行在broker内。</li>
  <li>Policy通过Ranger Admin存储在database中，plugin轮询地向admin请求最新的policy；policy存储在本地的一个文件中。</li>
  <li>在service请求到来的时候，ranger plugin中的policy engine会evaluate request，判断是否合法。</li>
</ol>

<p>Ranger的policy engine分为role based和tag based，kafka中使用的是tag based，evaluae的流程图如下：</p>

<p><img src="https://cwiki.apache.org/confluence/download/attachments/61322361/Ranger-Policy-Evaluation-Flow-with-Tags.png?version=2&amp;modificationDate=1444869949000&amp;api=v2" width="600px" /></p>

<p>Ranger Kafka 目前支持的功能还是比较少的，如下图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-4-12/11458053.jpg" width="600px" /></p>

<p>这些功能在Kafka自带的<code>SimpleAclAuthorizer</code>都是可以实现的。</p>

<h2 id="saslkerberos-and-ssl-implementation">Sasl/Kerberos and SSL implementation</h2>

<p>sasl 是broker的认证机制，ssl是数据传输的加密和认证机制。从协议的角度来说，kafka支持以下四种：</p>

<ul>
  <li>
    <p>PLAINTEXT (non-authenticated, non-encrypted)</p>

    <p>This channel will provide exact behavior for communication channels as previous releases</p>
  </li>
  <li>
    <p>SSL</p>

    <p>SSL  implementation. Authenticated principal in the session will be from the certificate presented or the peer host. </p>
  </li>
  <li>
    <p>SASL+PLAINTEXT</p>

    <p>SASL authentication will be used over plaintext channel. Once the sasl authentication established between client and server . Session will have client’s principal as authenticated user. There won’t be any wire encryption in this case as all the channel communication will be over plain text .</p>
  </li>
  <li>
    <p>SASL+SSL</p>

    <p>SSL will be established initially and  SASL authentication will be done over SSL. Once SASL authentication is established users principal will be used as authenticated user .  This option is useful if users want to use SASL authentication ( for example kerberos ) with wire encryption.</p>
  </li>
</ul>

<p>实现SSL需要做如下配置：</p>

<ol>
  <li>Generate SSL key and certificate for each Kafka broker
    <ul>
      <li>Ensure that common name (CN) matches exactly with the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not the malicious one.</li>
    </ul>
  </li>
  <li>Creating your own CA</li>
  <li>Signing the certificate</li>
  <li>Configuring Kafka Brokers</li>
  <li>Configuring Kafka Clients
    <ul>
      <li>需要将生成的<code>kafka.client.truststore.jks</code>拷贝到client</li>
      <li>如果进行双向认证则还需要生成和配置<code>kafka.client.keystore.jks</code></li>
    </ul>
  </li>
</ol>

<p>实现SASL需要：</p>

<ol>
  <li>Kerberos
    <ul>
      <li>客户端需要安装 kerberos client</li>
    </ul>
  </li>
  <li>Create Kerberos Principals
    <ul>
      <li>需要对应的用户principal</li>
    </ul>
  </li>
  <li>Make sure all hosts can be reachable using hostnames</li>
</ol>

<p>具体过程参考<a href="http://kafka.apache.org/documentation.html#security">http://kafka.apache.org/documentation.html#security</a></p>

<p><strong>zookeeper安全性</strong></p>

<ol>
  <li>不开通ibgw（端口），bcc无法直接访问</li>
  <li>zookeeper限制ip段，</li>
  <li>增加zookeeper authentication</li>
</ol>

<p>对每个resource都应该能够实现管理和控制。</p>

<h2 id="references">REFERENCES:</h2>

<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin">https://cwiki.apache.org/confluence/display/RANGER/Kafka+Plugin</a></p>

<p><a href="https://kafka.apache.org/090/configuration.html">https://kafka.apache.org/090/configuration.html</a></p>

<p><a href="https://kafka.apache.org/090/ops.html">https://kafka.apache.org/090/ops.html</a></p>

<p><a href="https://kafka.apache.org/090/security.html">https://kafka.apache.org/090/security.html</a></p>

<p><a href="http://kafka.apache.org/documentation.html">http://kafka.apache.org/documentation.html</a></p>

<p><a href="http://people.csail.mit.edu/matei/courses/2015/6.S897/readings/kafka.pdf">Kafka: A distributed messaging system for log processing</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Table Join Semantics]]></title>
    <link href="http://billowkiller.github.io/blog/2016/03/19/table-join/"/>
    <updated>2016-03-19T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/03/19/table-join</id>
    <content type="html"><![CDATA[<p>这篇文章主要是记录下各种不同 <code>join</code> 的语义。</p>

<p>在最高层面上，<code>join</code> 分为三种：</p>

<ul>
  <li>INNER</li>
  <li>OUTER</li>
  <li>CROSS</li>
</ul>

<!--more-->

<hr />

<ol>
  <li>
    <p><code>INNER JOIN</code> - fetches data if present in both the tables.</p>
  </li>
  <li>
    <p><code>OUTER JOIN</code> are of 3 types:</p>

    <ul>
      <li><code>LEFT OUTER JOIN</code> - fetches data if present in the left table.</li>
      <li><code>RIGHT OUTER JOIN</code> - fetches data if present in the right table.</li>
      <li><code>FULL OUTER JOIN</code> - fetches data if present in either of the two tables.</li>
    </ul>
  </li>
  <li>
    <p><code>CROSS JOIN</code>, as the name suggests, does [n X m] that joins everything to everything. Similar to scenario where we simply lists the tables for joining (in the FROM clause of the SELECT statement), using commas to separate them.</p>
  </li>
</ol>

<p>重点关注前两种，因为在语法中它们包含 <code>join</code> 这个关键词。语法如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&lt;join_type&gt; ::= 
</span><span class="line">    [ { INNER | { { LEFT | RIGHT | FULL } [ OUTER ] } } [ &lt;join_hint&gt; ] ]
</span><span class="line">    JOIN</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>OUTER</code>是一个可选词，有没有对于语义并没有什么区别。解释下上面的语法，可以看到以下的用法是等价的：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">A LEFT JOIN B            A LEFT OUTER JOIN B
</span><span class="line">A RIGHT JOIN B           A RIGHT OUTER JOIN B
</span><span class="line">A FULL JOIN B            A FULL OUTER JOIN B
</span><span class="line">A INNER JOIN B           A JOIN B</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><img src="http://i.stack.imgur.com/qje6o.png" width="500px" /></p>

<p>结果可以看到是这样的：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Table A | Table B     Table A | Table B      Table A | Table B      Table A | Table B
</span><span class="line">   1    |   5            1    |   1             1    |   1             1    |   1
</span><span class="line">   2    |   1            2    |   2             2    |   2             2    |   2
</span><span class="line">   3    |   6            3    |  null           3    |  null           -    |   -
</span><span class="line">   4    |   2            4    |  null           4    |  null           -    |   -
</span><span class="line">                        null  |   5             -    |   -            null  |   5
</span><span class="line">                        null  |   6             -    |   -            null  |   6
</span><span class="line">
</span><span class="line">                      OUTER JOIN (FULL)     LEFT OUTER (partial)   RIGHT OUTER (partial)</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/01/spark-tunning/"/>
    <updated>2016-02-01T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/01/spark-tunning</id>
    <content type="html"><![CDATA[<p>记录一些spark的调优参数。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-1/51172915.jpg" width="500px" /></p>

<!--more-->

<ol>
  <li>
    <p>一个集群有N个节点，每个拥有a cores，mGB的memory</p>

    <ul>
      <li>num-executor N*3-1， 每个节点包含3个executor，AM的那个节点两个</li>
      <li>executor-cores min(5, a-1)</li>
      <li>executor-memory   (m-1)/3 *  (1-spark.yarn.executor.memoryOverhead)，默认为0.07。</li>
    </ul>
  </li>
  <li>
    <p>并行化调优</p>

    <pre><code> 一个stage中task的数量，也就是处理数据的并行度。通常的配置为分配的core数目的2-3倍。task的数量过少会影响：
	
 * 在聚合操作上会产生更大的内存压力
 * 垃圾回收
 * 当记录在内存中无法装载时会spill to disk，磁盘IO和排序。

 解决办法：
	
 * 配置INputFormat产生更多的splits。
 * 用更小的block size将输入数据写到HDFS上。
 * repartition transformation, triggering a shuffle: `var rdd2 = rdd2.reduceByKey(_ + _, numPartitions = X)`
</code></pre>

    <ul>
      <li>Spark的RDD的partition个数创建task的个数是对应的</li>
      <li>Partition的个数在spark的RDD中由block的个数决定的</li>
      <li>尽可能地增加task的数量使得每个task的数据可以装入分配的内存中。
 Spark能够非常有效的支持段时间任务（例如200ms)，因为他会对所有的任务复用JVM，这样能减小任务启动的消耗。所以，可以放心的使任务的并行度远大于集群的CPU核数。</li>
    </ul>

    <p>spark.default.parallelism: 控制Spark中的分布式shuffle过程默认使用的task数量，默认为8个。如果不做调整，数据量大时，就容易运行时间很长，甚至是出Exception，因为8个task无法handle那么多的数据。</p>

    <pre><code>     spark.default.parallelism 300
</code></pre>
  </li>
  <li>
    <p>减少Java数据结构的消耗，内存中存储未序列化的java对象，硬盘和网络用序列化的数据。</p>

    <ul>
      <li>使用org.apache.spark.serializer.KryoSerializer</li>
      <li>fastutil库为原始数据类型提供方便的集合类，且兼容Java标准类库。</li>
      <li>内存少于32G，设置JVM参数-XX:+UseCompressedOops以便将8字节指针修改成4字节，设置JVM参数-XX:+UseCompressedStrings以便采用8比特来编码每一个ASCII字符。</li>
    </ul>
  </li>
  <li>
    <p>driver默认内存改成2G（原先1G）；</p>
  </li>
  <li>
    <p>默认开启预测执行，缓解慢节点带来的计算任务慢的问题：</p>

 		spark.speculation true
 		spark.speculation.quantile 0.75
 		spark.speculation.multiplier 1.5
  </li>
  <li>
    <p>调整默认GC策略至G1 GC </p>

    <p>需要测试一些数据来看一下环境里最适合的GC策略，测试结果显示G1没有提升</p>

 		CMS vs parallel GC vs G1 GC
 		G1 GC: InitiatingHeapOccupancyPercent、 ConcGCThreads
  </li>
  <li>
    <p>timeout参数优化</p>

 		spark.core.connection.ack.wait.timeout 
 		spark.core.connection.auth.wait.timeout
 		spark.akka.timeout 
 		spark.akka.askTimeout 
 		spark.shuffle.io.connectionTimeout 
  </li>
  <li>
    <p>frameSize 优化（控制Spark中通信消息的最大容量如 task 的输出结果，默认为10M）</p>

 		spark.akka.frameSize 100
  </li>
  <li>
    <p>codec 优化（Cache和Shuffle数据压缩所采用的算法）</p>

 		spark.io.compression.codec org.apache.spark.io.LZ4CompressionCodec
  </li>
  <li>
    <p>gc log打印</p>

    <pre><code>	-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/var/logs/spark/spark_gc.log   
</code></pre>
  </li>
  <li>
    <p>heap dump</p>
  </li>
</ol>

 			-xx:+HeapDumpOnOutOfMemoryError

<ol>
  <li>调整permsize</li>
</ol>

 			–driver-java-options “-XX:MaxPermSize=512m”

<ol>
  <li>
    <p>native lzo</p>
  </li>
  <li>
    <p>减小数据结构内存</p>

    <p>内存少于32G，设置JVM参数-XX:+UseCompressedOops以便将8字节指针修改成4字节，设置JVM参数-XX:+UseCompressedStrings以便采用8比特来编码每一个ASCII字符</p>
  </li>
</ol>

<h3 id="spark-terasort-by-databricks">Spark Terasort by Databricks</h3>

<p>Spark sorted the same data <strong>3X faster using 10X fewer machines</strong>. All the sorting took place on disk (HDFS), without using Spark’s in-memory cache. </p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-1/9096505.jpg" width="600px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Extract-Transform-Load Application Scenarios]]></title>
    <link href="http://billowkiller.github.io/blog/2016/01/13/etl/"/>
    <updated>2016-01-13T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/01/13/etl</id>
    <content type="html"><![CDATA[<h1 id="etl">实时流ETL应用场景</h1>

<p>现有的企业级数据量在不断增大，用户也在寻求大数据解决方案来处理这些日益增长的数据。那么什么是大数据处理的架构呢。Cloudera总结的很好，大数据架构是建立在一系列开发可靠、可扩张、完整的自动化data pipeline上，下面的一张图给了很好的解释：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/11725472.jpg" width="500px" /></p>

<!--more-->

<p>data pipeline目的在于获取数据并能够发掘其中的价值。数据工程师决定数据从何处来，如何进入数据处理层，以及如何处理，如何存储和进一步展示。当然还需要包括必不可少的集群设计、系统调优等。上图中后端的分析工具通常为BI展示或其他分析工具，中间的处理通常由Spark、Hadoop进行，前端的数据获取包括批量和实时的两种。</p>

<p>在BMR中，我们已经为您处理了底层的集群设计、系统调优，打通数据交互层等工作，您只需要专注于如何在业务上挖掘数据潜在的价值即可。</p>

<h2 id="section">应用场景举例</h2>

<p>公司的IDMapping接入层每天的PV达到上亿，每天产生的日志量达到100GB，日志中的信息包括用户的访问IP、访问时间、响应时间、用户请求、应答内容等。IDMapping由10台nginx服务器构成、分别部署在不同的服务器上。其中的日志格式如下：</p>

<p>数据格式如下：</p>

<pre><code>$remote_addr - [$time_local] "$request" $status $body_bytes_sent "$http_referer"  $http_cookie" $remote_user "$http_user_agent" $request_time  $host $msec
</code></pre>

<p>下面是一条具体日志：</p>

<pre><code>10.81.78.220 - [04/Oct/2015:21:31:22 +0800] "GET /u2bmp.html?dm=37no6.com/003&amp;ac=1510042131161237772&amp;v=y88j6-1.0&amp;rnd=1510042131161237772&amp;ext_y88j6_tid=003&amp;ext_y88j6_uid=1510042131161237772 HTTP/1.1" 200 54 "-" "-" 9CA13069CB4D7B836DC0B8F8FD06F8AF "ImgoTV-iphone/4.5.3.150815 CFNetwork/672.1.13 Darwin/14.0.0" 0.004 test.com.org 1443965482.737
</code></pre>

<p>负责人希望能够通过这些日志信息实时地获取服务的PV、UV等统计信息以及访问用户IP的所在地信息等，并且希望可以查询任意时间的用户访问信息，以此满足日常运营的需求，后续还可能添加告警和日运营报表等功能。</p>

<h2 id="section-1">解决方案</h2>

<p>在BMR中我们集成了Flume、Kafka、Spark、Hbase组件，可以很好的满足应用场景中IDMapping负责人的集群需求。我们设计了如下的大数据处理的pipeline。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-26/97268179.jpg" width="750px" /></p>

<p>每台机器上的日志数据通过flume实时地推送到Kafka集群中，在spark集群中订阅这些日志数据，经过ETL处理后存储到Hbase中。前端展示系统可以通过Hbase的Restful接口实时的获取数据。同时，可以提交新的spark streaming application修改原有的数据处理模型，也可以在后端也可以对数据进一步加工：通过集群内部的mahout、spark mllib或对接其他的BI系统，例如Palo、Saiku。</p>

<p>以下是前端获取数据后的展示效果图：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-24/88317070.jpg" width="350px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-3-23/77070404.jpg" width="400px" /></p>

<p>接下来通过三步骤对这个解决方案在BMR中的实现进行详细的阐述。</p>

<h3 id="step-1-">Step 1 创建集群</h3>

<p>创建包括Spark和Streaming组件的集群。在生产环境中建议分别创建Kafka和Spark集群。可以参考<a href="https://bce.baidu.com/doc/BMR/GettingStarted.html#.E5.88.9B.E5.BB.BA.E9.9B.86.E7.BE.A4">文档</a>。</p>

<h3 id="step-2-">Step 2 数据准备</h3>

<p>数据获取表示如何将nginx产生的日志通过flume导入到BMR集群中。我们执行下面的命令：</p>

<pre><code>wget http://bmr.bj.bcebos.com/tools/flume/flume-1.6.0.tar.gz
vim $FLUME_HOME/conf/flume-conf.properties
$FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/flume-conf.properties --name agent
</code></pre>

<p>以上的命令分别表示获取flume、编辑配置文件、运行flume agent。其中配置文件如下，将<code>agent.sources.s.command</code>改为<code>tail $NGINX_HOME/logs/access.log</code>。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class=""><span class="line">1.	#agent section
</span><span class="line">2.	agent.sources = s
</span><span class="line">3.	agent.channels = c
</span><span class="line">4.	agent.sinks = r
</span><span class="line">5.	
</span><span class="line">6.	#exec source
</span><span class="line">7.	agent.sources.s.type = exec
</span><span class="line">8.	agent.sources.s.command = /usr/bin/vmstat 1
</span><span class="line">9.	agent.sources.s.fileHeader = true
</span><span class="line">10.	agent.sources.s.batchSize = 100
</span><span class="line">11.	agent.sources.s.channels = c
</span><span class="line">12.	
</span><span class="line">13.	# Kafka Sink
</span><span class="line">14.	agent.sinks.r.type =  org.apache.flume.sink.kafka.KafkaSink
</span><span class="line">15.	agent.sinks.r.brokerList = host1:port1,host2:port2...     #推荐填写两个或以上
</span><span class="line">16.	agent.sinks.r.topic = topic
</span><span class="line">17.	agent.sinks.r.channel = c
</span><span class="line">18.	
</span><span class="line">19.	# Each channel's type is defined.
</span><span class="line">20.	agent.channels.c.type = memory
</span><span class="line">21.	agent.channels.c.capacity = 1000</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="step-3-">Step 3 数据处理</h3>
<p>建立新的spark集群，当然在测试阶段您也可以直接使用kafka集群中的spark进行处理，在实际应用中推荐使用新的spark集群。</p>

<ol>
  <li>下载spark streaming代码，进行编译，将编译结果<code>bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</code>放到bos中。</li>
  <li>
    <p>从console页面进去到对应集群的作业列表页面，然后点击“添加作业”，如果使用系统提供的输入数据和jar包，可以按照如下方式填写参数：</p>

    <blockquote>
      <p>作业类型：Spark</p>
    </blockquote>

    <blockquote>
      <p>名称：FKSTest</p>
    </blockquote>

    <blockquote>
      <p>bos输入地址： bos://${PATH}/bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</p>
    </blockquote>

    <blockquote>
      <p>失败后操作：继续</p>
    </blockquote>

    <blockquote>
      <p>Spark-submit: –class com.baidubce.bmr.sample.DirectFKSTest</p>
    </blockquote>

    <blockquote>
      <p>应用程序参数：ng1889b62-master-instance-f5lvbago topic</p>
    </blockquote>

    <p>其中应用程序参数分别代表集群master的hostname和kafka topic。 </p>
  </li>
  <li>
    <p>您可以通过集群页面的<code>Resource Manager Web UI</code>查看spark UI查看作业运行的状态。（进入页面所需要的用户名密码会通过短信形式发送到您手机上）
 <img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/49196148.jpg" alt="" /></p>
  </li>
  <li>
    <p>查看hbase中的数据：</p>

    <pre><code> hbase(main):001:0&gt; list
 hbase(main):002:0&gt; scan 'PVUV', {COLUMN=&gt;['statistics:PV:toInt', 'statistics:UV:toInt']}
</code></pre>
  </li>
</ol>

<h2 id="spark-streaming">关于Spark Streaming中实时流的说明建议</h2>

<p>在Spark Streaming中有两种API用于处理与kafka之间的交互。</p>

<ul>
  <li>一种是spark1.2.0引进的<code>KafkaUtils.createStream</code>，这种方式可以将kafka或其他流式输入先写入磁盘再分片处理，防止重启driver造成数据丢失。换句话说，可以保证At least Once语义，前提是开启<code>Write Ahead Logs</code>，方法如下
    <ul>
      <li>在代码中通过<code>streamingContext.checkpoint</code>配置checkpoint目录</li>
      <li>配置<code>spark.streaming.receiver.writeAheadLog.enable</code>为<code>true</code></li>
    </ul>
  </li>
  <li>另外一种则是spark1.3.0引进的Direct API。这种方式保证的是<code>Exactly Once</code>语义，解决上种方式中<code>consumer offset</code>和数据Logs存储不一致性造成的数据重复计算。这种方式通过将<code>offset</code>存入
checkpoints中，来保证接收数据的一致性。</li>
</ul>

<p>使用<code>KafkaUtils.createStream</code>需要有一下两种考虑：</p>

<ul>
  <li>提高streaming的吞吐量，我们通常会使用多个consumer来并行的获取数据，每个consumer分配到一个executor的单核上，最后将所有得到的Stream进行<code>Union</code>操作。
如果不进行<code>Union</code>则会导致<code>Transformation</code>数量增多<code>#consumer</code>倍。</li>
  <li>另外也要考虑RDD中partition的数量，减少partition数量有助于减少task个数以及调度时间。partition的数量是由batchInterval和spark.streaming.blockInterval共同决定的，根据spark官方指导，通常partition数目
为cores的2到3倍比较合适，所以可以调整适当的参数控制partition的个数。</li>
</ul>

<p>而在DirectAPI中会自动定期的根据kafka的topic+partition查询最新的offset，定义需要处理的offset范围。所以不需要考虑创建多少receivers，也不需要考虑partition的数量。在API中每个kafka partition都是自动地并行读取，并且对应每个RDD partition，从而简化Streaming处理的并行模式。</p>

<p>但是DirectAPI并不会在zookeeper中更新offset，所以基于zookeeper的kafka监控工具无法查看日志处理的进度。但您也可以查询checkpoint，将offset写入zookeeper中。</p>

<p>这两种使用方式在Sample中都有详细的例子可以参考，分别是<code>com.baidubce.bmr.sample.FKSTest</code>和<code>com.baidubce.bmr.sample.DirectFKSTest</code>。</p>

<h2 id="section-2">总结</h2>

<p>虽然针对不同的目标和业务案例使用流式处理的方式也不同，但其主要场景包括：</p>

<ul>
  <li>流ETL——将数据推入存储系统之前对其进行清洗和聚合。</li>
  <li>触发器——实时检测异常行为并触发相关的处理逻辑。</li>
  <li>数据浓缩——将实时数据与静态数据浓缩成更为精炼的数据以用于实时分析。</li>
  <li>复杂会话和持续学习——将与实时会话相关的事件组合起来进行分析。</li>
</ul>

<p>在上述例子中我们介绍了BMR中流ETL的场景。
在BMR中，我们提供了Hadoop生态圈中的全栈组件包括Hadoop、Spark、Hbase、Hive、Pig、Kafka、Mahout等，
您可以根据自己的业务场景灵活地选择不同的组件。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/20/hadoop-intro/"/>
    <updated>2015-12-20T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/20/hadoop-intro</id>
    <content type="html"><![CDATA[<p>看图说话，用一些图表来记录Hadoop。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/22389512.jpg" width="500px" /></p>

<!--more-->

<h3 id="mapreduce">MapReduce</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/71525619.jpg" width="500px" /></p>

<p>schema-on-read表示hadoop更容易处理非结构化或半结构化的数据，因为可以在数据处理的时候进行解析，因此在加载数据的时候更加轻量化。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/99564088.jpg" width="500px" /></p>

<h3 id="hdfs">HDFS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/60582298.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/98566811.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/1953976.jpg" width="500px" /></p>

<p>NameNode维护整个文件系统的文件目录树，文件目录的元信息和文件的数据块索引。这些信息以 FSImage 和 EditLog 方式存储在本地文件系统中。 FSImage 是某一时刻的镜像，后续修改都是放在 EditLog 中。 Second NameNode 会间隔将 FSImage 和 EditLog 进行合并后放在 NameNode 上。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/94295030.jpg" width="500px" /></p>

<h3 id="yarn">Yarn</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/80769446.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/31902359.jpg" width="400px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37369292.jpg" width="500px" /></p>

<ol>
  <li>
    <p>Fair Scheduler
 Facebook开发的适合共享环境的调度器，支持多用户多分组管理，每个分组可以配置资源量，也可限制每个用户和每个分组中的并发运行作业数量；每个用户的作业有优先级，优先级越高分配的资源越多。</p>
  </li>
  <li>
    <p>Capacity Scheduler
 Yahoo开发的适合共享环境的调度器，支持多用户多队列管理，每个队列可以配置资源量，也可限制每个用户和每个队列的并发运行作业数量，也可限制每个作业使用的内存量；每个用户的作业有优先级，在单个队列中，作业按照先来先服务（实际上是先按照优先级，优先级相同的再按照作业提交时间）的原则进行调度。</p>
  </li>
</ol>

<p><strong>Fair Scheduler vs Capacity Scheduler</strong></p>

<ul>
  <li>
    <p>相同点</p>

    <ul>
      <li>均支持多用户多队列，即：适用于多用户共享集群的应用环境</li>
      <li>单个队列均支持优先级和FIFO调度方式</li>
      <li>均支持资源共享，即某个queue中的资源有剩余时，可共享给其他缺资源的queue</li>
    </ul>
  </li>
  <li>
    <p>不同点</p>

    <ul>
      <li>
        <p>核心调度策略不同。 计算能力调度器的调度策略是，先选择资源利用率低的queue，然后在queue中同时考虑FIFO和memory constraint因素；而公平调度器仅考虑公平，而公平是通过作业缺额体现的，调度器每次选择缺额最大的job（queue的资源量，job优先级等仅用于计算作业缺额）。</p>
      </li>
      <li>
        <p>内存约束。计算能力调度器调度job时会考虑作业的内存限制，为了满足某些特殊job的特殊内存需求，可能会为该job分配多个slot；而公平调度器对这种特殊的job无能为力，只能杀掉这种task。</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/75633723.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-7-5/56706884.jpg" width="450px" /></p>

<h3 id="hadoop-io">HADOOP IO</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/15159126.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/90632065.jpg" width="500px" /></p>

<p>其他流行的兼容Hadoop的序列化框架还有：Avro、Thrift、google protobuf</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/11927837.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/70087429.jpg" width="500px" /></p>

<h3 id="hadoop-jobs">HADOOP JOBS</h3>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/38335644.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/37931472.jpg" width="500px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-29/10857296.jpg" width="500px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning/"/>
    <updated>2015-12-01T09:50:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning</id>
    <content type="html"><![CDATA[<p>Hadoop的调优涉及到整个MapReduce的各个过程，并且要对每个参数的意义和Counter信息有一定的了解。也就是说，根据Counter的信息推测哪些MapReduce阶段可能存在性能瓶颈，并且根据这个瓶颈理解对应的Hadoop 框架中的处理逻辑，进而可以调整相关参数的大小或程序的行为。</p>

<p>从大面上来看，MapReduce的瓶颈可能存在以下图中的各个部分。</p>

<p><img src="http://ww2.sinaimg.cn/large/74311666jw1eyjx8zpfz2j20rq0cegna.jpg" width="500px" /></p>

<!--more-->

<p>在开始调优之旅前，先来个开胃小菜。进行调优的过程中，我们首先要知道整个job的运行情况。</p>

<p>Hadoop 2中的JobHistory是能够提供作业运行时各个参数指标的展示工具，可以通过这个UI界面查看所有的Counter。另一方面如果不方便通过界面的方式查看，则可以利用Hadoop自带的命令行工具查看，方法是<code>hadoop job -history &lt;history file&gt;</code>, 这个history file的位置通常是在mapreduce.jobhistory.done-dir目录下，可以用如下方式查找<code>hadoop fs -lsr &lt;done-dir&gt; | grep job_1398974791337_0037</code>。</p>

<h2 id="map-optimizations">Map Optimizations</h2>

<p>在Map端的优化通常会涉及到输入的数据和它的处理过程，以及你的应用程序代码。Mapper需要读取作业的输入，输入文件的不同也会影响到作业运行的效率，例如文件是否是splittable，数据的本地性和input split的数量等等。</p>

<h3 id="data-locality">Data Locality</h3>

<p>在分布式计算中有条著名的准则<strong>Pushing compute to the data</strong>，map的task应该尽可能的被安排在数据存放的节点上。可以用Counter来判断作业是否符合这条准则:</p>

<ul>
  <li>HDFS_BYTES_READ：这个值应当不大于input file的block size</li>
  <li>DATA_LOCAL_MAPS：这个值应当为1</li>
  <li>RACK_LOCAL_MAPS：这个值应当为0</li>
</ul>

<p>以下几种情况可能会发生non-local read:</p>

<ul>
  <li>不能分割的大文件，这样mapper就必须从不同的节点中读取blocks。</li>
  <li>文件格式支持split，但是用的input format不对。典型的情况是LZOP格式，需要先建立索引后再进行读取。</li>
  <li>Yarn的调度器不能在某个节点上产生map container，通常是由于集群under load。</li>
</ul>

<p>解决方法是：</p>

<ul>
  <li>尽量保持unsplittable文件的大小接近一个block的大小。</li>
  <li>设置yarn的<code>scheduler.capacity.node-locality-delay</code>，引入跳过的调度次数，来增加map task分配到数据节点上的概率。</li>
  <li>使用Twitter提供的LZO Input Format来处理lzop数据，或者使用bzip2格式的文件。</li>
</ul>

<h3 id="map-number-overwhelm">Map Number Overwhelm</h3>

<p>当输入有很多的input split的时候，每个input split都需要一个mapper来执行，而每个mapper都是一个单独的进程。这样会给调度器和集群带来极大的压力。原因通常有两个：</p>

<ul>
  <li>input data由很多的小文件组成，Hadoop会为每个小文件生成一个mapper，最终时间会大量消耗在启动进程上。</li>
  <li>每个文件并不是很小（和block size相当），但总体的数据量很大，横跨上千个HDFS的blocks。这样每个block也会分配给单独的mapper。</li>
</ul>

<p>如果是第一种情况，可以先聚合这个小文件，或者使用类似avro的文件格式来存储。或者直接用<code>CombineFileInputFormat</code>来处理以上这两种情况，它可以在一个mapper里处理多个HDFS block的数据。<code>CombineFileInputFormat</code>会首先检查input files的所有blocks，简历每个block到data nodes的映射关系，接着将同一个节点上的blocks聚合到一个input split中以保持data locality。它有三个配置项来调整：</p>

<ul>
  <li>mapreduce.input.fileinputformat.split.minsize.per.node</li>
  <li>mapreduce.input.fileinputformat.split.minsize.per.rac</li>
  <li>mapreduce.input.fileinputformat.split.maxsize</li>
</ul>

<p>以上的默认配置会造成每个节点上尽量形成一个最大的input split，影响作业的并行性，可以用以上几个配置来调整。<code>CombineFileInputFormat</code>还包括两个具体的类：</p>

<ul>
  <li><code>CombineTextInputFormat</code></li>
  <li><code>CombineSequenceFileInputFormata</code></li>
</ul>

<h3 id="input-split-computation">Input Split Computation</h3>

<p>如果提交作业的client是在集群局域网之外，那么input split的计算可能带来高成本。</p>

<p>当输入的数据源是HDFS时，client需要做如下事情，包括file listing, file status retrieving，input files数量比较多的时候，整个过程带来数据传输的延迟是比较可观的。</p>

<p>可以通过设置<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>将input split的计算交给AppMaster处理，这是在集群内部进行的。</p>

<h2 id="shuffle-optimizations">Shuffle Optimizations</h2>

<h3 id="using-the-combiner">Using the Combiner</h3>

<p>combiner可以有效的减少mapper和reducer之间通信的数据量。</p>

<h3 id="using-binary-comparators">Using Binary Comparators</h3>

<p>MapReduce在做sorting或者merging的时候，使用<code>RawComparator</code>比较map output key。内置的<code>Writable</code> classes（<code>Text</code>、<code>IntWritable</code>）有byte-level的比较器，无需将二进制的数据重新组装成实际的对象，所以能够快速进行序列化对象的比较。</p>

<p>用户可以在自己构造的<code>Writable</code>对象里面实现<code>WritableComparable</code>接口，这处理起来会比较容易，但是另一方面要注意MapReduce中map output data是以byte的形式存储的，这会导致在shuffle和sort的阶段需要从byte到object的转化才可以完成对象的比较。</p>

<p>可以看到在Hadoop的内置<code>Writable</code>对象不仅实现了<code>WritableComparable</code>接口，还自定义继承自<code>WritableComparator</code>的比较器。<code>WritableComparator</code>有什么作用呢，可以看下它的一些方法申明。</p>

<pre><code>public class WritableComparator implements RawComparator {
    public int compare(byte[] b1, int s1, int l1,
                       byte[] b2, int s2, int l2); 
}
</code></pre>

<p>可以看到这是byte-level的Comparator，<code>Writable</code>对象正是覆盖了这里面的compare方法。在内置的<code>Writable</code>对象中都实现了<code>WritableComparator</code>，所以无需担心内置对象的效率。但是自己所构造的对象也可以实现<code>WritableComparator</code>方法来提高效率。</p>

<p>例如一个拥有firstName和lastName的Person对象：</p>

<pre><code>private String firstName;
private String lastName;

@Override
public void write(DataOutput out) throws IOException {
    out.writeUTF(lastName);
    out.writeUTF(firstName);
}
</code></pre>

<p><img src="http://i5.tietuku.com/e5eba32886b5773d.png" width="600px" /></p>

<pre><code>public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,
                     int l2) {
    int lastNameResult = compare(b1, s1, b2, s2);
    if (lastNameResult != 0) {
        return lastNameResult;
    }
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compare(b1, s1 + b1l1 + 2, b2, s2 + b2l1 + 2);
}

public static int compare(byte[] b1, int s1, byte[] b2, int s2) {
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compareBytes(b1, s1 + 2, b1l1, b2, s2 + 2, b2l1);
}

public static int readUnsignedShort(byte[] b, int offset) {
    int ch1 = b[offset];
    int ch2 = b[offset + 1];
    return (ch1 &lt;&lt; 8) + (ch2);
}
</code></pre>

<h3 id="tunning-the-shuffle-internals">Tunning the shuffle internals</h3>

<p>在mapper中，output record首先被存储在一个内存buffer中，当这个buffer增长到一定大小的时候，数据被spill到磁盘中的一个文件。整个过程持续到mapper完成所有的output record生成。过程如下：</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>在整个阶段中，I/O相关的splling和merging是最耗时的，所以理想状况应该是所有的output数据都可以装入buffer中，这样只有一个文件被spill到磁盘。这对所有的作业来说自然是不大可能，但是如果mapper可以通过filter或者project的方法减少input data，那么可以好好调整下<code>mapreduce.task.io.sort.mb</code>的大小，因为这个数据直接关系buffer的大小。可以通过检查下面的Counters来调整map端的shuffle：</p>

<ul>
  <li>MAP_OUTPUT_BYTES  用这个数据来估计是否可以调整<code>mapreduce.task.io.sort.mb</code>来装入map的output record</li>
  <li>SPILLED_RECORDS/MAP_OUTPUT_RECORDS  这两个数据的理想情况是一致的，表示只有一个spill发生。</li>
  <li>FILE_BYTES_READ/FILE_BYTES_WRITTEN  比较这两个数据和MAP_OUTPUT_BYTES可以理解在splling和merging阶段发生的读写副作用</li>
</ul>

<p>在reduce方面，map的output通过每个节点上运行的shuffle service进程被发送到对应的reducer。在reducer中，map output是被写入到一个内存buffer中，在数据接收的过程中buffer中的数据被排好序，并在到达一定数据量的时候写入磁盘。同时有一个后台进程负责不断merge这个小的spllied file到merged files中，当所有的fetcher获取了所有的outputs，会有一个最终的merging发生，这时候数据也就从merged files到reducer了。也就是如下图的这个过程：</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<p>通map端的调优一样，reduce端也是尽量将数据存入内存中，减少splling和merging发生的次数，但是这个过程并不如map端一样明显，因为数据是在边接收边merging的。默认情况下，无论数据是否可以装入内存中，splling总会发生，所以可以调整<code>mapreduce.reduce.merge.memtomem.enabled</code>为true启动memory-to-memory的merge。map端的Counter同样适用于reduce。</p>

<p>以下的参数可以用来调整Hadoop的shuffle行为：</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Default</th>
      <th>Map or Reduce</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mapreduce.task.io.sort.mb</td>
      <td>100 (MB)</td>
      <td>Map</td>
      <td>The total amount of buffer memory in megabytes to use when buffering map outputs. This should be approximately 70% of the map task’s heap size.</td>
    </tr>
    <tr>
      <td>mapreduce.map.sort.spill.percent</td>
      <td>0.8</td>
      <td>Map</td>
      <td>Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than 0.5.</td>
    </tr>
    <tr>
      <td>mapreduce.task.io.sort.factor</td>
      <td>10</td>
      <td>Map and Reduce</td>
      <td>The number of streams to merge at once while sorting files. This determines the number of open file handles. Larger clusters with 1,000 or more nodes can bump this up to 100.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.parallelcopies</td>
      <td>5</td>
      <td>Reduce</td>
      <td>The default number of parallel transfers run on the reduce side during the copy (shuffle) phase. Larger clusters with 1,000 or more nodes can bump this up to 20.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.input.buffer.percent</td>
      <td>0.7</td>
      <td>Reduce</td>
      <td>The percentage of memory to be allocated from the maximum heap size to store map outputs during the shuffle.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.merge.percent</td>
      <td>0.66</td>
      <td>Reduce</td>
      <td>The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.merge.memtomem.enabled</td>
      <td>false</td>
      <td>Reduce</td>
      <td>If all the map outputs for each reducer can be stored in memory, then set this property to true.</td>
    </tr>
  </tbody>
</table>

<p>Shuffle的原则是使用filter和project减少数据量，使用combiner以及压缩map的output，尽可能的减少mapper和reducer质检传递的数据，减低IO带来的开销。这样之后再利用上述提到的参数来调整Shuffle的过程。</p>

<h2 id="recuder-optimizations">Recuder Optimizations</h2>

<h3 id="the-number-of-reducers">The Number of Reducers</h3>

<p>大多数情况下map端的并行度是由框架根据input files和input format来自动决定的，但在Reduce端，reducer的数量是由用户自行决定的。太少的reducers意味着没能充分利用集群的资源，太多的reducer则会让调度器疲于奔命，如果没有太多的资源供所有reducer运行则会拖累reducer的执行效率。在一些使用场景中，不能避免的需要使用少量的reducer来运行作业，例如数据写入DB系统中。另外一些场景中需要确认数据是否会发生倾斜，以及如何partition的，数据量是否会让reducer发生OOM的情况。</p>

<h2 id="general-tunning-tips">General tunning tips</h2>

<ul>
  <li>压缩</li>
  <li>使用类似于Avro或Parquet的数据格式存储数据，带来的好处是空间利用率，序列化和反序列化更加有效。
    <ul>
      <li>在Hadoop中，text并不是一个有效的数据格式，空间利用率低，解析成本高，特别是用到正则的时候。</li>
      <li>尽可能考虑使用二进制的文件存储格式。</li>
    </ul>
  </li>
</ul>

<h2 id="tunning-tools">Tunning tools</h2>

<h3 id="stack-dumps">stack dumps</h3>

<p>ssh到task运行的机器上，执行下面的命令：</p>

<pre><code>ps aux | grep container_1393242034820_0001_01_000002
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
</code></pre>

<p><code>SIGQUIT</code>信号的发送应该要间隔几秒，当JVM收到这个信号的时候会执行stack dump，这样可以了解到程序在这段时间内的运行情况。最后可以在task 的output file中查看dump出来的栈信息。</p>

<h3 id="profiling-map-and-reduce-task">Profiling Map and Reduce Task</h3>

<p>可以使用HPROF结合一些Mapreduce job method来进行Profiling。HPROF是JVM内置的java profiling工具，Hadoop内置了对HPROF的支持。可以在driver中加入如下的代码：</p>

<pre><code>job.setProfileEnabled(true);
job.setProfileParams(
    "-agentlib:hprof=depth=8,cpu=samples,heap=sites,force=n," +
        "thread=y,verbose=n,file=%s");
job.setProfileTaskRange(true, "0,1,5-10");
job.setProfileTaskRange(false, "");
</code></pre>

<p>在<code>setProfileParams</code>方法中设置的参数会在每个container中建立一个名为profile.out的文件，这个文件可以很容易被解析。可以通过ssh到目标机器查看或者通过JobHistory UI界面查看。</p>

<p>profile.out包括一些stack traces，还包括内存和CPU时间的信息。以下是一个profile.out文件：</p>

<p><img src="http://i5.tietuku.com/ba1dcbebf426b1a8.png" width="600px" /></p>

<p>可以看出来第一个问题是在<code>String.split</code>这个方法的使用上，它采用正则表达式来分割字符串，这个是相当耗时的一个举措，可以用Apache Commons Lang library的<code>StringUtils.split</code>来替换。另外一个是发生在Text的构造上，可以只构造一个Text实例，采用<code>set</code>方法进行设置，这样会更加有效率。</p>

<p>需要注意，使用HPROF会给程序的执行带来额外的负担，需要持续的收集profiling的信息，所以在正常的运行过程中不应该加上。</p>

<h2 id="conclusion">Conclusion</h2>

<p>一些在工作中实际用到的调优方法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-6-30/1330399.jpg" width="900px" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Secondary Sorting]]></title>
    <link href="http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting/"/>
    <updated>2015-11-22T17:27:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting</id>
    <content type="html"><![CDATA[<p>Hadoop MapReduce的神奇之处发生在mapper和reducer之间，将所有相同key的map输出记录聚集在一块，使得用户可以方便的处理聚合在一起的数据。Hadoop内部使用了partition、sort和merge（shuffle的一部分），在每个reducer中流式地得到排序后的key和value集合。在MapReduce Sorting中有个特别的部分是secondary sort，也就是对value进行排序。</p>

<!--more-->

<p>Secondary sort在两种情况下特别有用：</p>

<ul>
  <li>需要某一部分的数据比其他数据更快的到达reducer。</li>
  <li>希望job的输出按照两个key进行排序。</li>
</ul>

<p>实现Secondary sort需要对MapReduce中的数据流和处理有一定的了解，下图展示了对reducer中出现的数据有影响的三个部分。</p>

<p><img src="http://i5.tietuku.com/7ad3ad872415c4b6.png" width="600px" /></p>

<p><code>partitioner</code>决定哪个reducer接收该mapper数据记录；<code>sorting RawComparator</code>用于在各自的分片中排序输出的结果，map和reduce阶段都有它，其中map阶段的sorting是对reduce阶段sorting的一个优化，让reducer的sorting更高效；最后，<code>grouping RawComparator</code>用于决定reducer处理排序后记录的边界，发生在reducer从本地磁盘读取数据的时候，也就是说，你可以用这个方法决定数据记录是如何聚集起来调用一个reduce方法的。MapReduce默认把这个三个方法作用于map方法输出的key上。</p>

<p>要实现Secondary sorting，我们需要重写partitioner、sort comparator和grouping comparator。</p>

<p>下面，通过对人名的排序来说明如何使用Secondary sorting。使用primary sort排序last name，secondary sort排序first name。</p>

<p>我们需要构建一个由map方法输出的Composite key，这个key由两部分组成：</p>

<ul>
  <li>Natural Key</li>
  <li>Secondary Key</li>
</ul>

<p><img src="http://i5.tietuku.com/25eedf0319e92775.png" width="430px" /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Person</span> <span class="kd">implements</span> <span class="n">WritableComparable</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">firstName</span><span class="o">;</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">lastName</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">readFields</span><span class="o">(</span><span class="n">DataInput</span> <span class="n">in</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">firstName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">lastName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">write</span><span class="o">(</span><span class="n">DataOutput</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">lastName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>下图说明hadoop框架配置中用于设置partitioning、sorting和grouping类的名字和方法。</p>

<p><img src="http://i5.tietuku.com/520e7242cd6ecc43.png" width="530px" /></p>

<h3 id="partitioner">Partitioner</h3>

<p>默认的partitioner使用对key进行hash后取reducer个数的模。但是默认的partitioner使用整个key，会导致相同的natural key发往不同的reducer。所以需要实现自己的partitioner。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNamePartitioner</span> <span class="kd">extends</span>
</span><span class="line">    <span class="n">Partitioner</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numPartitions</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="k">return</span> <span class="n">Math</span><span class="o">.</span><span class="na">abs</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">*</span> <span class="mi">127</span><span class="o">)</span> <span class="o">%</span>
</span><span class="line">        <span class="n">numPartitions</span><span class="o">;</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="sorting">Sorting</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">w1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">w2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">
</span><span class="line">    <span class="kt">int</span> <span class="n">cmp</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">if</span> <span class="o">(</span><span class="n">cmp</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="k">return</span> <span class="n">cmp</span><span class="o">;</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">());</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="grouping">grouping</h3>

<p>grouping阶段所有的数据记录已经是secondary sort了，grouping comparator需要将相同的last name聚合在一起。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNameComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonNameComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">o1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">o2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="mapreduce">MapReduce</h3>

<p>最后在driver中，需要设置上文提到的三个类：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setPartitionerClass</span><span class="o">(</span><span class="n">PersonNamePartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setSortComparatorClass</span><span class="o">(</span><span class="n">PersonComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setGroupingComparatorClass</span><span class="o">(</span><span class="n">PersonNameComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Map</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">Person</span> <span class="n">outputKey</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Person</span><span class="o">();</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">Text</span> <span class="n">lastName</span><span class="o">,</span> <span class="n">Text</span> <span class="n">firstName</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">outputKey</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">lastName</span><span class="o">.</span><span class="na">toString</span><span class="o">(),</span> <span class="n">firstName</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class="line">    <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">outputKey</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Reduce</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="n">Text</span> <span class="n">lastName</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span>
</span><span class="line">                     <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">lastName</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">for</span> <span class="o">(</span><span class="n">Text</span> <span class="n">firstName</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">lastName</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Secondary sort涉及到的自定义的partitioner、sorter和grouper，还是比较复杂的。可以考虑<a href="http://htuple.org">htuple</a>对简单类型进行secondary sort。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming]]></title>
    <link href="http://billowkiller.github.io/blog/2015/10/27/spark-streaming/"/>
    <updated>2015-10-27T21:04:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/10/27/spark-streaming</id>
    <content type="html"><![CDATA[<p>流式计算通常是为了满足日益增长的数据的实时获取和低延时计算的需求。通常来说，一个优秀的流式计算引擎需要满足一下的一些需求：</p>

<ol>
  <li>不重不丢的保证（在节点或计算失败的时候，内存中的计算状态能被正确的恢复）</li>
  <li>低延迟</li>
  <li>高吞吐量</li>
  <li>强大的计算模型</li>
  <li>容错机制的低开销</li>
  <li>流控</li>
</ol>

<p><img src="https://spark.apache.org/images/spark-logo.png" width="200px" /></p>

<!--more-->

<p>介绍完了spark后就可以来说说spark streaming，毕竟spark streaming是完全构建在spark之上，熟悉了spark的RDD原理之后就比较容易理解spark streaming。说白了，spark streaming中的流式计算是<code>伪实时</code>的，之所以是伪实时的是因为它将实时的处理变成时间跨度较小的批量处理。没错，就是<code>将一段时间间隔中的数据变成RDD</code>，然后利用spark原有的架构处理这段时间内的数据，接着在时间维度上对这些处理后的RDD进行迭代计算。也就是将流式计算分解为一系列微小的、原子的批量作业，每个微批量作业如果失败则可以重新计算。这种分解的思想可以应用在批量计算框架、也可以应用在流式计算框架上，例如Storm Trident。</p>

<p>这样的处理带来了几个明显的好处：</p>

<ul>
  <li>高吞吐量</li>
  <li>不从不丢的保证</li>
  <li>快速falut recovery</li>
  <li>更方便处理慢节点</li>
  <li>实时和批量统一的编程接口</li>
</ul>

<p>下面介绍下spark streaming中的具体实现来理解这几点。</p>

<h3 id="section">计算模型</h3>

<p><img src="http://blog.selfup.cn/wp-content/uploads/2014/08/streaming-flow.png" alt="image" /></p>

<p>首先，Spark Streaming把实时输入数据流以时间片（如1秒）为单位切分成块。Spark Streaming会把每块数据作为一个RDD，并使用RDD操作处理每一小块数据。每个块都会生成一个Spark Job处理，最终结果也返回多块。</p>

<p>举个栗子来说明下这个过程：</p>

<pre><code>pageViews = readStream("http://..."， “ls”)
ones = pageViews.map(event =&gt; (event.url, 1))
counts = ones.runningReduce((a, b) =&gt; a+b)
</code></pre>

<p>上述代码的作用是根据URL的数量计算访问次数。处理的过程为首先通过HTTP获取到一个pageview事件的RDD，经过<em>transformation</em>操作变成(URL, 1), 最后的操作计算相同的URL数目。整个streaming过程可以用下图来表示：</p>

<p><img src="http://img-storage.qiniudn.com/15-10-18/95269436.jpg" width="500px" /></p>

<p>整个的处理过程可以看出，输入流被分成每个都是1秒的batch，经过处理后生成resultRDD，这个resultRDD在每个时间间隔中都会产生一个，并经过reduce迭代计算。在上图中最终的reduce的输入参数就包括上一个时间间隔的resultRDD和这个时间间隔中map操作的结果。上图也可以看出这些RDD的lineage graph，在节点失败的时候，可以根据这个lineage graph以partition的粒度为单位重新执行任务计算丢失的分片，可以看出这些计算的任务都是可以并行执行的。同时，对于慢节点来说，因为<code>计算是无状态的</code>，且每个job的结果是可以确定的，spark streaming可以执行类似于hadoop中的预测模型——在其他节点上计算同样的任务。另外，spark streaming也会有些checkpoint来防止无限制的恢复计算。</p>

<p>对比于其他流式处理的方式，spark streaming在处理失败任务和慢节点上无疑更有效率。它可以从<code>时间和partition</code>两个维度上并行地计算数据来加快恢复速度。而对于像Strom的流式处理来说，往往是通过<strong>上游数据backup</strong>或者<strong>同时复制执行相同的作业</strong>来保证数据处理的可靠性。这两种处理方式对于资源的消耗无疑都是巨大的，且恢复的时间也比较长。</p>

<ul>
  <li>其中对于前者来说，每个节点都需要保存上一个checkpoing后所发送数据的拷贝，在节点失败时由standby机器重新计算上游节点发送过来的数据，因为这些计算都是有状态的，所以恢复的时间比较长，Storm就只保证“at least once”语义来提高处理速度。Trident之所以能够保证不重不丢是使用了数据库来复制计算状态。</li>
  <li>对于后者无疑更加消耗资源，并且需要保证两个数据处理操作接受到的上游数据的顺序是一样的。</li>
</ul>

<p>同spark一样，只有当某个Output Operations原语被调用时，stream才会开始真正的计算过程。现阶段支持的Output方式有以下几种：</p>

<ul>
  <li>print()</li>
  <li>foreachRDD(func)</li>
  <li>saveAsObjectFiles(prefix, [suffix])</li>
  <li>saveAsTextFiles(prefix, [suffix])</li>
  <li>saveAsHadoopFiles(prefix, [suffix])</li>
</ul>

<h3 id="section-1">流式处理中的几点难点</h3>

<p>在流式处理中通常都会有几个难点需要考虑。</p>

<ul>
  <li>时间窗口问题</li>
  <li>数据一致性问题</li>
  <li>内存状态管理问题</li>
</ul>

<p>我们来看下spark streaming是如何解决的。</p>

<ol>
  <li>
    <p><strong>时间窗口问题。</strong>spark streaming是根据数据到达系统的时间将记录放到对应的RDD中，这种时间窗口划分是基于墙上时间的，好处可以保证系统及时产生一个新的batch运行job，并且可以让程序运行在数据生成的地方，不必再进行分发。</p>

    <p>这种基于墙上时间的统计有一个非常严重的问题是不能回放数据流。当数据流是实时产生的时候，“墙上时间”的一分钟也就只会有一分钟的event被产生出来。但是如果统计的数据流是基于历史event的，那么一分钟可以产生消费的event数量只受限于数据处理速度。另外event在分布式采集的时候也遇到有快有慢的问题，一分钟内产生的event未必可以在一分钟内精确到达统计端，这样就会因为采集的延迟波动影响统计数据的准确性。所以产生了另外一种时间窗口划分的方法。</p>

    <p>另一种时间窗口划分的方法是基于外部时间的，例如日志时间。spark streaming提供两种方法来处理这种情况：</p>

    <ul>
      <li>延迟处理，等待一定时间来处理每个batch。</li>
      <li>用户的应用程序中保证乱序事件的正确处理。</li>
    </ul>

    <p>以上也说明在<code>批处理的流式计算模型是受限的</code>，很多情况下只能依靠用户的应用程序来做处理，例如实时的统计5s内的pv；其次这种方式也没有很好的流控技术手段，如果有突发的大量数据产生，会导致结果产生的时间更长，甚至是将系统的JVM撑爆。最后实时性也是受限的，只能达到次秒级的处理延迟，毕竟是要等待一个时间batch的处理完成。</p>
  </li>
  <li>
    <p><strong>数据一致性问题。</strong>什么是数据一致性，举个栗子，要统计网站中来自各个国家的page view，把不同国家的pv统计放在不同的节点上处理。但是现在统计英国的节点处理速度要慢于法国的，这将导致两个节点上数据的时间状态不一致。在流式处理中，数据的一致性的保证同时意味着资源的消耗。流式处理的数据一致性有三种解决思路，在上文中也有提到，这里概括下：</p>

    <ul>
      <li>上游备份策略：重启的时候重放kafka的历史数据，恢复内存状态</li>
      <li>中间状态持久化：把统计的状态放到外部的持久的数据库里，不放内存里</li>
      <li>同时跑两份：同时有两个完全一样的统计任务，重启一个，另外一个还能正常运行。</li>
    </ul>

    <p>而在spark streaming中，数据一致性天然得到保证的。因为记录根据时间来分片，所以中间的resultRDD反应的是当前时间和之前时间所计算出来的结果，无论计算和结果被分配到哪个节点上都不会有节点间数据不一致的情况。也就是数据的不重不丢可以得到保证。</p>
  </li>
  <li>
    <p><strong>内存状态管理问题。</strong>
做流式统计的有两种做法：</p>

    <ul>
      <li>依赖于外部存储管理状态：比如没收到一个event，就往redis里发incr增1</li>
      <li>纯内存统计：在内存里设置一个counter，每收到一个event就+1</li>
    </ul>

    <p>第一种会把整个压力全部压到数据库上，造成处理速度下降；第二种的状态相对来说容易管理一些，计算直接是基于这个内存状态做的。如果重启丢失了，重放一段历史数据就可以重建出来。内存的问题是它总是不够用的，解决的方法是input分割和把存储移到外边去。在内存计算中把窗口统计的中间状态落地的好处是显而易见的：重启之后不用通过重算来恢复内存状态。但是这种对外部数据库使用不小心就会导致两个问题：</p>

    <ul>
      <li>处理速度慢。不用一些批量的操作，数据库操作很快就会变成瓶颈</li>
      <li>数据库的状态不一致。内存的状态重启了就丢失了，外部的状态重启之后不丢失。重放数据流就可能导致数据的重复统计</li>
    </ul>

    <p>在spark streaming中支持传统批量计算中的无状态<em>transformation</em>操作，例如<code>map</code>、<code>reduce</code>、<code>groupBy</code>和<code>join</code>。这就避免了普通流式计算中麻烦的状态保存问题。但spark streaming中也支持多个时间间隔中有状态的<em>transformation</em>操作，包括：</p>

    <ol>
      <li>Windowing: 生成滑动窗口RDD。<code>words.window("5s")</code>将产生一个RDD包含[0,5),[1,6),[2,7)的时间间隔。</li>
      <li>增量聚合：在滑动窗口的基础上进行RDD的聚合操作，也就是<code>reduceByWindow</code>。在下图的<em>a</em>中对应的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b)</code>，也就是计算5s内的计数之后。图<em>b</em>的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b, (a, b) =&gt; a-b)</code>。其实很简单，第一个lambda表达式为进入滑动窗口的处理函数，第二个表达式为离开滑动窗口的处理函数。这样也就不用重复求和了。
 <img src="http://img-storage.qiniudn.com/15-10-18/97873121.jpg" alt="" /></li>
      <li>状态跟踪：
 如下图所示，就是保存上一个时间间隔的RDD与本次的记录进行groupBy加map计算的到状态的变化情况。
 <img src="http://img-storage.qiniudn.com/15-10-18/91458919.jpg" alt="" /></li>
    </ol>

    <p>在对这些带状态的操作的处理过程中也就用到了上述的所属的利用外存在保存中间的状态。spark streaming中这只发生在intervel之间，所以整个内存的状态管理会比传统的流式处理简单许多，而且高效，不需要对每一步都进行状态同步，状态恢复的成本也比较低，上文中提到的可以在多个节点上并行计算恢复。</p>
  </li>
</ol>

<h3 id="system-architecture">System Architecture</h3>
<p><img src="http://img-storage.qiniudn.com/15-10-18/82954556.jpg" alt="" /></p>

<p>Spark streaming和Spark的系统结构有些许改动，如上图所示主要包括3个部分：</p>

<ul>
  <li><em>master</em> that tracks the D-Stream lineage graph and schedules tasks to compute new RDD partitions.</li>
  <li><em>Worker</em> nodes that receive data, store the partitions of input and computed RDDs, and execute tasks.</li>
  <li>A <em>client</em> library used to send data into the system.</li>
</ul>

<p>从上图中可以看出，Spark Streaming和传统的流式系统最大的区别就是Spark Streaming将计算分成小的，无状态的确定性的任务，这些任务会在集群的任意节点上运行。并且相对于传统流式系统的拓扑结构来说，无需消耗大量时间将将任务进行迁移，Spark Streaming可以很好的对机器上的节点进行负载均衡，处理失败任务并且对慢节点进行预测。</p>

<p>对比于Spark的系统，Spark Streaming做了一下的改进：</p>

<ul>
  <li>网络传输。使用异步I/O获取远端数据。</li>
  <li>TimeStep pipelining。Spark的调度器可以在当前任务还未完成的时候可以提交下一个时间分片的任务。</li>
  <li>任务调度：优化任务调度器，例如调整控制消息的大小，可以在每隔几百毫秒时间内启动几百个并行任务。</li>
  <li>存储层：支持异步的RDD checkpoint，RDD是不可变的，所以异步存储不会阻塞现有的计算。</li>
  <li>Lineage切割：控制RDD linage graph的大小，在checkpoint之前的lineage便可以删除。</li>
</ul>

<p>当master fail的时候可以进行HA，所有的worker重新连接到新的master上，将原来的checkpoint和原始数据重新计算。因为所有的操作都是确定性的，所以RDD是可以重复计算，也就是说在HA的时候丢失一些正在运行的计算任务不会对最终结果造成什么影响。所有的元数据都是存储在HDFS上的，包括：</p>

<ol>
  <li>RDD的lineage graph，代表用户代码的Scala函数对象。</li>
  <li>上一个checkpoint的时间</li>
  <li>RDD的ID。因为HDFS的checkpoint文件会在每个时间片重新命名。</li>
</ol>

<h3 id="faq">FAQ</h3>
<ol>
  <li>
    <p>Dstream与RDD之间的关系</p>

    <p>首先来看下Spark streaming的代码<code>val ssc = new StreamingContext(sc, Seconds(2))</code>。在这句的作用是定义Dstream生成的时间间隔，<code>2s</code>就是这个时间间隔，也叫<code>batch interval</code>。具体说来<strong>一个streaming batch对应一个RDD</strong>，也就是这个batch interval里产生的数据。</p>

    <p>在这个RDD中，有n个partition，n = batch interval / block interval。 <code>block interval</code>是spark steaming内部定义的一个变量<code>spark.streaming.blockInterval</code>，通常是200ms。上述例子就产生10个partitions。</p>

    <p>Blocks由一个receiver产生，receiver就是流式数据的接收端，每个receiver被分配到一个host上，所以上述的10个partitions就由一个node产生，同时被<code>复制到第二个节点上做容错</code>。注意，这里产生了data locality的问题。好的做法是，分配多个receivers接收数据，最后使用union合并数据做processing。当然还可以对Dstream做<code>repartition</code>操作提高并行度。</p>
  </li>
  <li>
    <p>Spark Streaming 的容错性</p>

    <p>对于文件这样的源数据，这个driver恢复机制足以做到零数据丢失，因为所有的数据都保存在了像HDFS或S3这样的容错文件系统中了。但对于像Kafka和Flume等其它数据源，有些接收到的数据还只缓存在内存中，尚未被处理，它们就有可能会丢失。这是由于Spark应用的分布操作方式引起的。当driver进程失败时，所有在standalone/yarn/mesos集群运行的executor，连同它们在内存中的所有数据，也同时被终止。</p>

    <p>首先driver会利用checkpoint来保存灾备需要的数据，有两种各类型的checkpoint，Metadata Checkpoint（保存streaming计算相关数据） 和 Data Checkpoint（保存生成RDD数据）</p>

    <p>对于Spark Streaming来说，从诸如Kafka和Flume的数据源接收到的所有数据，在它们处理完成之前，一直都缓存在executor的内存中。纵然driver重新启动，这些缓存的数据也不能被恢复。为了避免这种数据损失，Spark 1.2发布版本中引进了预写日志（Write Ahead Logs）功能。</p>

    <p>当启用了预写日志以后，所有收到的数据同时还保存到了容错文件系统的日志文件中。因此即使Spark Streaming失败，这些接收到的数据也不会丢失。另外，接收数据的正确性只在数据被预写到日志以后接收器才会确认，已经缓存但还没有保存的数据可以在driver重新启动之后由数据源再发送一次。这两个机制确保了零数据丢失，即所有的数据或者从日志中恢复，或者由数据源重发。</p>

    <p><a href="http://www.csdn.net/article/2015-03-03/2824081">http://www.csdn.net/article/2015-03-03/2824081</a></p>
  </li>
</ol>

]]></content>
  </entry>
  
</feed>
