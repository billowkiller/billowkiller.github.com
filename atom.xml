<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Billowkiller's Blog]]></title>
  <link href="http://billowkiller.github.io/atom.xml" rel="self"/>
  <link href="http://billowkiller.github.io/"/>
  <updated>2016-02-28T23:09:16+08:00</updated>
  <id>http://billowkiller.github.io/</id>
  <author>
    <name><![CDATA[wutao]]></name>
    <email><![CDATA[billowkiller@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Support Vector Machine]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/27/svm/"/>
    <updated>2016-02-27T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/27/svm</id>
    <content type="html"><![CDATA[<p>支持向量机(support vector machine, SVM)是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，可以形式化为凸二次规划问题的求解，也等价于正则化的合页损失函数的最小化问题。SVM还包括kernel trick，使得它可以成为实质上的非线性分类器。下面就介绍Perceptron到三种类型的SVM模型。</p>

<p><img src="http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/figs/svm2.PNG" width="400px" /></p>

<!--more-->

<h2 id="perceptron">Perceptron</h2>

<p>SVM可以说是Perceptron的一种进化。什么是Perceptron，Wiki的解释如下：</p>

<blockquote>
  <p>the perceptron is an algorithm for supervised learning of binary classifiers</p>
</blockquote>

<p>实则是一个二元线性分类器，通过一个线性的预测函数将观察点分成两类，这个预测函数就是特征空间中的一个分离超平面。对应于方程 $wx+b=0$, $w$ 为法向量或者权值，$b$ 是截距或偏置。</p>

<p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_i＝+1$ 的实例 $i$，有 $wx_i+b&gt;0$，对所有 $y_i＝-1$ 的实例 $i$，有 $wx_i+b&lt;0$，则称数据集为线性可分数据集（linearly separable data set）;否则，称数据集线性不可分。</p>

<p>对于误分类点有 $y_i(wx_i+b) &lt; 0$, 设误分类点集 $M$, 采用0/1损失函数, 则得到感知机的损失函数如下：</p>

<script type="math/tex; mode=display"> L(w, b) = -\sum_{x_i \in M} y_i(w x_i + b) </script>

<p>要使损失函数最小，可以采用随机梯度下降法。任意选取一个超平面 $w_0, b_0$，然后用梯度下降法不断地最小化目标函数，每次选取一个误分类点使其梯度下降。每次迭代如下，随机选取一个误分类点 $(x_i, y_i)$, 对 $(w, b)$ 更新, $\eta$ 为步长：</p>

<script type="math/tex; mode=display"> w \gets w + \eta y_ix_i </script>

<script type="math/tex; mode=display"> b \gets b + \eta y_i </script>

<u>下面我们来证明下经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。</u>

<p>存在超平面 $y_i(\hat{w}_{opt} \cdot \hat{x}_i) = w_{opt} \cdot x_i + b_{opt} = 0$，使 $|\hat{w}_{opt}| = 1$，那么可以对于任意的点 $i$，$y_i(\hat{w}_{opt} \cdot \hat{x}_i) &gt; 0$，所有存在 $\gamma$，使得</p>

<script type="math/tex; mode=display">y_i(\hat{w}_{opt} \cdot \hat{x}_i) = w_{opt} \cdot x_i + b_{opt} \ge \gamma</script>

<p>感知机算法从 $\hat{w}_0 = 0$ 开始，如果实例被误分类，则更新权重。设 $\hat{w}_{k-1}$ 是第 $k$ 个误分类点之前扩充的权值向量，则第 $k$ 个误分类点满足$y_i(\hat{w}_{k-1} \cdot \hat{x}_i) \le 0$，$\hat{w}$ 更新后有</p>

<script type="math/tex; mode=display"> \hat{w}_{k} = \hat{w}_{k-1} + \eta y_i \hat{x}_i </script>

<p>可以得到</p>

<script type="math/tex; mode=display"> \hat{w}_{k} \cdot \hat{w}_{opt} = \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta y_i \hat{w}_{opt} \cdot \hat{x}_i \ge \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma </script>

<script type="math/tex; mode=display">\to \hat{w}_{k} \cdot \hat{w}_{opt} \ge k \eta \gamma</script>

<p>另外假设 $R = max(|\hat{x}_i|)$，有</p>

<script type="math/tex; mode=display"> \|\hat{w}_{k}\|^2 = \|\hat{w}_{k-1}\|^2 + 2\eta y_i \hat{w}_{k-1} \cdot \hat{x}_i + \eta^2 \|\hat{x}_i\|^2 \le \|\hat{w}_{k-1}\|^2 + \eta^2 \|\hat{x}_i\|^2 \le \|\hat{w}_{k-1}\|^2 \eta^2 R^2 </script>

<script type="math/tex; mode=display">\to \|\hat{w}_{k}\|^2  \le k \eta^2 R^2 </script>

<p>我们可以得到 </p>

<script type="math/tex; mode=display">k\eta\gamma \le \hat{w}_{k} \cdot \hat{w}_{opt} \le \|\hat{w}_{k}\| \|\hat{w}_{opt}\| \le \sqrt{k}\eta R</script>

<p>于是 $k \le (R / \gamma)^2$，表示误分类次数 $k$ 是有上界的，也就是经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。</p>

<h3 id="pocket">pocket</h3>

<p>这里想另外介绍一种算法，Pocket算法。当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。 Pocket算法也就是用来权衡分离超平面和误分类点的。</p>

<p>从直觉上，我们知道如果当前超平面犯错越少越好，Pocket本质上就是在改错的时候多做一步，判断当前改正犯的错是否比之前更小，也就是贪心选择。</p>

<p>方法如图所示：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/79944104.jpg" width="500px" /></p>

<h2 id="linear-support-vector-machine">Linear Support Vector Machine</h2>

<p>Perceptron的问题是什么？它存在许多种解，只要是能够分割观察点的超平面全是它的解，既依赖于初值的选择，也依赖于迭代过程中误分类点的选择。这样的算法带来了不稳定性。为了得到唯一的超平面，需要增加一些约束条件。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/18916886.jpg" width="400px" /></p>

<p>我们认为点距离超平面越远，则越能够容忍噪声，并且对于overfitting的安全边界更大，也就是置信度越大。所以希望能够找到一个距离观察点最远的超平面作为我们的解，这个距离称为Margin，这样的超平面是唯一的。</p>

<p>点到平面的距离为沿着法向量方向的距离，所以有</p>

<script type="math/tex; mode=display"> distance(x, b, w) = \frac{1}{\|w\|} \vert w^Tx + b \vert </script>

<p>顺便提下，上式对法向量进行规范化，使得法向量为单位法向量，这个距离称<strong>几何间隔</strong>，否则是<strong>函数间隔</strong>。接下来，我们想要优化的目标可以写作：</p>

<script type="math/tex; mode=display"> \underset{b,w}{max} \frac{1}{\|w\|} </script>

<script type="math/tex; mode=display">s.t.\ every\ y_n(w^T x_n + b) > 0, \underset{n=1,2..N}{min} y_n(w^T x_n + b) = 1</script>

<p>这里我们有对distance进行缩放，除以 $w^Tx + b$。进一步对问题优化我们得到：</p>

<script type="math/tex; mode=display"> \underset{b,w}{max} \frac{1}{2}w^T w </script>

<script type="math/tex; mode=display">s.t.\ y_n(w^T x_n + b) \ge 1\ for\ all\ n</script>

<p>这样的一个问题其实就是<strong>凸二次规划问题</strong>。可以看下凸二次规划的解法：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/59800326.jpg" width="500px" /></p>

<p>使用任意一款可以解决二次规划的语言包就可以套用上图解决我们的目标问题。这个时候解出来的 $b，w$ 称之为hard-margin，因为没有任何一个点违反我们的限制条件。后面我们会看到一个soft-margin，这个就类似于pocket之于perceptron，可以解决线性不可分的数据集。</p>

<p>对于少量的数据集可以用凸二次规划直接求解，但是数据量一旦增多，求解的速度就成问题。我们可以用拉格朗日乘子法求解原始问题的对偶问题，得到最优解。定义的拉格朗日函数为：</p>

<script type="math/tex; mode=display"> L(b, w, \alpha) = \frac{1}{2}w^Tw + \sum_{n=1}^N \alpha_n(1-y_n(w^T z_n +b)) </script>

<p>这里的 $z_n = \phi(x_n)$ 是为了表示可以对 $x_n$ 做非线性的转换，也就是下一章中提到的kernel function，这里可以直接理解为 $z_n=x_n$。可以这么理解上式：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/6390373.jpg" width="450px" /></p>

<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题，下面我们需要证明：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/47082134.jpg" width="400px" /></p>

<p>假设对于任意的 $\alpha’$，所有的 $\alpha_n’ \ge 0$, 那么</p>

<script type="math/tex; mode=display"> \underset{b,w}{min}(\underset{\alpha_n \ge 0}{max}\ L(b, w, \alpha)) \ge \underset{b,w}{min}\ L(b, w,\alpha’)</script>

<p>因为 $max \ge any$。则对于右式的最优解 $\alpha’$ 有 $best \in any$</p>

<script type="math/tex; mode=display"> \underset{b,w}{min}(\underset{\alpha_n \ge 0}{max}\ L(b, w, \alpha)) \ge \underset{\alpha_n' \ge 0}{max}\underset{b,w}{min}\ L(b, w,\alpha’)</script>

<p>对于大于等于符号来说，这是一个weak duality。如果等号成立则是strong duality，也就是对偶问题和原始问题的最优值相等。需要满足一些限制条件，那就是<a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT条件</a>。</p>

<p>求解对偶问题，首先求 （1）$\underset{b,w}{min}\ L(w, b, \alpha)$ </p>

<script type="math/tex; mode=display"> \nabla_w L(w, b, \alpha) = w - \sum_{i=1}^N \alpha_i y_i z_i = 0 \to w=\sum_{i=1}^N \alpha_i y_i z_i</script>

<script type="math/tex; mode=display">\nabla_b L(w, b, \alpha) = \sum_{i=1}^N \alpha_i y_i = 0 \to \sum_{i=1}^N \alpha_i y_i=0</script>

<p>带入原公式得到</p>

<script type="math/tex; mode=display"> L(w, b, \alpha)= -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_i y_j (z_i \cdot z_j) + \sum_{i=1}^N \alpha_i </script>

<p>接下来求解 （2） $\underset{b,w}{min}\ L(w, b, \alpha)$ 对 $\alpha$ 的极大值，极大值可以变为极小值</p>

<script type="math/tex; mode=display"> \underset{\alpha}{min}\ \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_i y_j (z_i \cdot z_j) - \sum_{i=1}^N \alpha_i</script>

<script type="math/tex; mode=display"> s.t.\ \sum_{i=1}^N \alpha_i y_i=0,\ \alpha_i \ge 0</script>

<p>求解（2）可以用到<a href="http://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2591592.html">SMO</a>（序列最小最优化）算法。现在回过头来看 $\alpha$ 的最优解，我们发现至少会有一个 $\alpha_j &gt; 0$，因为根据KKT条件有complementary slackness：</p>

<script type="math/tex; mode=display"> \alpha_i(y_i(w \cdot z_i +b)-1) = 0,\ i=1,2,...N</script>

<p>如果 $\alpha=0$ 则会导致 $w=0$，对于$\alpha_j &gt; 0$，我们看到会有 $y_j(w \cdot z_j +b)-1=0$。如此，可以定义分离超平面为</p>

<script type="math/tex; mode=display"> \sum_{i=1}^N \alpha_i y_i (z \cdot z_i) + b = 0 </script>

<script type="math/tex; mode=display"> b = y_i - \sum_{i=1}^N \alpha_i y_i (z_i \cdot z_j)</script>

<p>上面的推导表明，这些点 $(z_j, y_j)$ 也就是站在分离超平面的margin上的点，所以说SVM只依赖于边界上的点，它们被称为support vectors，支持向量，这也是SVM的由来。</p>

<h2 id="kernel-support-machine">Kernel Support Machine</h2>

<p>在上文中，我们已经了解到了SVM处理线性可分的情况，而对于非线性的情况，SVM 的处理方法是选择一个核函数 $K(⋅,⋅)$，<u>通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题</u>。</p>

<p><img src="http://my.csdn.net/uploads/201206/02/1338612063_1634.JPG" width="400px" /></p>

<p>例如，对于上面的数据集中两类数据，分别分布为两个圆圈的形状，这样的数据本身就是线性不可分的。理想的分界应该是一个二次曲面，可以写成：</p>

<script type="math/tex; mode=display">a_1X_1 + a_2X_1^2 + a_3X_2 + a_4X_2^2 + a_5X_1X_2 + a_6 = 0</script>

<p>上式其实就是一个五维的空间。一般地对于二次多项式的转换形式，我们有</p>

<script type="math/tex; mode=display"> \phi_2(x) = (1, x_1, x_2....x_d, x_1^2, x_1x_2,...x_1x_d,x_2x_1, x_2^2...x_d^2) </script>

<p>在这个 $O(d^2)$ 高维空间做计算无疑非常困难。幸运的是从上一章推导出的分离超平面中，我们可以看到<strong>分类决策其实只依赖输入和训练样本输入的内积</strong>，也就是说，我们可以直接计算 </p>

<script type="math/tex; mode=display"> \phi_2(x)^T \phi_2(x') = 1 + \sum_{i=1}^dx_ix_i' +  \sum_{i=1}^d\sum_{j=1}^d x_ix_j'x_ix_j' = 1 + x^Tx' + (x^Tx')^2</script>

<p>这里我们直接计算高维转换后的内积，有什么好处呢？注意到计算可以在原来的低维空间$O(d)$中发生，不需要再高维空间中计算。这样，称呼<strong>计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数</strong>。可以对原有空间进行一些线性变换，得到</p>

<script type="math/tex; mode=display"> \phi_2(x) = (1, \sqrt{2\gamma}x_1,....\gamma x_d^2) \to K_2(x, x')=1 + 2\gamma x^Tx' + \gamma^2 (x^Tx')^2 </script>

<p>推广之后，我们得到一般的多项式Kernel：</p>

<script type="math/tex; mode=display">K_n(x, x')= (\xi + \gamma x^Tx')^n,\ \xi>0,\gamma>0</script>

<p>对于不同的 $\xi,\gamma$ SVM是不同的，它们的支持向量也是不同的，因为对于SVM来说变化Kernel就意味着重新定义margin。下面是二次多项式Kernel的一些例子：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/72039507.jpg" width="450px" /></p>

<p>其他常用的核函数还包括高斯核，它可以把原来的低维空间扩展到无线大的高维空间中，它的一般公式为 $K(x,x’)=exp(-\gamma |x-x’|^2),\gamma&gt;0$。高斯核函数也被称为 Radial Basis Function(RBF) kernel。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/61331101.jpg" width="500px" /></p>

<p>满足核函数的充要条件是Mercer’s condition，包括：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/48999470.jpg" width="450px" /></p>

<h2 id="soft-margin-support-vector-machine">Soft-Margin Support Vector Machine</h2>

<p>下面我们来讨论下 Soft-Margin SVM。Soft-Margin可以支持数据集线性不可分的情况，允许一些误分类点的存在，在目标优化函数上会对这些误分类点增加处罚：</p>

<script type="math/tex; mode=display"> \underset{b,w,\xi}{min} \frac{1}{2}w^T w  + C\sum_{n=1}\xi_n</script>

<script type="math/tex; mode=display">s.t.\ y_i(w^T z_n + b) \ge 1-\xi_n,\ \xi_n \ge 0\ for\ all\ n</script>

<p>对于参数 $C$ 而言，它是大margin和误分类点的trade-off，大 $C$ 表示少误分类点，小 $C$ 表示大margin。同样计算拉格朗日对偶问题：</p>

<script type="math/tex; mode=display">L(b, w, \xi, \alpha, \beta) = \frac{1}{2}w^Tw + C\sum_{n=1}\xi_n + \sum_{n=1}^N \alpha_n(1-\xi_n-y_n(w^T z_n +b)) + \sum_{n=1}^N \beta_n (-\xi_n)</script>

<script type="math/tex; mode=display">want\ \underset{\alpha \ge 0, \beta \ge 0}{max}(\underset{b,w,\xi}{min}\ L(b, w, \xi, \alpha, \beta))</script>

<p>和hard-margin一样的计算后可以得到</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/23447671.jpg" width="350px" /></p>

<p>对于soft-margin的complementary slackness有</p>

<script type="math/tex; mode=display"> \alpha_n(1- \xi_n - y_n(w^T z_n +b)) = 0,\ (C-\alpha_n)\xi_n=0</script>

<p>存在以下三种情况：</p>

<ul>
  <li>$\alpha_n=0$: $\xi_n=0$, 在边界之外正确分类的点.</li>
  <li>$0&lt;\alpha_n&lt;C$: $\xi_n=0$，支持向量落在边界上。也正是通过这种情况计算截距 $b$.</li>
  <li>$\alpha_n=C$：这种情况比较复杂，可以有下图表示，支持向量的位置由$\xi_n$决定。$0&lt;\xi_n&lt;1$则分类正确，在间隔边界和分离超平面之间；$\xi_n=1$则在分离超平面上；$\xi_n&gt;1$则位于超平面误分类的一侧.</li>
</ul>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/76271972.jpg" width="300px" /></p>

<h2 id="hinge-loss-function">Hinge Loss Function</h2>

<p>最后我们说明下SVM可以用合页损失函数表示，就是最小化以下目标函数：</p>

<script type="math/tex; mode=display"> \sum_{i=1}^N[1-y_i(w \cdot x_i + b)]_+ + \lambda\|w\|^2 </script>

<p>下标“+”表示，对 $[z]_+ = z,\ z&gt;0; [z]_+ = 0,\ z \le 0$</p>

<p>可以看到 $1-y_i(w \cdot x_i + b)$ 可以写成</p>

<script type="math/tex; mode=display"> y_i(w \cdot x_i + b) \ge 1-\xi_i,\ \xi_i \ge 0, \ i=1,2,...N </script>

<p>也就是soft-margin SVM的限制条件，那么取 $\lambda= 1/2C$ 则有</p>

<script type="math/tex; mode=display"> \underset{b,w}{min} \frac{1}{C}(\frac{1}{2}\|w\|^2 + C\sum_{n=1}\xi_n)</script>

<p>也就是soft-margin SVM的目标优化函数。合页损失函数的形状如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-27/97100573.jpg" width="400px" /></p>

<p>虚线显示的是感知机的损失函数 $[y_i(w \cdot x_i+b)]_+$。这时，当样本点 $(x_i，y_i)$ 被正确分类时，损失是0，否则损失是 $-y_i(w \cdot x_i+b)$。相比之下，合页损失函数不仅要分类正确，而且置信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PCA and SVD]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/26/pca-svd/"/>
    <updated>2016-02-26T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/26/pca-svd</id>
    <content type="html"><![CDATA[<p>PCA即为（Principal Components Analysis）主成分分析，SVD是（Singular value decomposition）奇异值分解。从字面上的理解就可以看出这两个并不是在同一语义层面的东西。之所以把这两个放在一块，一是为了文章的完整性，二是为了说明二者在数据转换（基转换）上的共性。</p>

<!--more-->

<h2 id="principal-components-analysis">Principal Components Analysis</h2>

<p>由于采样的受限，我们的观察值并不能最有效的反应出事物的特征，因此我们希望大而全的收集能收集到的所有数据。但是这里面存在两个问题：噪声和冗余。因此在描述事物的时候，我们希望能够排除这些多余的甚至是错误的数据，得到最简洁，最省力的数据。</p>

<p>那么什么是最简洁，最省力的数据呢？把我们的观察值想象成一个向量空间，排除噪声和冗余后，那么这个空间上的点应该可以用一系列的正交单位向量缩放后的向量和表示。这个就是PCA的目的。</p>

<p>在上面的描述中，有一些很重要的假设：</p>

<ul>
  <li>原有的基是通过线性转化转化为现有空间的基，否则就是Kernel PCA</li>
  <li>现有空间的基是正交的</li>
  <li>每个维度数据的均值和方差是充分统计的。</li>
  <li>数据中方差能够表示数据的重要程度。</li>
</ul>

<p>第一点比较容易理解，其实是做了一些限制，限制潜在最优基的数目并且相信数据集存在线性的连续性，即我们可以用线性的方式内推出独立的数据点。第二点就比较直接，直觉上是合理的并且可以用线性代数的矩阵分解解决。</p>

<p>第三点比较复杂，充分统计的意思是可以用均值和方差完整的描述数据的概率分布。如果方差为0，只用方差完整的描述概率分布的只有高斯分布。也就是说维度上的数据服从高斯分布，如果不服从呢，这就涉及到ICA算法（Independent Component Analysis）。根据中心极限定理，PCA还是比较robust的一种解决方案。</p>

<p>第四点来自信号处理，认为信号具有较大的方差，噪声有较小的方差，信噪比(Signal-to-noise ratio, SNR)就是信号与噪声的方差比，越大越好。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-24/39988407.jpg" width="600px" /></p>

<p>噪声可以用<em>SNR</em>量化表示，那么冗余呢？冗余可以用协方差表示。如果向量 $a$、向量 $b$ 的协方差为0，则代表 $a$ 和 $b$ 完全没有关系，协方差越大则关联的程度越高。<u>这里对所有的数据集都是*mean deviaton form*，即均值为0。</u>则我们可以得到向量 $a$ 和向量 $b$ 的协方差 $\delta_{ab}^2 = \frac{1}{n-1}ab^T$，之所以除以 $n-1$ 是为了得到无偏估计，因为样本中的最后一个值可以通过均值推到出来。</p>

<p>假设原有的输入是一个 $m \times n$ 的矩阵，m是特征维度，n是样本数量，那么它的协方差矩阵为 $S_x = \frac{1}{n-1}XX^T$. $S_x$ 是 $m \times m$ 的矩阵，<strong>表示特征之间的关联程度</strong>。$S_x$ 量化了原有任意两个维度数据之间的关系。那么既然如此，我们希望 $S_x$ 是什么样的？答案就是非对角元素全为0，表示任意维度之间的数据没有冗余，这个过程叫对角化（Diagonalize）得到的协方差矩阵我们成为 $S_y$。</p>

<p>有很多种方法可以实现对角化，PCA选择特征值分解的方法。现在问题定义如下，找到一个正交矩阵 $P$，$Y=PX$, 使得 $S_y = \frac{1}{n-1}YY^T$ 是对角化的矩阵。这时，$P$ 的每排就是 $X$ 的 principal components。这也是PCA名字的由来，$Y$ 是经过矩阵 $P$ 线性转换后的矩阵。$S_y$ 用 $P$ 表示：</p>

<script type="math/tex; mode=display"> S_y = \frac{1}{n-1}YY^T = \frac{1}{n-1}PXX^TP^T = \frac{1}{n-1}PAP^T </script>

<p>这里 $A=XX^T$ 是一个 $m \times m$  的对称矩阵。对称矩阵可以由特征向量构成的正交矩阵表示 $A=EDE^T$，$D$ 是对角矩阵，$E$ 的列向量为 $A$ 的特征向量。如果 $P=E^T$，即 $P$ 的行向量为 $A$ 的特征向量，则有</p>

<script type="math/tex; mode=display">S_y = \frac{1}{n-1}PAP^T = \frac{1}{n-1}(PP^T)D(PP^T) = \frac{1}{n-1}D</script>

<p>可以看到 $P$ 对角化 $S_y$，这就是PCA要求的。下面我们总结下</p>

<ul>
  <li>$X$ 的<strong>主成分</strong>也就是 $XX^T$ 的特征向量，或者 $P$ 的行向量。</li>
  <li>$S_y$ 的第 $i$ 个对角值（特征值）也就是 $X$ 在 $p_i$方向上的方差。</li>
</ul>

<p>所以PCA的计算很简单，就两个步骤</p>

<ol>
  <li>计算dataset的<em>mean deviaton form</em></li>
  <li>计算$XX^T$的特征分解。</li>
</ol>

<p>PCA可以选择最大的几个特征值降维，也可以防止overfitting，但是经过线性变换后拟合的函数就不好理解了。</p>

<h2 id="singular-value-decomposition">Singular value decomposition</h2>

<p>SVD是另外一种基变换的更通用的方法，二者在使用上通常可以互相的替换。SVD和上文中提到的特征值分解都是一种矩阵的对角化分解方法。</p>

<p>假设 $X$ 是任意 $m \times n$ 矩阵，$XX^T$ 是秩为 $r$ 的对称矩阵，我们定义</p>

<ul>
  <li>$(v_1, v_2,…v_r)$ 是 $XX^T$ 的 $n \times 1$ 的特征向量，特征值为 $(\lambda_1, \lambda_2,…\lambda_r)$, $(XX^T)v_i = \lambda_i v_i$。</li>
  <li>$\sigma_i = \sqrt{(n-1) \lambda_i}$ 为正实数，也被成为奇异值。</li>
  <li>$(u_1, u_2,…u_r)$ 是 $m \times 1$ 的正交向量集，$u_i = \frac{1}{\sigma_i}Xv_i$</li>
</ul>

<p>重新组织下第三个定义，有 $Xv_i=\sigma_iu_i$, $U = (u_1, u_2,…u_r)，V = (v_1, v_2,…v_r)$ 都是定义在 $r$ 维空间的正交基。用任意的 $(m-r), (n-r)$ 正交向量补充到 $U, V$ 中，得到 $m$ 和 $n$ 维的 $U、V$，我们有下面用一个矩阵乘法：$XV=U \Sigma$, 其中</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-24/86503163.jpg" width="200px" /></p>

<p>因为 $V$ 是正交的，所以上式又可以写成</p>

<script type="math/tex; mode=display"> X=U \Sigma V^T </script>

<p>这个就是奇异值分解，表示任意的矩阵都可以表示一个正交矩阵，一个对角矩阵和另一个正交矩阵的乘积，或者说是旋转、拉伸和另外一个旋转。其中，$U_{m \times m}$ 是左奇异向量矩阵，$V_{n \times n}$ 是右奇异向量矩阵。</p>

<p>关于奇异值分解的问题最常用的算法分为两大类，QR分解和Jacobi选择，这里就不细说。</p>

<h3 id="pcasvd">PCA和SVD的关系</h3>

<p>通常来说，PCA要求计算协方差矩阵的特征值和特征向量，因为协方差矩阵是对称的，因此是可对角化的，特征向量也是正交的。对于SVD，我们有</p>

<script type="math/tex; mode=display">XX^T=(U\Sigma V)(U\Sigma V)^T = U \Sigma^2 U^T</script>

<p>复习下介绍PCA时我们的到的公式：</p>

<script type="math/tex; mode=display">XX^T = (n-1)P^TS_yP</script>

<p>这时二者的关系就很清晰了，$P、U$ 都是正交矩阵：</p>

<ul>
  <li>
    <p>$XX^T$ 的特征值 $\lambda=\frac{\sigma^2}{n-1}$</p>
  </li>
  <li>
    <p>$\frac{1}{\sqrt{n-1}}X$ 经过SVD分解后的 $U$ 的列向量也正是PCA中的主成分。</p>
  </li>
</ul>

<p>所以我们在PCA的最后一步中可以用SVD或者特征分解。但是SVD在数值上的精确程度会高于特征分解，因为计算 $XX^T$ 可能会带来一些精度的损失。</p>

<h3 id="svd">SVD的说明</h3>

<p>可以对SVD进行一些有趣的变换:</p>

<script type="math/tex; mode=display"> U^TX = \Sigma V^T </script>

<script type="math/tex; mode=display"> U^TX = Z </script>

<p>定义 $Z=\Sigma V^T$ 可以看到 $U^T$ 是改变了 $X$ 的基，使其变成 $Z$, 这里是改变了 $X$ 的列向量。同理对于 $V$ 来说，$V^TX^T = U^T \Sigma$ 这是改变 $X$ 的行向量。而 $\Sigma$ 则表示对某些维度的缩放，之所以说某些维度是 $\Sigma$ 中有为0的奇异值，非0奇异值的个数也就是矩阵的秩的大小。</p>

<p>$\Sigma$中奇异值的大小和特征值有关系，表示特征的重要程度，因此我们可以令奇异值较小的数0，这样重新计算 $X$ 的时候也就进行降噪和去冗余。</p>

<p>总的来说，无论是特征分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。通过特征分解和奇异值分解我们可以更加明白这些矩阵信息背后的真实含义，简化我们对矩阵的认识。</p>

<p>关于更多对SVD物理意义的说明可以参考<a href="http://www.ams.org/samplings/feature-column/fcarc-svd">http://www.ams.org/samplings/feature-column/fcarc-svd</a>.</p>

<h2 id="limits-and-extensions-of-pca">Limits And Extensions of PCA</h2>

<p>可以看到PCA是无参数分析的，所以只需要做出上文提到的假设，无需对参数进行训练和选择就可以得到结果。但是也正是上述假设所限，如果一个人正好知道数据的一些先验知识，那么他也无法通过这些先验知识得到更好的分析结果，这个时候如果能够将这些先验知识融入有参数的算法中会得到更好的结果。</p>

<p>如果这个先验知识表示需要做些数据的非线性转化（kernel transformation），那么这样的参数算法就是<code>kernel PCA</code>。</p>

<p>有时候需要作出如下假设，主成分不必正交，特征数据的分布也不是高斯分布。那么可以用<code>Idependent Component Analysis</code>解决，它和PCA有同意的目的，降噪和去冗余。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ensemble Methods]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/18/ensemble/"/>
    <updated>2016-02-18T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/18/ensemble</id>
    <content type="html"><![CDATA[<p>Ensemble<code>|ɒnˈsɒmbl|</code> Methods 称为集成方法，它还有其他类似的名字，meta-algorithm、aggregation model，这些都代表这同一个意思，就是不同弱分类器的组合成一个强分类器。这里的弱分类器要比随机猜测的结果好，错误率小于50%；弱分类器可以是决策树、逻辑回归、朴素贝叶斯等算法。Ensemble的形式有很多种：</p>

<ul>
  <li>不同算法的集成;</li>
  <li>同一算法在不同设置下的集成;</li>
  <li>数据集不同部分分配给不同分类器之后的集成。</li>
</ul>

<p>那么这多个弱分类器又是如何组合的呢，下面给出一个big picture，后面的文章也是对其的阐述。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-18/38802537.jpg" width="450px" /></p>

<!--more-->

<p>弱分类的组合可以是linear或者stacking的，linear又可以是uniform或者non-uniform.
stacking或者stacked generalization的大意是non-linear combining. 通常stacking的组合算法有LR，GBM, KNN, NN, RF 和 ET，可以参考<a href="http://mlwave.com/kaggle-ensembling-guide/">http://mlwave.com/kaggle-ensembling-guide/</a>. 作者Wolpert是这样形容的：</p>

<blockquote>
  <p>stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn’t duplicated in what the other generalizer’s have to say), the better the resultant stacked generalization. </p>
</blockquote>

<p>那么为什么要把这些分类器进行组合呢？组合之后是否能获得更好的效果？可以看下下面的例子。</p>

<p>假设我们有10个samples的测试集，正确的结果是<code>1111111111</code>。现在有三个分类器，它们只有70%的正确率，那么三个分类器进行majority vote，可以得到如下的正确率：</p>

<script type="math/tex; mode=display">0.7 * 0.7 * 0.7 + \binom{3}{2}0.7 * 0.7 * 0.3 = 0.784</script>

<p>也就是由原来的70%提升到了78%。正确率会随着分类器的增加而增加。5个分类器的正确率大约为83%。这个在统计学上就是“Wisdom of Crowds”。但是这个结果提高的前提在于sample的diversity，也就是减少sample之间的correlation。例如：</p>

<pre><code>1111111100 = 80% accuracy
1111111100 = 80% accuracy
1011111100 = 70% accuracy
</code></pre>

<p>在这个例子中得到<code>1111111100</code>还是只有80%的正确率，而</p>

<pre><code>1111111100 = 80% accuracy
0111011101 = 70% accuracy
1000101111 = 60% accuracy
</code></pre>

<p>经过Ensemle就可以得到<code>1111111101</code>，90%的正确率。</p>

<p>那么如何证明多个组合会比单个的结果好呢，可以用Uniform Linear的组合进行下面的理论描述。</p>

<script type="math/tex; mode=display"> Let\ G(x) = \frac{1}{T}\sum_{t=1}^T g_t(x)</script>

<script type="math/tex; mode=display">   avg((g_t(x) - f(x))^2) = avg(g_t^2 - 2g_tf + f^2)</script>

<script type="math/tex; mode=display">   =avg(g_t^2) - G^2 + (G-f)^2</script>

<script type="math/tex; mode=display">   =avg((g_t-G)^2) + (G-f)^2</script>

<script type="math/tex; mode=display"> avg(E(g_t)) = avg((g_t-G)^2) + E(G) \ge E(G) </script>

<p>更一般的，有如下的预测模型$\hat{F}(x)^T = [\hat{f}_1(x), \hat{f}_2(x)…\hat{f}_M(x)]$, 用最小二乘法寻找线性最小值</p>

<script type="math/tex; mode=display"> \hat{w} = argmin_w E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 </script>

<script type="math/tex; mode=display"> \hat{w} = E[\hat{F}(x)\hat{F}(x)^T]^{-1}E[\hat{F}(x)Y] </script>

<script type="math/tex; mode=display"> E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 \le E[Y-\hat{f}_m(x)]^2 \forall m</script>

<h2 id="bagging">Bagging</h2>

<p>Bagging或者bootstrap aggregation是上文提到的Uniform Linear aggregation。其中用到bootstrapping，这是是一种resample的方法，定义如下</p>

<blockquote>
  <p>re-sample N examples from original sample <strong>uniformly with replacement</strong> – can also use arbitrary N’ instead of original N</p>
</blockquote>

<p>有training set $Z$, 对每个bootstrap sample $Z^{*b}, b=1,2…B$，bagging定义如下：</p>

<script type="math/tex; mode=display"> \hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x) </script>

<h2 id="adaboost">AdaBoost</h2>

<p>AdaBoost或者Adaptive Boosting中只要弱分类器的正确率优于随机选择，那么通过AdaBoost就会得到非常好的结果。它是一种Boosting方法，所谓Boosting就是改变训练数据的概率分布（权值分布）针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。对应于上面的non-uniform linear model。</p>

<p>AdaBoost强调对错误分类的反复学习，但是最后对错误率较高的分类器赋予低权重。每轮学习中都会重新对分类器赋予不同的权重，这是为了得到更多关于数据的不同假设。如何得到更多的不同假设呢？</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-19/91708853.jpg" width="300px" /></p>

<p>那么我们希望在$t+1$迭代学习的时候，$t$轮的结果尽可能的随机，即有错误率</p>

<script type="math/tex; mode=display"> \epsilon_t = \frac{\sum_{n=1}^N u_n^{(t+1)}I(y_n \ne g_t(x_n))}{\sum_{n=1}^N u_n^{(t+1)}} = \frac{1}{2} </script>

<p>于是可以 multiply incorrect $\propto (1 - \epsilon_t)$; multiply correct $\propto \epsilon_t$</p>

<p>定义scalling factor $\blacklozenge_t = \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}}$</p>

<script type="math/tex; mode=display"> incorrect \gets incorrect \cdot \blacklozenge_t \\ correct \gets correct \div \blacklozenge_t </script>

<p>最后对scalling factor取自然对数作为权值 $\alpha_t$ 将弱分类器线性组合在一块，取自然对数的逻辑如下：</p>

<script type="math/tex; mode=display"> \epsilon_t = 1/2 => \blacklozenge_t = 1 => \alpha_t = 0 \\
\epsilon_t = 0 => \blacklozenge_t = \infty => \alpha_t = \infty </script>

<p>最后伪代码为</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/40495628.jpg" width="450px" /></p>

<p>比较常见的弱分类器是Decision Stump，和AdaBoost组成<code>AdaBoost-Stump</code>，具有efficient feature selection and efficiency的特点。</p>

<h2 id="random-forest">Random Forest</h2>

<p>Bagging是通过平均带有噪声但是近似无偏的模型来减少variance，而对于足够深的Decision Tree来说，它的bias可以非常少，但是variance非常高。所以自然的想将二者结合，综合他们的优点。Bagged Tree并不能减少bias，但是可以有效的减少variance。这个Boosting正好相反，Boosting通过自适应的变化树的样子来减少bias。Random Forest就是Bagging + Decision Tree(C&amp;RT)。EST给出RF的本质：</p>

<blockquote>
  <p>The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much.</p>
</blockquote>

<p>我们了解到增加hypothesis diversity可以提高最终结果的表现，那么Random Forest就将这种Random性发挥到极致，得到多样的hypothesis。为了增加随机性，可以做了以下的措施：</p>

<ul>
  <li>re-sample new feature subspace for each b(x) in C&amp;RT, 记得bagging进行data randomness for diversity, 那么在RF中是feature randomness for diversity</li>
  <li>random low-dimensional projections for each b(x) in C&amp;RT, 这就是对feature进行投影，进行feature combination，在特征空间中随机选择若干特征组投影到若干个方向上。</li>
</ul>

<p>伪代码如下（只用了第一个Randomness）：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/2671550.jpg" width="500px" /></p>

<p>RF还有的特点是训练的时候可以自带<strong>Model Selection和Feature Selection</strong>。那么RF是如何做到的？</p>

<h3 id="model-selection">Model Selection</h3>
<p>在RF中使用bagging时，每个bootstrap sample都会有一定的概率没有选择原来sample中的一些数据，这些数据就是out-of-bag (OOB) Samples. 在一个RF中，某个Decision Tree训练没有用到数据 $(x_n, y_n)$的概率为：$(1 - \frac{1}{N}) ^ N$，当N无限大的时候，接近 $\frac{1}{e}$.</p>

<p>可以用OOB来validate G, $E_{oob}G = \frac{1}{N} \sum_{n=1}^N err(y_n, G_n^-(x_n))$, $G_n^-$ 表示 $x_n$在OOB中的Decision Tree。这样可以用 $E_{oob}$ 对bagging/RF进行self-validation。</p>

<p>这有什么用呢，当然是进行模型选择了，可以使用 $E_{oob}$ 进行RF的参数选择，例如feature subspace。下图表示使用Validation中进行的模型选择，可以看到RF中少了re-training的步骤。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/30059076.jpg" width="250px" /></p>

<h3 id="feature-selection">Feature Selection</h3>

<p>在模型训练的时候通常希望能够去掉多余的、无关的特征，这样能够得到的好处有：</p>

<ul>
  <li><strong>efficiency</strong>: simpler hypothesis and shorter prediction time</li>
  <li><strong>generalization</strong>: feature noise removed</li>
  <li><strong>interpretability</strong></li>
</ul>

<p>通常可以通过特征的重要性来进行特征的选择，对于线性模型来说就是 $w_i$ 的绝对值，也就是特征对于最终结果的影响程度。那么在非线性的RF模型中呢？</p>

<p>所用的方法就是random test，例如特征 $i$ 被选择，那么在特征 $i$ 的数据集中加入随机变量重新训练则会降低模型正确率，而对于不重要的特征，怎么改变数据集当然对模型没有什么影响。</p>

<p>RF中使用的random test就是一种常用的统计学工具permutation test，也就是将特征 $i$ 的数据做重新排列。数据表达也就是：</p>

<script type="math/tex; mode=display"> importance(i) = performance(\mathcal{D}) - performance(\mathcal{D}^{(p)}) </script>

<script type="math/tex; mode=display"> \mathcal{D}^{(p)}\ is\ \mathcal{D}\ with\ \{x_{n,i}\}\ replaced\ by\ permuted\ \{x_{n,i}\}_{n=1}^N </script>

<p>$performance(\mathcal{D}^{(p)})$需要重新训练和评估，那么有什么办法可以避免呢？我们可以重新定义 $importance(i) = E_{oob}(G^-) - E_{oob}^{(p)}(G^-)$，表达式后项就是一个permuted OOB value。</p>

<p>具体过程如下，当 $b$ 个树生成的时候，记录OOB评估的 $G^-$ 的正确率，然后OOB sample中的特征 $i$ 数据重新随机排列后再次评估的 $G^-$ 的正确率，二者相减得到特征 $i$的重要性。</p>

<p>RF的缺点是，如果随机过程表现的不稳定，则需要很多的Decision Tree来支持。所以需要重新检查 $G$ 的稳定性来确保有足够多的树。</p>

<h2 id="gradient-boosted-decision-tree">Gradient Boosted Decision Tree</h2>

<p>回忆下假设AdaBoost的分类器输出是binary的，则权值迭代可以转化为</p>

<script type="math/tex; mode=display"> u_n^{t+1} = \begin{cases}{u_n^t \cdot \blacklozenge_t\ if\ incorrect}\\{u_n^t \div \blacklozenge_t\ if\ correct}\end{cases} = u_n^t \cdot \blacklozenge_t^{-y_ng_t(x_n)} = u_n^t \cdot exp(-y_n\alpha_tg_t(x_n))</script>

<script type="math/tex; mode=display"> u_n^{(T+1)} = u_n^{(1)} \cdot \prod_{t=1}^Texp(-y_n\alpha_tg_t(x_n)) = \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) </script>

<p>在AdaBoost中 $G(x) = sign(\sum_{t=1}^T\alpha_tg_t(x_n))$ 括号中的表达式也被成为voting score。现在我们想要 $y_n(voting\ score)$ 为正且越大越好，也就是让 $u_n^{(T+1)}$ 越小越好。</p>

<p>所以AdaBoost的过程也就是让 $\sum_{n=1}^N u_n^{(t)}$ 减小，也就是最小化</p>

<script type="math/tex; mode=display"> \sum_{n=1}^Nu_n^{(T+1)} =  \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) </script>

<p>注意到上式是一个损失函数，Exponential Loss Function，之所以用指数损失函数是为了后续的计算方便，可以对比下不同的损失函数。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-22/70940842.jpg" width="400x" /></p>

<p>注意到在gradient descent中第 $t$ 次迭代有：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/31496263.jpg" width="400px" /></p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/14307315.jpg" width="400px" /></p>

<p>所以找到一个好的 $h(function\ direction)$ 函数也就是最小化 $\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$。对于二元分类，$y_n, h(x_n) \in {-1, +1}$，有</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/93309458.jpg" width="400px" /></p>

<p>也就是每次迭代都需要最小化其中的hypothesis $E_{in}^{u^{(t)}}(h)$，那么谁最小化 $E_{in}^{u^{(t)}}(h)$呢，当然是AdaBoost中的reweighted sample所对应的 $g_t$ 了。</p>

<p>所以现在需要优化 $\eta_t$ 得到梯度下降方向最佳步长，原来的 $\hat{E}_{ADA}$ 变为 $(\sum_{n=1}^N u_n^{(t)}) \cdot ((1-\epsilon_t)exp(-\eta) + \epsilon_t exp(+\eta))$。微分后容易得到 $\eta_t=ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}} = \alpha_t$。这和我们上面得到的scaling Factor是一致的。</p>

<p>所以AdaBoost是steepest descent with approximate functional gradient. 我们总结下，AdaBoost的数学表达式：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/16682553.jpg" width="400px" /></p>

<p>Gradient Boost就是把二元分类的假设推广到任意的假设并且损失函数也可以任意的。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/85486720.jpg" width="400px" /></p>

<p>当选择平方损失函数时，有如下的可以看到</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/61222916.jpg" width="460px" /></p>

<p>现在需要对 $h$ 加一些限制，否则 $h(x_n) = -\infty \cdot (s_n - y_n)$</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/34172769.jpg" width="400px" /></p>

<p>最优 $g_t = h$ 就是 $(x_n, y_n-s_n)$ 上的最小二乘回归函数，$y_n - s_n$就是残差。于是原来的Gradient Boost表达式变成：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/37566404.jpg" width="400px" /></p>

<p>最小化 $\eta$ 就是 $(g_t\ transformed\ input,\ residual)$ 上的单变量线性回归。所以GradientBoost for regression 的 $\alpha_t = optimal\ \eta\ by\ g_t\ transformed\ linear\ regression$.</p>

<p>Gradient Boosted Decision Tree也就是使用回归算法为Decision Tree。伪代码如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/38067174.jpg" width="500px" /></p>

<p>总结下Ensemble Methods的有点：</p>

<ul>
  <li>cure underfitting, 通过feature transform加强$G(x)$的Bias。</li>
  <li>cure overfitting, 通过多样化假设的合并达到regularization的目的，减少$G(x)$的variance。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logistic Regression and Linear Discriminant Analysis]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/17/lr-and-lda/"/>
    <updated>2016-02-17T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/17/lr-and-lda</id>
    <content type="html"><![CDATA[<p>在回归方法中，我们找一个超平面作为类与类之间的decision boundary。回归方法为每个分类建立一个判别函数 $\sigma_k (x)$, 对任意的 $x$，选出得到最大值的判别函数最为归属类。对于后验概率模型 $Pr(G = k|X = x)$ 也是使用同样的方法。对于$\sigma_k (x) 和 Pr(G = k|X = x)$ 来说，只要它们是线性的，那么得到的decision boundary也是线性的。</p>

<p>虽然无法直接使得 $\sigma_k (x) 和 Pr(G = k|X = x)$ 是线性的，但是如果有一个单调的转化函数能够使得是线性的，那么我们也可以得到一个超平面分割数据点。</p>

<p>Logistic Regression 和 Linear Discriminant Analysis就是基于这样的需求构造的模型。</p>

<!--more-->

<h2 id="logistic-regression">Logistic Regression</h2>

<p>逻辑回归的模型也是源于通过 $x$ 的线性函数建立 $K$ 个类的后验概率模型，同时保证它们在[0,1]之间以及和为1。</p>

<script type="math/tex; mode=display"> log {Pr(G = 1 \vert X = x) \over Pr(G = K \vert X = x)} = \beta_{10} + \beta_1^Tx </script>

<script type="math/tex; mode=display"> log {Pr(G = 2 \vert X = x) \over Pr(G = K \vert X = x)} = \beta_{20} + \beta_2^Tx </script>

<script type="math/tex; mode=display"> log {Pr(G = K-1 \vert X = x) \over Pr(G = K \vert X = x)} = \beta_{(K-1)0} + \beta_{K-1}^Tx </script>

<p>这个转换函数称为 logit transformation, 概率称为 log-odds。得到</p>

<script type="math/tex; mode=display"> Pr(G = k \vert X = x) = {exp(\beta_{k0} + \beta_k^Tx) \over {1 + \sum_{l=1}^{K-1} exp(\beta_{l0} + \beta_l^Tx) }} </script>

<script type="math/tex; mode=display"> Pr(G = K \vert X = x) = {1 \over {1 + \sum_{l=1}^{K-1} exp(\beta_{l0} + \beta_l^Tx) }} </script>

<p>当 $K=2$ 的时候，输出设为0/1, 输出结果就是一个伯努利过程。可以使用maximum likelihood来对模型进行参数估计。</p>

<p>假设 $y_i = 1\ when\ g_i = 1,\ y_i = 0\ when\ g_i = 2$, 那么不妨设 $p(x_i; \beta) = Pr(G = 1|X = x) = {\beta^Tx \over {1 + exp(\beta^Tx)}}$，这时的 $p(x_i; \beta) = {1 \over {1 + exp(-\beta^Tx)}}$, 也成为<code>sigmoid function</code>。</p>

<p>log-likelihood得到结果为</p>

<script type="math/tex; mode=display"> \ell(\beta) = \sum {y_i log p(x_i; \beta) + (1 - y_i)log(1-p(x_i, \beta))} 
        = \sum {y_i\beta^Tx_i - log(1 + exp(\beta^Tx_i))}, </script>

<script type="math/tex; mode=display">其中\beta = \{\beta_{10}, \beta_1\}, x_i$ 的第一个元素为截距1.</script>

<script type="math/tex; mode=display"> 求导后得到\frac{\partial \ell (\beta)}{\partial \beta} = \sum_{i=1}^N {x_i(y_i - p(x_i; \beta))} = 0 </script>

<p>此时，可以使用梯度下降法或者牛顿法求解 $\beta$。下面使用牛顿法求解。</p>

<p>$$ \frac{\partial^2 \ell (\beta)}{\partial \beta \partial \beta^T} = 
    - \sum_{i=1}^N  {x_i x_i^T p(x_i; \beta)(1 - p(x_i; \beta))} $$
于是牛顿迭代即为
<script type="math/tex"> \beta^{new} = \beta^{old} - (\frac{\partial^2 \ell (\beta)}{\partial \beta \partial \beta^T})^{-1} \frac{\partial \ell (\beta)}{\partial \beta}, 其中所有的倒数都是在\beta^{old}的时候计算的 </script></p>

<p>下面说明这个问题就是加权最小二乘问题，可以变换得到
<script type="math/tex">\frac{\partial \ell (\beta)}{\partial \beta} = X^T(Y-P),\ \frac{\partial^2 \ell (\beta)}{\partial \beta \partial \beta^T} = -X^TWX </script>
其中 $Y$ 是 $y_i$的向量，$X$ 为 $N * (p+1)$ 的矩阵，$W$ 是一个 $N*N$ 的对角矩阵，元素为 $p(x_i; \beta^{old})(1 - p(x_i; \beta^{old}))$</p>

<p>可以得到 <script type="math/tex">\beta^{new} = (X^TWX)^{-1}X^TWz,\ z = X\beta^{old} + W^{-1}(Y-P)</script>, 这个表达式得到的就是weighted least squares step. $z$ 称为 response，或者说是 <em>adjusted response</em>。每个迭代$p$都会变，所以$W 和 z$也都会变，可以用<em>iteratively reweighted least squares</em>或者IRLS算法来计算，每个迭代就是解决一个weighted least squares问题：</p>

<script type="math/tex; mode=display">\beta^{new} \gets arg min_\beta (z - X\beta)^TW(z - X\beta)</script>

<p><em>注：weighted linear least squares</em></p>

<script type="math/tex; mode=display"> arg min_\beta \sum_{i=1}^m w_i \|y_i - \sum_{j=1}^n x_{ij}\beta_j\|^2 = arg min_\beta \vert W^{1 \over 2}(Y - X\beta) \vert ^2 </script>

<script type="math/tex; mode=display"> \hat{\beta} = (X^TWX)^{-1}X^TWY </script>

<h2 id="linear-discriminant-analysis">Linear Discriminant Analysis</h2>

<p>接下来我们给出另外一个模型，它的后验概率的logit也同样是一个线性模型。</p>

<p>假设 $f_k(x)$ 是类 $G=k$ 的输入的条件密度函数，$\pi_k$ 是类 $k$ 的先验概率，有 $\sum_{k=1}^K \pi_k = 1$。那么依据贝叶斯公式得到</p>

<script type="math/tex; mode=display"> Pr(G=k \vert X=x) = \frac{f_k(x) \pi_k}{\sum_{i=1}^K f_i(x) \pi_i} </script>

<p>假设每个类服从multivariate Guassian分布， 那么</p>

<script type="math/tex; mode=display">f_k(x) = \frac{e^{-1/2(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}}{(2\pi)^{p/2} \vert \Sigma_k \vert ^{1/2}}</script>

<p>当每个类都有一个相同的协方差矩阵 $\Sigma$ 的时候，我们有以下推导</p>

<script type="math/tex; mode=display"> log {Pr(G = k \vert X = x) \over Pr(G = \ell \vert X = x)} = log{\pi_k \over \pi_\ell} - \frac{1}{2}(\mu_k + \mu_\ell)^T\Sigma^{-1}(\mu_k - \mu_\ell) + x^T\Sigma^{-1}(\mu_k - \mu_\ell) </script>

<p>注意到这个式子也就是 $\alpha_{k0} + \alpha_k^Tx$，也就是和logistic regression一样的模型。</p>

<p>并且这个式子可以推出线性判别函数为 </p>

<script type="math/tex; mode=display"> \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log\pi_k</script>

<script type="math/tex; mode=display"> G(x) = argmax_k \delta_k(x) </script>

<p>实际上，高斯分布的参数可以从训练集中估计：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-16/18265816.jpg" width="400px" /></p>

<p>可以看出来LDA做了一下的假设：</p>

<ul>
  <li>类的分布是高斯函数</li>
  <li>每个类都有一个共同的协方差矩阵</li>
</ul>

<p>看起来LDA和logistic regression模型是一样的，它们的区别是对线性参数估计的方法不同。
$X$ 和 $G$ 的联合概率如下</p>

<script type="math/tex; mode=display"> Pr(X, G=k) = Pr(X)Pr(G=k \vert X) </script>

<p>对于LDA和logistic regression，公式的后半部分都是一样的。LR模型也是忽略了前半部分的边际概率，直接对条件概率进行最大似然估计。但是LDA的参数估计是基于整个联合概率分布的</p>

<p><script type="math/tex"> Pr(X, G=k) = \phi(X; \mu_k, \Sigma)\pi_k, \phi是高斯密度函数 </script>
<script type="math/tex"> Pr(X) = \sum_{k=1}^K\phi(X; \mu_k, \Sigma)\pi_k</script></p>

<p>这个边际概率有什么用呢，总的来说是提供参数估计的更多信息，减少参数估计的方差。但是在LDA中，由于outliers会对协方差矩阵做出一定贡献，所以LDA对outliers会比较敏感。如果我们忽略这些假设，而Input确实是高斯分布的，那么根据Efrom的论文，会有”in the worst case ignoring this marginal part of the likelihood constitutes a loss of efficiency of about 30% asymptotically in the error rate”.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Descent and Newton Method]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/15/gradient-descent-and-newton-method/"/>
    <updated>2016-02-15T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/15/gradient-descent-and-newton-method</id>
    <content type="html"><![CDATA[<p>梯度下降和牛顿法都是最优化算法，二者都是求解无约束优化问题的方法，通过递归地逼近最优值来达到求解值。区别在于梯度下降是一阶收敛，而牛顿法是二阶收敛的，所以牛顿法通常会更快，因为牛顿法是用一个二次曲面去拟合当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面。wiki上有张图形象地说明了这个问题：</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/da/Newton_optimization_vs_grad_descent.svg" width="200px" /></p>

<p>下面给出两种方法的具体推导。</p>

<!--more-->

<h2 id="gradient-descent">Gradient Descent</h2>

<p>假设 $f(x)$ 是 $R^n$ 上具有一阶连续偏导数的函数，要求解无约束最优化问题 $min f(x)$.</p>

<p>由于 $f(x)$ 具有一阶连续偏导数，$k$ 次迭代后在 $x^{(k)}$ 附近进行一阶泰勒展开：</p>

<script type="math/tex; mode=display"> f(x) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x - x^{(k)}) </script>

<p>第 $k+1$ 次迭代值 $x^{(k+1)} \gets x^{(k)} - \lambda \nabla f(x^{(k)})$, 其中 $-\nabla f(x^{(k)})$是负梯度方向，$\lambda$是步长。</p>

<p>在上述公式中，$\lambda$ 的每次迭代都可以由一维搜索得到结果，这时的梯度搜索方法叫<code>Exact line search</code>。它形成的搜索路径很有意思，相邻的搜索路径是正交的，形状是 zig-zagging，如图：</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/db/Gradient_ascent_%28contour%29.png" width="300px" /></p>

<p>很容易证明：
<script type="math/tex"> \varphi(\lambda) = f(x^{(k)}) + \lambda d^{(k)}, d^{(k)} = -\nabla f(x^{(k)}) </script>
为求出从 $x^{(k)}$ 出发沿着负梯度方向的极小值，令
<script type="math/tex"> \varphi'(\lambda) = \nabla f(x^{(k)} + \lambda d^{(k)})^T d^{(k)} = 0</script>
<script type="math/tex">-\nabla f(x^{(k+1)})^T -\nabla f(x^{(k)}) = 0 </script></p>

<p>上述表明 $d^{(k)}$ 与 $d^{(k+1)}$ 正交，搜索路径是锯齿形状的，当接近极小值点的时候，每次迭代移动的步长很小，这样影响了收敛速度。</p>

<p>大多数的梯度搜索方法代用<code>inexact line search</code>，这种方法使用更加的普遍，它不要求每次迭代得到准确的步长值，而是采用估计值。有种搜索方法叫<code>backtracking line search</code>，它依赖两个常量：$\alpha, \beta$, </p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-15/4737593.jpg" width="600px" /></p>

<h2 id="newton-method">Newton Method</h2>

<p>牛顿法用迭代点的梯度和二阶导数对目标函数进行二次逼近，把二次函数的极小点作为新的迭代点，不断重复此过程，直到找到最优点。</p>

<p>假设 $f(x)$ 是 $R^n$ 上具有二阶连续偏导数的函数，要求解无约束最优化问题 $min f(x)$.</p>

<p>由于 $f(x)$ 具有二阶连续偏导数，$k$ 次迭代后在 $x^{(k)}$ 附近进行二阶泰勒展开：</p>

<script type="math/tex; mode=display"> f(x) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x - x^{(k)}) + 1/2 (x - x^{(k)})^T \nabla^2 f(x^{(k)}) (x - x^{(k)})</script>

<p>其中 $\nabla^2 f(x^{(k)})$ 是 $f(x)$ 在 $x^{(k)}$ 处的Hesse矩阵，为了求极值，对二阶泰勒公式求导，得到</p>

<script type="math/tex; mode=display"> \nabla f(x) = \nabla f(x^{(k)}) + \nabla^2 f(x^{(k)})(x - x^{(k)}) = 0 </script>

<script type="math/tex; mode=display"> x^{(k+1)} \gets x^{(k)} - \nabla^2 f(x^{(k)})^{-1} \nabla f(x^{(k)}) </script>

<p>其中我们假设Hesse矩阵是可逆的，并且对于正定的Hesse矩阵，我们可以确定是迭代方向是下降的，因为 $-\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) &lt; 0$, $-\nabla^2 f(x)^{-1} \nabla f(x)$就被称为 <em>Newton step</em>. 算法如下：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-15/19107554.jpg" width="600px" /></p>

<h3 id="quasi-newton-methods">Quasi-Newton Methods</h3>

<p>Quasi-Newton即拟牛顿法，这是一种“模拟“的牛顿法，它模拟牛顿法中的搜索方向的生成方式。那么为什么要模拟呢？</p>

<p>在牛顿法中，有如下的缺点：</p>

<ul>
  <li>可能出现Hesse矩阵奇异的情形，因此不能确定后继点；</li>
  <li>即使矩阵非奇异，也未必正定，因而牛顿方向不一定是下降方向</li>
  <li>需要计算Hesse矩阵的逆矩阵，计算比较复杂。</li>
</ul>

<p>我们可以使用另外一个n阶矩阵 $G_k$ 来代替，并且需要确保 $G_k$ 的正定。</p>

<p>在牛顿法中，我们令 $\nabla f(x)$ 中 $x$ 为 $x^{(k+1)}$，得到
<script type="math/tex"> g_{k+1} - g_k = H_k (x^{(k+1)} - x^{k}), 其中 g_k = \nabla f(x^{(k)}), H_k = \nabla^2 f(x^{(k)})</script></p>

<p>记 $y_k = g_{k+1} - g_k, \delta_k = x^{(k+1)} - x^{k}$, 则有
<script type="math/tex">y_k = H_k \delta_k 或者 H_k^{-1} y_k = \delta_k </script></p>

<p>拟牛顿法将 $G_k$ 作为 $H_k^{-1}$ 的近似，要求矩阵 $G_k$ 同样满足，每次迭代都是正定，并且 $G_{k+1} y_k = \delta_k$ . 按照拟牛顿条件，每次迭代中可以选择更新矩阵 $G_{k+1} = G_k + \nabla G_k$ . 由此延伸出来三种算法</p>

<ol>
  <li>DFP算法使用 $G_k$ 逼近Hesse矩阵的逆矩阵。</li>
  <li>BFGS算法使用 $B_k$ 逼近Hesse矩阵。</li>
  <li>Broyden类算法。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some Pre-Concepts in Machine Learning]]></title>
    <link href="http://billowkiller.github.io/blog/2016/02/13/concepts-ml/"/>
    <updated>2016-02-13T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/02/13/concepts-ml</id>
    <content type="html"><![CDATA[<p>几个容易模糊的机器学习前置概念。做下记录，包括classifier, Hypothesis, Model等。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-21/78335003.jpg" width="400px" /></p>

<!--more-->

<p>Essentially, the terms “classifier” and “model” are synonymous in certain contexts; however, sometimes people refer to “classifier” as the learning algorithm that learns the model from the training data. To makes things more tractable, let’s define some of the key terminology:</p>

<ul>
  <li>
    <p><code>Training sample</code>: A training sample is a data point x in an available training set that we use for tackling a predictive modeling task. For example, if we are interested in classifying emails, one email in our dataset would be one training sample. Sometimes, people also use the synonymous terms training instance ortraining example.</p>
  </li>
  <li>
    <p><code>Target function</code>: In predictive modeling, we are typically interested in modeling a particular process; we want to learn or approximate a particular function that, for example, let’s us distinguish spam from non-spam email. The target function f(x) = y is the true function f that we want to model.</p>
  </li>
  <li>
    <p><code>Hypothesis</code>: A hypothesis is a certain function that we believe (or hope) is similar to the true function, the target function that we want to model. In context of email spam classification, it would be the rule we came up with that allows us to separate spam from non-spam emails.</p>
  </li>
  <li>
    <p><code>Model</code>: In machine learning field, the terms hypothesis and model are often used interchangeably. In other sciences, they can have different meanings, i.e., the hypothesis would be the “educated guess” by the scientist, and the model would be the manifestation of this guess that can be used to test the hypothesis.</p>
  </li>
  <li>
    <p><code>Learning algorithm</code>: Again, our goal is to find or approximate the target function, and the learning algorithm is a set of instructions that tries to model the target function using our training dataset. A learning algorithm comes with ahypothesis space, the set of possible hypotheses it can come up with in order to model the unknown target function by formulating the final hypothesis</p>
  </li>
  <li>
    <p><code>Classifier</code>: A classifier is a special case of a hypothesis (nowadays, often learned by a machine learning algorithm). A classifier is a hypothesis or discrete-valued function that is used to assign (categorical) class labels to particular data points. In the email classification example, this classifier could be a hypothesis for labeling emails as spam or non-spam. However, a hypothesis must not necessarily be synonymous to a classifier. In a different application, ourhypothesis could be a function for mapping study time and educational backgrounds of students to their future SAT scores.</p>
  </li>
</ul>

<p>So, we can say that a <code>classifier</code> is a special case of a <code>hypothesis</code> or <code>model</code>: a classifier is a function that assigns a class label to a data point.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Extract-Transform-Load Application Scenarios]]></title>
    <link href="http://billowkiller.github.io/blog/2016/01/13/etl/"/>
    <updated>2016-01-13T14:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2016/01/13/etl</id>
    <content type="html"><![CDATA[<h1 id="etl">实时流ETL应用场景</h1>

<p>现有的企业级数据量在不断增大，用户也在寻求大数据解决方案来处理这些日益增长的数据。那么什么是大数据处理的架构呢。Cloudera总结的很好，大数据架构是建立在一系列开发可靠、可扩张、完整的自动化data pipeline上，下面的一张图给了很好的解释：</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/11725472.jpg" width="500px" /></p>

<!--more-->

<p>data pipeline目的在于获取数据并能够发掘其中的价值。数据工程师决定数据从何处来，如何进入数据处理层，以及如何处理，如何存储和进一步展示。当然还需要包括必不可少的集群设计、系统调优等。上图中后端的分析工具通常为BI展示或其他分析工具，中间的处理通常由Spark、Hadoop进行，前端的数据获取包括批量和实时的两种。</p>

<p>在BMR中，我们已经为您处理了底层的集群设计、系统调优，打通数据交互层等工作，您只需要专注于如何在业务上挖掘数据潜在的价值即可。</p>

<h2 id="section">应用场景举例</h2>

<p>百度的IDMapping接入层每天的PV达到上亿，每天产生的日志量达到100GB，日志中的信息包括用户的访问IP、访问时间、响应时间、用户请求、应答内容等。IDMapping由10台nginx服务器构成、分别部署在不同的服务器上。其中的日志格式如下：</p>

<p>数据格式如下：</p>

<pre><code>$remote_addr - [$time_local] "$request" $status $body_bytes_sent "$http_referer"  $http_cookie" $remote_user "$http_user_agent" $request_time  $host $msec
</code></pre>

<p>下面是一条具体日志：</p>

<pre><code>10.81.78.220 - [04/Oct/2015:21:31:22 +0800] "GET /u2bmp.html?dm=37no6.com/003&amp;ac=1510042131161237772&amp;v=y88j6-1.0&amp;rnd=1510042131161237772&amp;ext_y88j6_tid=003&amp;ext_y88j6_uid=1510042131161237772 HTTP/1.1" 200 54 "-" "-" 9CA13069CB4D7B836DC0B8F8FD06F8AF "ImgoTV-iphone/4.5.3.150815 CFNetwork/672.1.13 Darwin/14.0.0" 0.004 test.com.org 1443965482.737
</code></pre>

<p>负责人希望能够通过这些日志信息实时地获取服务的PV、UV等统计信息以及访问用户IP的所在地信息等，并且希望可以查询任意时间的用户访问信息，以此满足日常运营的需求，后续还可能添加告警和日运营报表等功能。</p>

<h2 id="section-1">解决方案</h2>

<p>在BMR中我们集成了Flume、Kafka、Spark、Hbase组件，可以很好的满足应用场景中IDMapping负责人的集群需求。我们设计了如下的大数据处理的pipeline。</p>

<p><img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-26/97268179.jpg" width="750px" /></p>

<p>每台机器上的日志数据通过flume实时地推送到Kafka集群中，在spark集群中订阅这些日志数据，经过ETL处理后存储到Hbase中。前端展示系统可以通过Hbase的Restful接口实时的获取数据。同时，可以提交新的spark streaming application修改原有的数据处理模型，也可以在后端也可以对数据进一步加工：通过集群内部的mahout、spark mllib或对接其他的BI系统，例如Palo、Saiku。</p>

<p>以下是前端获取数据后的展示效果图：</p>

<p><img src="http://i4.tietuku.com/50dd01cd4f5db9c2.png" width="350px" /></p>

<p><img src="http://i4.tietuku.com/93356a3071840b10.png" width="400px" /></p>

<p>接下来通过三步骤对这个解决方案在BMR中的实现进行详细的阐述。</p>

<h3 id="step-1-">Step 1 创建集群</h3>

<p>创建包括Spark和Streaming组件的集群。在生产环境中建议分别创建Kafka和Spark集群。可以参考<a href="https://bce.baidu.com/doc/BMR/GettingStarted.html#.E5.88.9B.E5.BB.BA.E9.9B.86.E7.BE.A4">文档</a>。</p>

<h3 id="step-2-">Step 2 数据准备</h3>

<p>数据获取表示如何将nginx产生的日志通过flume导入到BMR集群中。我们执行下面的命令：</p>

<pre><code>wget http://bmr.bj.bcebos.com/tools/flume/flume-1.6.0.tar.gz
vim $FLUME_HOME/conf/flume-conf.properties
$FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/flume-conf.properties --name agent
</code></pre>

<p>以上的命令分别表示获取flume、编辑配置文件、运行flume agent。其中配置文件参考<a href="http://wiki.baidu.com/pages/viewpage.action?pageId=158727265">http://wiki.baidu.com/pages/viewpage.action?pageId=158727265</a>，将<code>agent.sources.s.command</code>改为<code>tail $NGINX_HOME/logs/access.log</code>。</p>

<h3 id="step-3-">Step 3 数据处理</h3>
<p>建立新的spark集群，当然在测试阶段您也可以直接使用kafka集群中的spark进行处理，在实际应用中推荐使用新的spark集群。</p>

<ol>
  <li>下载spark streaming代码，进行编译，将编译结果<code>bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</code>放到bos中。</li>
  <li>
    <p>从console页面进去到对应集群的作业列表页面，然后点击“添加作业”，如果使用系统提供的输入数据和jar包，可以按照如下方式填写参数：</p>

    <blockquote>
      <p>作业类型：Spark</p>
    </blockquote>

    <blockquote>
      <p>名称：FKSTest</p>
    </blockquote>

    <blockquote>
      <p>bos输入地址： bos://${PATH}/bmr-spark-kafka-samples-1.0-SNAPSHOT-jar-with-dependencies.jar</p>
    </blockquote>

    <blockquote>
      <p>失败后操作：继续</p>
    </blockquote>

    <blockquote>
      <p>Spark-submit: –class com.baidubce.bmr.sample.DirectFKSTest</p>
    </blockquote>

    <blockquote>
      <p>应用程序参数：ng1889b62-master-instance-f5lvbago topic</p>
    </blockquote>

    <p>其中应用程序参数分别代表集群master的hostname和kafka topic。 </p>
  </li>
  <li>
    <p>您可以通过集群页面的<code>Resource Manager Web UI</code>查看spark UI查看作业运行的状态。（进入页面所需要的用户名密码会通过短信形式发送到您手机上）
 <img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-1-25/49196148.jpg" alt="" /></p>
  </li>
  <li>
    <p>查看hbase中的数据：</p>

    <pre><code> hbase(main):001:0&gt; list
 hbase(main):002:0&gt; scan 'PVUV', {COLUMN=&gt;['statistics:PV:toInt', 'statistics:UV:toInt']}
</code></pre>
  </li>
</ol>

<h2 id="spark-streaming">关于Spark Streaming中实时流的说明建议</h2>

<p>在Spark Streaming中有两种API用于处理与kafka之间的交互。</p>

<ul>
  <li>一种是spark1.2.0引进的<code>KafkaUtils.createStream</code>，这种方式可以将kafka或其他流式输入先写入磁盘再分片处理，防止重启driver造成数据丢失。换句话说，可以保证At least Once语义，前提是开启<code>Write Ahead Logs</code>，方法如下
    <ul>
      <li>在代码中通过<code>streamingContext.checkpoint</code>配置checkpoint目录</li>
      <li>配置<code>spark.streaming.receiver.writeAheadLog.enable</code>为<code>true</code></li>
    </ul>
  </li>
  <li>另外一种则是spark1.3.0引进的Direct API。这种方式保证的是<code>Exactly Once</code>语义，解决上种方式中<code>consumer offset</code>和数据Logs存储不一致性造成的数据重复计算。这种方式通过将<code>offset</code>存入
checkpoints中，来保证接收数据的一致性。</li>
</ul>

<p>使用<code>KafkaUtils.createStream</code>需要有一下两种考虑：</p>

<ul>
  <li>提高streaming的吞吐量，我们通常会使用多个consumer来并行的获取数据，每个consumer分配到一个executor的单核上，最后将所有得到的Stream进行<code>Union</code>操作。
如果不进行<code>Union</code>则会导致<code>Transformation</code>数量增多<code>#consumer</code>倍。</li>
  <li>另外也要考虑RDD中partition的数量，减少partition数量有助于减少task个数以及调度时间。partition的数量是由batchInterval和spark.streaming.blockInterval共同决定的，根据spark官方指导，通常partition数目
为cores的2到3倍比较合适，所以可以调整适当的参数控制partition的个数。</li>
</ul>

<p>而在DirectAPI中会自动定期的根据kafka的topic+partition查询最新的offset，定义需要处理的offset范围。所以不需要考虑创建多少receivers，也不需要考虑partition的数量。在API中每个kafka partition都是自动地并行读取，并且对应每个RDD partition，从而简化Streaming处理的并行模式。</p>

<p>但是DirectAPI并不会在zookeeper中更新offset，所以基于zookeeper的kafka监控工具无法查看日志处理的进度。但您也可以查询checkpoint，将offset写入zookeeper中。</p>

<p>这两种使用方式在Sample中都有详细的例子可以参考，分别是<code>com.baidubce.bmr.sample.FKSTest</code>和<code>com.baidubce.bmr.sample.DirectFKSTest</code>。</p>

<h2 id="section-2">总结</h2>

<p>虽然针对不同的目标和业务案例使用流式处理的方式也不同，但其主要场景包括：</p>

<ul>
  <li>流ETL——将数据推入存储系统之前对其进行清洗和聚合。</li>
  <li>触发器——实时检测异常行为并触发相关的处理逻辑。</li>
  <li>数据浓缩——将实时数据与静态数据浓缩成更为精炼的数据以用于实时分析。</li>
  <li>复杂会话和持续学习——将与实时会话相关的事件组合起来进行分析。</li>
</ul>

<p>在上述例子中我们介绍了BMR中流ETL的场景。
在BMR中，我们提供了Hadoop生态圈中的全栈组件包括Hadoop、Spark、Hbase、Hive、Pig、Kafka、Mahout等，
您可以根据自己的业务场景灵活地选择不同的组件。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Tunning]]></title>
    <link href="http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning/"/>
    <updated>2015-12-01T09:50:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/12/01/hadoop-tunning</id>
    <content type="html"><![CDATA[<p>Hadoop的调优涉及到整个MapReduce的各个过程，并且要对每个参数的意义和Counter信息有一定的了解。也就是说，根据Counter的信息推测哪些MapReduce阶段可能存在性能瓶颈，并且根据这个瓶颈理解对应的Hadoop 框架中的处理逻辑，进而可以调整相关参数的大小或程序的行为。</p>

<p>从大面上来看，MapReduce的瓶颈可能存在以下图中的各个部分。</p>

<p><img src="http://ww2.sinaimg.cn/large/74311666jw1eyjx8zpfz2j20rq0cegna.jpg" width="500px" /></p>

<!--more-->

<p>在开始调优之旅前，先来个开胃小菜。进行调优的过程中，我们首先要知道整个job的运行情况。</p>

<p>Hadoop 2中的JobHistory是能够提供作业运行时各个参数指标的展示工具，可以通过这个UI界面查看所有的Counter。另一方面如果不方便通过界面的方式查看，则可以利用Hadoop自带的命令行工具查看，方法是<code>hadoop job -history &lt;history file&gt;</code>, 这个history file的位置通常是在mapreduce.jobhistory.done-dir目录下，可以用如下方式查找<code>hadoop fs -lsr &lt;done-dir&gt; | grep job_1398974791337_0037</code>。</p>

<h2 id="map-optimizations">Map Optimizations</h2>

<p>在Map端的优化通常会涉及到输入的数据和它的处理过程，以及你的应用程序代码。Mapper需要读取作业的输入，输入文件的不同也会影响到作业运行的效率，例如文件是否是splittable，数据的本地性和input split的数量等等。</p>

<h3 id="data-locality">Data Locality</h3>

<p>在分布式计算中有条著名的准则<strong>Pushing compute to the data</strong>，map的task应该尽可能的被安排在数据存放的节点上。可以用Counter来判断作业是否符合这条准则:</p>

<ul>
  <li>HDFS_BYTES_READ：这个值应当不大于input file的block size</li>
  <li>DATA_LOCAL_MAPS：这个值应当为1</li>
  <li>RACK_LOCAL_MAPS：这个值应当为0</li>
</ul>

<p>以下几种情况可能会发生non-local read:</p>

<ul>
  <li>不能分割的大文件，这样mapper就必须从不同的节点中读取blocks。</li>
  <li>文件格式支持split，但是用的input format不对。典型的情况是LZOP格式，需要先建立索引后再进行读取。</li>
  <li>Yarn的调度器不能在某个节点上产生map container，通常是由于集群under load。</li>
</ul>

<p>解决方法是：</p>

<ul>
  <li>尽量保持unsplittable文件的大小接近一个block的大小。</li>
  <li>设置yarn的<code>scheduler.capacity.node-locality-delay</code>，引入跳过的调度次数，来增加map task分配到数据节点上的概率。</li>
  <li>使用Twitter提供的LZO Input Format来处理lzop数据，或者使用bzip2格式的文件。</li>
</ul>

<h3 id="map-number-overwhelm">Map Number Overwhelm</h3>

<p>当输入有很多的input split的时候，每个input split都需要一个mapper来执行，而每个mapper都是一个单独的进程。这样会给调度器和集群带来极大的压力。原因通常有两个：</p>

<ul>
  <li>input data由很多的小文件组成，Hadoop会为每个小文件生成一个mapper，最终时间会大量消耗在启动进程上。</li>
  <li>每个文件并不是很小（和block size相当），但总体的数据量很大，横跨上千个HDFS的blocks。这样每个block也会分配给单独的mapper。</li>
</ul>

<p>如果是第一种情况，可以先聚合这个小文件，或者使用类似avro的文件格式来存储。或者直接用<code>CombineFileInputFormat</code>来处理以上这两种情况，它可以在一个mapper里处理多个HDFS block的数据。<code>CombineFileInputFormat</code>会首先检查input files的所有blocks，简历每个block到data nodes的映射关系，接着将同一个节点上的blocks聚合到一个input split中以保持data locality。它有三个配置项来调整：</p>

<ul>
  <li>mapreduce.input.fileinputformat.split.minsize.per.node</li>
  <li>mapreduce.input.fileinputformat.split.minsize.per.rac</li>
  <li>mapreduce.input.fileinputformat.split.maxsize</li>
</ul>

<p>以上的默认配置会造成每个节点上尽量形成一个最大的input split，影响作业的并行性，可以用以上几个配置来调整。<code>CombineFileInputFormat</code>还包括两个具体的类：</p>

<ul>
  <li><code>CombineTextInputFormat</code></li>
  <li><code>CombineSequenceFileInputFormata</code></li>
</ul>

<h3 id="input-split-computation">Input Split Computation</h3>

<p>如果提交作业的client是在集群局域网之外，那么input split的计算可能带来高成本。</p>

<p>当输入的数据源是HDFS时，client需要做如下事情，包括file listing, file status retrieving，input files数量比较多的时候，整个过程带来数据传输的延迟是比较可观的。</p>

<p>可以通过设置<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>将input split的计算交给AppMaster处理，这是在集群内部进行的。</p>

<h2 id="shuffle-optimizations">Shuffle Optimizations</h2>

<h3 id="using-the-combiner">Using the Combiner</h3>

<p>combiner可以有效的减少mapper和reducer之间通信的数据量。</p>

<h3 id="using-binary-comparators">Using Binary Comparators</h3>

<p>MapReduce在做sorting或者merging的时候，使用<code>RawComparator</code>比较map output key。内置的<code>Writable</code> classes（<code>Text</code>、<code>IntWritable</code>）有byte-level的比较器，无需将二进制的数据重新组装成实际的对象，所以能够快速进行序列化对象的比较。</p>

<p>用户可以在自己构造的<code>Writable</code>对象里面实现<code>WritableComparable</code>接口，这处理起来会比较容易，但是另一方面要注意MapReduce中map output data是以byte的形式存储的，这会导致在shuffle和sort的阶段需要从byte到object的转化才可以完成对象的比较。</p>

<p>可以看到在Hadoop的内置<code>Writable</code>对象不仅实现了<code>WritableComparable</code>接口，还自定义继承自<code>WritableComparator</code>的比较器。<code>WritableComparator</code>有什么作用呢，可以看下它的一些方法申明。</p>

<pre><code>public class WritableComparator implements RawComparator {
    public int compare(byte[] b1, int s1, int l1,
                       byte[] b2, int s2, int l2); 
}
</code></pre>

<p>可以看到这是byte-level的Comparator，<code>Writable</code>对象正是覆盖了这里面的compare方法。在内置的<code>Writable</code>对象中都实现了<code>WritableComparator</code>，所以无需担心内置对象的效率。但是自己所构造的对象也可以实现<code>WritableComparator</code>方法来提高效率。</p>

<p>例如一个拥有firstName和lastName的Person对象：</p>

<pre><code>private String firstName;
private String lastName;

@Override
public void write(DataOutput out) throws IOException {
    out.writeUTF(lastName);
    out.writeUTF(firstName);
}
</code></pre>

<p><img src="http://i5.tietuku.com/e5eba32886b5773d.png" width="600px" /></p>

<pre><code>public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,
                     int l2) {
    int lastNameResult = compare(b1, s1, b2, s2);
    if (lastNameResult != 0) {
        return lastNameResult;
    }
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compare(b1, s1 + b1l1 + 2, b2, s2 + b2l1 + 2);
}

public static int compare(byte[] b1, int s1, byte[] b2, int s2) {
    int b1l1 = readUnsignedShort(b1, s1);
    int b2l1 = readUnsignedShort(b2, s2);
    return compareBytes(b1, s1 + 2, b1l1, b2, s2 + 2, b2l1);
}

public static int readUnsignedShort(byte[] b, int offset) {
    int ch1 = b[offset];
    int ch2 = b[offset + 1];
    return (ch1 &lt;&lt; 8) + (ch2);
}
</code></pre>

<h3 id="tunning-the-shuffle-internals">Tunning the shuffle internals</h3>

<p>在mapper中，output record首先被存储在一个内存buffer中，当这个buffer增长到一定大小的时候，数据被spill到磁盘中的一个文件。整个过程持续到mapper完成所有的output record生成。过程如下：</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>在整个阶段中，I/O相关的splling和merging是最耗时的，所以理想状况应该是所有的output数据都可以装入buffer中，这样只有一个文件被spill到磁盘。这对所有的作业来说自然是不大可能，但是如果mapper可以通过filter或者project的方法减少input data，那么可以好好调整下<code>mapreduce.task.io.sort.mb</code>的大小，因为这个数据直接关系buffer的大小。可以通过检查下面的Counters来调整map端的shuffle：</p>

<ul>
  <li>MAP_OUTPUT_BYTES  用这个数据来估计是否可以调整<code>mapreduce.task.io.sort.mb</code>来装入map的output record</li>
  <li>SPILLED_RECORDS/MAP_OUTPUT_RECORDS  这两个数据的理想情况是一致的，表示只有一个spill发生。</li>
  <li>FILE_BYTES_READ/FILE_BYTES_WRITTEN  比较这两个数据和MAP_OUTPUT_BYTES可以理解在splling和merging阶段发生的读写副作用</li>
</ul>

<p>在reduce方面，map的output通过每个节点上运行的shuffle service进程被发送到对应的reducer。在reducer中，map output是被写入到一个内存buffer中，在数据接收的过程中buffer中的数据被排好序，并在到达一定数据量的时候写入磁盘。同时有一个后台进程负责不断merge这个小的spllied file到merged files中，当所有的fetcher获取了所有的outputs，会有一个最终的merging发生，这时候数据也就从merged files到reducer了。也就是如下图的这个过程：</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<p>通map端的调优一样，reduce端也是尽量将数据存入内存中，减少splling和merging发生的次数，但是这个过程并不如map端一样明显，因为数据是在边接收边merging的。默认情况下，无论数据是否可以装入内存中，splling总会发生，所以可以调整<code>mapreduce.reduce.merge.memtomem.enabled</code>为true启动memory-to-memory的merge。map端的Counter同样适用于reduce。</p>

<p>以下的参数可以用来调整Hadoop的shuffle行为：</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Default</th>
      <th>Map or Reduce</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mapreduce.task.io.sort.mb</td>
      <td>100 (MB)</td>
      <td>Map</td>
      <td>The total amount of buffer memory in megabytes to use when buffering map outputs. This should be approximately 70% of the map task’s heap size.</td>
    </tr>
    <tr>
      <td>mapreduce.map.sort.spill.percent</td>
      <td>0.8</td>
      <td>Map</td>
      <td>Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than 0.5.</td>
    </tr>
    <tr>
      <td>mapreduce.task.io.sort.factor</td>
      <td>10</td>
      <td>Map and Reduce</td>
      <td>The number of streams to merge at once while sorting files. This determines the number of open file handles. Larger clusters with 1,000 or more nodes can bump this up to 100.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.parallelcopies</td>
      <td>5</td>
      <td>Reduce</td>
      <td>The default number of parallel transfers run on the reduce side during the copy (shuffle) phase. Larger clusters with 1,000 or more nodes can bump this up to 20.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.input.buffer.percent</td>
      <td>0.7</td>
      <td>Reduce</td>
      <td>The percentage of memory to be allocated from the maximum heap size to store map outputs during the shuffle.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.shuffle.merge.percent</td>
      <td>0.66</td>
      <td>Reduce</td>
      <td>The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.</td>
    </tr>
    <tr>
      <td>mapreduce.reduce.merge.memtomem.enabled</td>
      <td>false</td>
      <td>Reduce</td>
      <td>If all the map outputs for each reducer can be stored in memory, then set this property to true.</td>
    </tr>
  </tbody>
</table>

<p>Shuffle的原则是使用filter和project减少数据量，使用combiner以及压缩map的output，尽可能的减少mapper和reducer质检传递的数据，减低IO带来的开销。这样之后再利用上述提到的参数来调整Shuffle的过程。</p>

<h2 id="recuder-optimizations">Recuder Optimizations</h2>

<h3 id="the-number-of-reducers">The Number of Reducers</h3>

<p>大多数情况下map端的并行度是由框架根据input files和input format来自动决定的，但在Reduce端，reducer的数量是由用户自行决定的。太少的reducers意味着没能充分利用集群的资源，太多的reducer则会让调度器疲于奔命，如果没有太多的资源供所有reducer运行则会拖累reducer的执行效率。在一些使用场景中，不能避免的需要使用少量的reducer来运行作业，例如数据写入DB系统中。另外一些场景中需要确认数据是否会发生倾斜，以及如何partition的，数据量是否会让reducer发生OOM的情况。</p>

<h2 id="general-tunning-tips">General tunning tips</h2>

<ul>
  <li>压缩</li>
  <li>使用类似于Avro或Parquet的数据格式存储数据，带来的好处是空间利用率，序列化和反序列化更加有效。
    <ul>
      <li>在Hadoop中，text并不是一个有效的数据格式，空间利用率低，解析成本高，特别是用到正则的时候。</li>
      <li>尽可能考虑使用二进制的文件存储格式。</li>
    </ul>
  </li>
</ul>

<h2 id="tunning-tools">Tunning tools</h2>

<h3 id="stack-dumps">stack dumps</h3>

<p>ssh到task运行的机器上，执行下面的命令：</p>

<pre><code>ps aux | grep container_1393242034820_0001_01_000002
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
kill -s SIGQUIT 554284
</code></pre>

<p><code>SIGQUIT</code>信号的发送应该要间隔几秒，当JVM收到这个信号的时候会执行stack dump，这样可以了解到程序在这段时间内的运行情况。最后可以在task 的output file中查看dump出来的栈信息。</p>

<h3 id="profiling-map-and-reduce-task">Profiling Map and Reduce Task</h3>

<p>可以使用HPROF结合一些Mapreduce job method来进行Profiling。HPROF是JVM内置的java profiling工具，Hadoop内置了对HPROF的支持。可以在driver中加入如下的代码：</p>

<pre><code>job.setProfileEnabled(true);
job.setProfileParams(
    "-agentlib:hprof=depth=8,cpu=samples,heap=sites,force=n," +
        "thread=y,verbose=n,file=%s");
job.setProfileTaskRange(true, "0,1,5-10");
job.setProfileTaskRange(false, "");
</code></pre>

<p>在<code>setProfileParams</code>方法中设置的参数会在每个container中建立一个名为profile.out的文件，这个文件可以很容易被解析。可以通过ssh到目标机器查看或者通过JobHistory UI界面查看。</p>

<p>profile.out包括一些stack traces，还包括内存和CPU时间的信息。以下是一个profile.out文件：</p>

<p><img src="http://i5.tietuku.com/ba1dcbebf426b1a8.png" width="600px" /></p>

<p>可以看出来第一个问题是在<code>String.split</code>这个方法的使用上，它采用正则表达式来分割字符串，这个是相当耗时的一个举措，可以用Apache Commons Lang library的<code>StringUtils.split</code>来替换。另外一个是发生在Text的构造上，可以只构造一个Text实例，采用<code>set</code>方法进行设置，这样会更加有效率。</p>

<p>需要注意，使用HPROF会给程序的执行带来额外的负担，需要持续的收集profiling的信息，所以在正常的运行过程中不应该加上。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Secondary Sorting]]></title>
    <link href="http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting/"/>
    <updated>2015-11-22T17:27:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/11/22/hadoop-secondary-sorting</id>
    <content type="html"><![CDATA[<p>Hadoop MapReduce的神奇之处发生在mapper和reducer之间，将所有相同key的map输出记录聚集在一块，使得用户可以方便的处理聚合在一起的数据。Hadoop内部使用了partition、sort和merge（shuffle的一部分），在每个reducer中流式地得到排序后的key和value集合。在MapReduce Sorting中有个特别的部分是secondary sort，也就是对value进行排序。</p>

<!--more-->

<p>Secondary sort在两种情况下特别有用：</p>

<ul>
  <li>需要某一部分的数据比其他数据更快的到达reducer。</li>
  <li>希望job的输出按照两个key进行排序。</li>
</ul>

<p>实现Secondary sort需要对MapReduce中的数据流和处理有一定的了解，下图展示了对reducer中出现的数据有影响的三个部分。</p>

<p><img src="http://i5.tietuku.com/7ad3ad872415c4b6.png" width="600px" /></p>

<p><code>partitioner</code>决定哪个reducer接收该mapper数据记录；<code>sorting RawComparator</code>用于在各自的分片中排序输出的结果，map和reduce阶段都有它，其中map阶段的sorting是对reduce阶段sorting的一个优化，让reducer的sorting更高效；最后，<code>grouping RawComparator</code>用于决定reducer处理排序后记录的边界，发生在reducer从本地磁盘读取数据的时候，也就是说，你可以用这个方法决定数据记录是如何聚集起来调用一个reduce方法的。MapReduce默认把这个三个方法作用于map方法输出的key上。</p>

<p>要实现Secondary sorting，我们需要重写partitioner、sort comparator和grouping comparator。</p>

<p>下面，通过对人名的排序来说明如何使用Secondary sorting。使用primary sort排序last name，secondary sort排序first name。</p>

<p>我们需要构建一个由map方法输出的Composite key，这个key由两部分组成：</p>

<ul>
  <li>Natural Key</li>
  <li>Secondary Key</li>
</ul>

<p><img src="http://i5.tietuku.com/25eedf0319e92775.png" width="430px" /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Person</span> <span class="kd">implements</span> <span class="n">WritableComparable</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">firstName</span><span class="o">;</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">String</span> <span class="n">lastName</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">readFields</span><span class="o">(</span><span class="n">DataInput</span> <span class="n">in</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">firstName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">    <span class="k">this</span><span class="o">.</span><span class="na">lastName</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">write</span><span class="o">(</span><span class="n">DataOutput</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">lastName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>下图说明hadoop框架配置中用于设置partitioning、sorting和grouping类的名字和方法。</p>

<p><img src="http://i5.tietuku.com/520e7242cd6ecc43.png" width="530px" /></p>

<h3 id="partitioner">Partitioner</h3>

<p>默认的partitioner使用对key进行hash后取reducer个数的模。但是默认的partitioner使用整个key，会导致相同的natural key发往不同的reducer。所以需要实现自己的partitioner。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNamePartitioner</span> <span class="kd">extends</span>
</span><span class="line">    <span class="n">Partitioner</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numPartitions</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="k">return</span> <span class="n">Math</span><span class="o">.</span><span class="na">abs</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">*</span> <span class="mi">127</span><span class="o">)</span> <span class="o">%</span>
</span><span class="line">        <span class="n">numPartitions</span><span class="o">;</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="sorting">Sorting</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">w1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">w2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">w2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">
</span><span class="line">    <span class="kt">int</span> <span class="n">cmp</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">if</span> <span class="o">(</span><span class="n">cmp</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="k">return</span> <span class="n">cmp</span><span class="o">;</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getFirstName</span><span class="o">());</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="grouping">grouping</h3>

<p>grouping阶段所有的数据记录已经是secondary sort了，grouping comparator需要将相同的last name聚合在一起。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonNameComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="kd">protected</span> <span class="nf">PersonNameComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="kd">super</span><span class="o">(</span><span class="n">Person</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">o1</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">o2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="n">Person</span> <span class="n">p1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o1</span><span class="o">;</span>
</span><span class="line">    <span class="n">Person</span> <span class="n">p2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Person</span><span class="o">)</span> <span class="n">o2</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">p1</span><span class="o">.</span><span class="na">getLastName</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">p2</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="mapreduce">MapReduce</h3>

<p>最后在driver中，需要设置上文提到的三个类：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setPartitionerClass</span><span class="o">(</span><span class="n">PersonNamePartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setSortComparatorClass</span><span class="o">(</span><span class="n">PersonComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line"><span class="n">job</span><span class="o">.</span><span class="na">setGroupingComparatorClass</span><span class="o">(</span><span class="n">PersonNameComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Map</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">  <span class="kd">private</span> <span class="n">Person</span> <span class="n">outputKey</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Person</span><span class="o">();</span>
</span><span class="line">
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">Text</span> <span class="n">lastName</span><span class="o">,</span> <span class="n">Text</span> <span class="n">firstName</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">outputKey</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">lastName</span><span class="o">.</span><span class="na">toString</span><span class="o">(),</span> <span class="n">firstName</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class="line">    <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">outputKey</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Reduce</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Person</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">  <span class="n">Text</span> <span class="n">lastName</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>
</span><span class="line">  <span class="nd">@Override</span>
</span><span class="line">  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Person</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span>
</span><span class="line">                     <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class="line">      <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class="line">    <span class="n">lastName</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getLastName</span><span class="o">());</span>
</span><span class="line">    <span class="k">for</span> <span class="o">(</span><span class="n">Text</span> <span class="n">firstName</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">      <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">lastName</span><span class="o">,</span> <span class="n">firstName</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">  <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Secondary sort涉及到的自定义的partitioner、sorter和grouper，还是比较复杂的。可以考虑<a href="http://htuple.org">htuple</a>对简单类型进行secondary sort。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming]]></title>
    <link href="http://billowkiller.github.io/blog/2015/10/27/spark-streaming/"/>
    <updated>2015-10-27T21:04:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/10/27/spark-streaming</id>
    <content type="html"><![CDATA[<p>流式计算通常是为了满足日益增长的数据的实时获取和低延时计算的需求。通常来说，一个优秀的流式计算引擎需要满足一下的一些需求：</p>

<ol>
  <li>不重不丢的保证（在节点或计算失败的时候，内存中的计算状态能被正确的恢复）</li>
  <li>低延迟</li>
  <li>高吞吐量</li>
  <li>强大的计算模型</li>
  <li>容错机制的低开销</li>
  <li>流控</li>
</ol>

<p><img src="http://spark.apache.org/images/spark-logo.png" alt="image" />
<!--more--></p>

<p>介绍完了spark后就可以来说说spark streaming，毕竟spark streaming是完全构建在spark之上，熟悉了spark的RDD原理之后就比较容易理解spark streaming。说白了，spark streaming中的流式计算是伪实时的，之所以是伪实时的是因为它将实时的处理变成时间跨度较小的批量处理。没错，就是将一段时间间隔中的数据变成RDD，然后利用spark原有的架构处理这段时间内的数据，接着在时间维度上对这些处理后的RDD进行迭代计算。也就是将流式计算分解为一系列微小的、原子的批量作业，每个微批量作业如果失败则可以重新计算。这种分解的思想可以应用在批量计算框架、也可以应用在流式计算框架上，例如Storm Trident。</p>

<p>这样的处理带来了几个明显的好处：</p>

<ul>
  <li>高吞吐量</li>
  <li>不从不丢的保证</li>
  <li>快速falut recovery</li>
  <li>更方便处理慢节点</li>
  <li>实时和批量统一的编程接口</li>
</ul>

<p>下面介绍下spark streaming中的具体实现来理解这几点。</p>

<h3 id="section">计算模型</h3>

<p><img src="http://blog.selfup.cn/wp-content/uploads/2014/08/streaming-flow.png" alt="image" /></p>

<p>首先，Spark Streaming把实时输入数据流以时间片（如1秒）为单位切分成块。Spark Streaming会把每块数据作为一个RDD，并使用RDD操作处理每一小块数据。每个块都会生成一个Spark Job处理，最终结果也返回多块。</p>

<p>举个栗子来说明下这个过程：</p>

<pre><code>pageViews = readStream("http://..."， “ls”)
ones = pageViews.map(event =&gt; (event.url, 1))
counts = ones.runningReduce((a, b) =&gt; a+b)
</code></pre>

<p>上述代码的作用是根据URL的数量计算访问次数。处理的过程为首先通过HTTP获取到一个pageview事件的RDD，经过<em>transformation</em>操作变成(URL, 1), 最后的操作计算相同的URL数目。整个streaming过程可以用下图来表示：
<img src="http://img-storage.qiniudn.com/15-10-18/95269436.jpg" alt="" /></p>

<p>整个的处理过程可以看出，输入流被分成每个都是1秒的batch，经过处理后生成resultRDD，这个resultRDD在每个时间间隔中都会产生一个，并经过reduce迭代计算。在上图中最终的reduce的输入参数就包括上一个时间间隔的resultRDD和这个时间间隔中map操作的结果。上图也可以看出这些RDD的lineage graph，在节点失败的时候，可以根据这个lineage graph以partition的粒度为单位重新执行任务计算丢失的分片，可以看出这些计算的任务都是可以并行执行的。同时，对于慢节点来说，因为计算是无状态的，且每个job的结果是可以确定的，spark streaming可以执行类似于hadoop中的预测模型——在其他节点上计算同样的任务。另外，spark streaming也会有些checkpoint来防止无限制的恢复计算。</p>

<p>对比于其他流式处理的方式，spark streaming在处理失败任务和慢节点上无疑更有效率。它可以从时间和partition两个维度上并行地计算数据来加快恢复速度。而对于像Strom的流式处理来说，往往是通过<strong>上游数据backup</strong>或者<strong>同时复制执行相同的作业</strong>来保证数据处理的可靠性。这两种处理方式对于资源的消耗无疑都是巨大的，且恢复的时间也比较长。</p>

<ul>
  <li>其中对于前者来说，每个节点都需要保存上一个checkpoing后所发送数据的拷贝，在节点失败时由standby机器重新计算上游节点发送过来的数据，因为这些计算都是有状态的，所以恢复的时间比较长，Storm就只保证“at least once”语义来提高处理速度。Trident之所以能够保证不重不丢是使用了数据库来复制计算状态。</li>
  <li>对于后者无疑更加消耗资源，并且需要保证两个数据处理操作接受到的上游数据的顺序是一样的。</li>
</ul>

<p>同spark一样，只有当某个Output Operations原语被调用时，stream才会开始真正的计算过程。现阶段支持的Output方式有以下几种：</p>

<ul>
  <li>print()</li>
  <li>foreachRDD(func)</li>
  <li>saveAsObjectFiles(prefix, [suffix])</li>
  <li>saveAsTextFiles(prefix, [suffix])</li>
  <li>saveAsHadoopFiles(prefix, [suffix])</li>
</ul>

<h3 id="section-1">流式处理中的几点难点</h3>

<p>在流式处理中通常都会有几个难点需要考虑。</p>

<ul>
  <li>时间窗口问题</li>
  <li>数据一致性问题</li>
  <li>内存状态管理问题</li>
</ul>

<p>我们来看下spark streaming是如何解决的。</p>

<ol>
  <li>
    <p><strong>时间窗口问题。</strong>spark streaming是根据数据到达系统的时间将记录放到对应的RDD中，这种时间窗口划分是基于墙上时间的，好处可以保证系统及时产生一个新的batch运行job，并且可以让程序运行在数据生成的地方，不必再进行分发。</p>

    <p>这种基于墙上时间的统计有一个非常严重的问题是不能回放数据流。当数据流是实时产生的时候，“墙上时间”的一分钟也就只会有一分钟的event被产生出来。但是如果统计的数据流是基于历史event的，那么一分钟可以产生消费的event数量只受限于数据处理速度。另外event在分布式采集的时候也遇到有快有慢的问题，一分钟内产生的event未必可以在一分钟内精确到达统计端，这样就会因为采集的延迟波动影响统计数据的准确性。所以产生了另外一种时间窗口划分的方法。</p>

    <p>另一种时间窗口划分的方法是基于外部时间的，例如日志时间。spark streaming提供两种方法来处理这种情况：</p>

    <ul>
      <li>延迟处理，等待一定时间来处理每个batch。</li>
      <li>用户的应用程序中保证乱序事件的正确处理。</li>
    </ul>

    <p>以上也说明在批处理的流式计算模型是受限的，很多情况下只能依靠用户的应用程序来做处理，例如实时的统计5s内的pv；其次这种方式也没有很好的流控技术手段，如果有突发的大量数据产生，会导致结果产生的时间更长，甚至是将系统的JVM撑爆。最后实时性也是受限的，只能达到次秒级的处理延迟，毕竟是要等待一个时间batch的处理完成。</p>
  </li>
  <li>
    <p><strong>数据一致性问题。</strong>什么是数据一致性，举个栗子，要统计网站中来自各个国家的page view，把不同国家的pv统计放在不同的节点上处理。但是现在统计英国的节点处理速度要慢于法国的，这将导致两个节点上数据的时间状态不一致。在流式处理中，数据的一致性的保证同时意味着资源的消耗。流式处理的数据一致性有三种解决思路，在上文中也有提到，这里概括下：</p>

    <ul>
      <li>上游备份策略：重启的时候重放kafka的历史数据，恢复内存状态</li>
      <li>中间状态持久化：把统计的状态放到外部的持久的数据库里，不放内存里</li>
      <li>同时跑两份：同时有两个完全一样的统计任务，重启一个，另外一个还能正常运行。</li>
    </ul>

    <p>而在spark streaming中，数据一致性天然得到保证的。因为记录根据时间来分片，所以中间的resultRDD反应的是当前时间和之前时间所计算出来的结果，无论计算和结果被分配到哪个节点上都不会有节点间数据不一致的情况。也就是数据的不重不丢可以得到保证。</p>
  </li>
  <li>
    <p><strong>内存状态管理问题。</strong>
做流式统计的有两种做法：</p>

    <ul>
      <li>依赖于外部存储管理状态：比如没收到一个event，就往redis里发incr增1</li>
      <li>纯内存统计：在内存里设置一个counter，每收到一个event就+1</li>
    </ul>

    <p>第一种会把整个压力全部压到数据库上，造成处理速度下降；第二种的状态相对来说容易管理一些，计算直接是基于这个内存状态做的。如果重启丢失了，重放一段历史数据就可以重建出来。内存的问题是它总是不够用的，解决的方法是input分割和把存储移到外边去。在内存计算中把窗口统计的中间状态落地的好处是显而易见的：重启之后不用通过重算来恢复内存状态。但是这种对外部数据库使用不小心就会导致两个问题：</p>

    <ul>
      <li>处理速度慢。不用一些批量的操作，数据库操作很快就会变成瓶颈</li>
      <li>数据库的状态不一致。内存的状态重启了就丢失了，外部的状态重启之后不丢失。重放数据流就可能导致数据的重复统计</li>
    </ul>

    <p>在spark streaming中支持传统批量计算中的无状态<em>transformation</em>操作，例如<code>map</code>、<code>reduce</code>、<code>groupBy</code>和<code>join</code>。这就避免了普通流式计算中麻烦的状态保存问题。但spark streaming中也支持多个时间间隔中有状态的<em>transformation</em>操作，包括：</p>

    <ol>
      <li>Windowing: 生成滑动窗口RDD。<code>words.window("5s")</code>将产生一个RDD包含[0,5),[1,6),[2,7)的时间间隔。</li>
      <li>增量聚合：在滑动窗口的基础上进行RDD的聚合操作，也就是<code>reduceByWindow</code>。在下图的<em>a</em>中对应的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b)</code>，也就是计算5s内的计数之后。图<em>b</em>的代码为<code>pairs.reduceByWindow("5s", (a, b) =&gt; a+b, (a, b) =&gt; a-b)</code>。其实很简单，第一个lambda表达式为进入滑动窗口的处理函数，第二个表达式为离开滑动窗口的处理函数。这样也就不用重复求和了。
 <img src="http://img-storage.qiniudn.com/15-10-18/97873121.jpg" alt="" /></li>
      <li>状态跟踪：
 如下图所示，就是保存上一个时间间隔的RDD与本次的记录进行groupBy加map计算的到状态的变化情况。
 <img src="http://img-storage.qiniudn.com/15-10-18/91458919.jpg" alt="" /></li>
    </ol>

    <p>在对这些带状态的操作的处理过程中也就用到了上述的所属的利用外存在保存中间的状态。spark streaming中这只发生在intervel之间，所以整个内存的状态管理会比传统的流式处理简单许多，而且高效，不需要对每一步都进行状态同步，状态恢复的成本也比较低，上文中提到的可以在多个节点上并行计算恢复。</p>
  </li>
</ol>

<h3 id="system-architecture">System Architecture</h3>
<p><img src="http://img-storage.qiniudn.com/15-10-18/82954556.jpg" alt="" /></p>

<p>Spark streaming和Spark的系统结构有些许改动，如上图所示主要包括3个部分：</p>

<ul>
  <li><em>master</em> that tracks the D-Stream lineage graph and schedules tasks to compute new RDD partitions.</li>
  <li><em>Worker</em> nodes that receive data, store the partitions of input and computed RDDs, and execute tasks.</li>
  <li>A <em>client</em> library used to send data into the system.</li>
</ul>

<p>从上图中可以看出，Spark Streaming和传统的流式系统最大的区别就是Spark Streaming将计算分成小的，无状态的确定性的任务，这些任务会在集群的任意节点上运行。并且相对于传统流式系统的拓扑结构来说，无需消耗大量时间将将任务进行迁移，Spark Streaming可以很好的对机器上的节点进行负载均衡，处理失败任务并且对慢节点进行预测。</p>

<p>对比于Spark的系统，Spark Streaming做了一下的改进：</p>

<ul>
  <li>网络传输。使用异步I/O获取远端数据。</li>
  <li>TimeStep pipelining。Spark的调度器可以在当前任务还未完成的时候可以提交下一个时间分片的任务。</li>
  <li>任务调度：优化任务调度器，例如调整控制消息的大小，可以在每隔几百毫秒时间内启动几百个并行任务。</li>
  <li>存储层：支持异步的RDD checkpoint，RDD是不可变的，所以异步存储不会阻塞现有的计算。</li>
  <li>Lineage切割：控制RDD linage graph的大小，在checkpoint之前的lineage便可以删除。</li>
</ul>

<p>当master fail的时候可以进行HA，所有的worker重新连接到新的master上，将原来的checkpoint和原始数据重新计算。因为所有的操作都是确定性的，所以RDD是可以重复计算，也就是说在HA的时候丢失一些正在运行的计算任务不会对最终结果造成什么影响。所有的元数据都是存储在HDFS上的，包括：</p>

<ol>
  <li>RDD的lineage graph，代表用户代码的Scala函数对象。</li>
  <li>上一个checkpoint的时间</li>
  <li>RDD的ID。因为HDFS的checkpoint文件会在每个时间片重新命名。</li>
</ol>

<h3 id="faq">FAQ</h3>
<ol>
  <li>
    <p>Dstream与RDD之间的关系</p>

    <p>首先来看下Spark streaming的代码<code>val ssc = new StreamingContext(sc, Seconds(2))</code>。在这句的作用是定义Dstream生成的时间间隔，<code>2s</code>就是这个时间间隔，也叫<code>batch interval</code>。具体说来<strong>一个streaming batch对应一个RDD</strong>，也就是这个batch interval里产生的数据。</p>

    <p>在这个RDD中，有n个partition，n = batch interval / block interval。 <code>block interval</code>是spark steaming内部定义的一个变量<code>spark.streaming.blockInterval</code>，通常是200ms。上述例子就产生10个partitions。</p>

    <p>Blocks由一个receiver产生，receiver就是流式数据的接收端，每个receiver被分配到一个host上，所以上述的1-个partitions就由一个node产生，同时被复制到第二个节点上做容错。注意，这里产生了data locality的问题。好的做法是，分配多个receivers接收数据，最后使用union合并数据做processing。当然还可以对Dstream做<code>repartition</code>操作提高并行度。</p>
  </li>
  <li>
    <p>时间窗口和job的关系</p>
  </li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/10/27/spark-introduction/"/>
    <updated>2015-10-27T21:00:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/10/27/spark-introduction</id>
    <content type="html"><![CDATA[<p>简单介绍下Spark。Spark是分布式的内存计算模型，对于两类计算形式能够极大地提升处理的效率：迭代计算，和交互式的数据挖掘工具。迭代计算例如PageRank、K-means聚类、逻辑回归等要求计算结果的重新利用，交互式的数据挖掘例如针对一份的数据集进行多次特定查询也要求原始数据的重复利用。对比于MR作业，它不需要外部存储的介入，从而提升处理速度。接下来将会从Spark的数据模型、编程模型和架构来介绍Spark。</p>

<p><img src="https://spark.apache.org/images/spark-logo.png" alt="image" /></p>

<!--more-->

<h3 id="rdd-data-model">RDD Data Model</h3>
<p>那么如何利用多个节点的内存进行分布式的计算呢，这就是Spark的核心RDD（Resilient Distributed Dataset）。RDD是一个个记录的集合，只读和分片的。它只能通过外部存储或者其他RDD生成。RDD的这些变化操作名称为<em>transformation</em>，在Spark编程中还有另外一个操作需要知道，<em>action</em>，意思是返回一个值或者将数据导出到外部存储，像<code>count</code>、<code>collect</code>、<code>save</code>。一个spark程序往往是从外部存储中定义一个或多个RDD开始的，接着经过各种<em>transformation</em>，最后<em>action</em>。同时<em>action</em>也是计算的开始，在spark中计算是延迟的，各种<em>transformation</em>形成了computation pipeline在<em>action</em>中进行计算。</p>

<p>下表是对RDD中的<em>transformation</em>和<em>action</em>操作：
<img src="http://ww2.sinaimg.cn/large/74311666jw1ex310kinr3j210c0fujwg.jpg" alt="" /></p>

<p>RDD还有两个操作：<em>persistence</em>和<em>partitioning</em>。其实顾名思义，<em>persistence</em>即RDD的持久化操作，这是为了避免重复计算。<em>partitioning</em>是重新分区，为了得到更好的执行并发度。</p>

<p>合理的分区可以有效减少shuffle的数据量，根据特定的应用场景执行不同的<em>partitioning</em>。例如在PageRank中根据域名来对URL进行Hash，因为很多链接都是内部的。下图表示<em>partitioning</em>的效果。</p>

<p><img src="http://img-storage.qiniudn.com/15-10-16/6543188.jpg" alt="" /></p>

<p>持久化操作是也是为了更高的计算效率。例如在下图中，Spark有两个action，所以产生两个job。两个job各自计算RDD1和RDD2，对于数据量大的RDD无疑会影响性能，所以可以将RDD进行<em>persistence</em>操作，可以存入内存、硬盘或者二者结合。后续缓存的资源可以手动清除或者通过LRU算法自动清除。
<img src="http://ww2.sinaimg.cn/large/74311666jw1ex30g3zpazj20u60f2tb9.jpg" alt="" /></p>

<p>在上文中我们提到RDD是有<code>transformation</code>和<code>lazy compute</code>的特性的。这两个特性使得RDD不需要一直被实例化，只需要保存这个RDD是如何产生的，在延迟计算的时候便可以通过这些dependence信息进行RDD transformation操作。以上就是RDD lineage，一个RDD的血统关系图。RDD的只读特性也是为了更好地描述这个lineage graph。在上图中两个Output的产生也可以直观地看到RDD的lineage信息。那么这个linage究竟有什么好处呢，RDD可以根据dependency信息直接追踪到在外存中的数据，在发生错误的时候直接通过外存的原始数据计算丢失或错误的RDD分片信息。</p>

<h3 id="rdd-programming-model">RDD Programming Model</h3>

<p>RDD的编程模型需要通过适当的接口来表示RDD是如何经过一系列的transformations来达到现在的状态。在Spark中，RDD的编程模型暴露了5方面的信息：</p>

<ul>
  <li>dateset中的分片信息</li>
  <li>父RDD的依赖关系</li>
  <li>基于其父RDD的计算RDD方法</li>
  <li>分片的shceme元数据</li>
  <li>数据存放的位置
<img src="http://ww3.sinaimg.cn/large/74311666jw1ex32afwa8gj20qq0ds41v.jpg" alt="" /></li>
</ul>

<p>在第3个方法中，也就是根据父RDD分片计算本RDD的分片有两种情况：每个父RDD分片最多被一个子RDD分片依赖；父RDD分片被多个子RDD分片依赖。这两种情况分别对应<em>narrow dependency</em>和<em>wide dependency</em>。为什么区分这两种依赖关系呢，这和RDD的延迟计算特性有关系。</p>

<ul>
  <li>在<em>narrow dependency</em>中，一个节点上的RDD分片可以通过pipeline的方式从原始数据开始计算，多个分片可以并行的进行。而对于<em>wide dependency</em>需要所有父RDD的分片可用，而且涉及到data shuffle。</li>
  <li><em>narrow dependency</em>在节点失效后的恢复的效率更高，因为可以并行地在不同的节点上计算丢失的分片。而在<em>wide dependency</em>中，一个父RDD分片可能被多个子RDD分片使用，所以可能导致这些子RDD的祖先分片都有丢失，所以需要重新完整的计算。</li>
</ul>

<p>整个spark作业的执行调度也是和这两种dependency有关。当一个用户执行一个<em>action</em>操作的时候，spark的调度器检测RDD的lineage graph，建立一个DAG图来执行，DAG图中的每个节点就是一个<em>stage</em>。在每个<em>stage</em>中包含了多个pipeline的<em>transformation</em>操作，这些RDD的关系全都是
<em>narrow dependency</em>。每个<em>stage</em>的边界都是由<em>wide dependency</em>的shuffle操作，或者已经计算好的分片。调度器这时候就可以建立执行任务计算每个stage中缺失的<em>partitions</em>，直到得到最终的RDD。</p>

<p>那么如何合理划分 stage，并确定 task 的类型和个数？
<img src="http://img-storage.qiniudn.com/15-10-16/48846624.jpg" alt="" /></p>

<p>可以看到在上图中，每个stage中的数据都是形成了pipeline计算的，这里的pipeline思想是：<strong>数据用的时候再算，而且数据是流到要计算的位置的</strong>。有两层意思，一是延迟计算，二是计算本地化。比如在第一个 task 中，从 FlatMappedValuesRDD 中的 partition 向前推算，只计算要用的（依赖的） RDDs 及 partitions。在第二个 task 中，从 CoGroupedRDD 到 FlatMappedValuesRDD 计算过程中，不需要存储中间结果（MappedValuesRDD 中 partition 的全部数据）。</p>

<p>在有<em>wide dependency</em>的时候需要Shuffle后无法进行pipeline。那么我们可以<strong>从后往前推算，遇到 <em>wide dependency</em>就断开，遇到<em>narrow dependency</em>就将其加入该stage。每个stage里面task 的数目由该stage最后一个RDD中的partition个数决定</strong>。因此上图中最后一个stage的id是0，stage 1  stage 2都是stage 0的parents。</p>

<p>整个的computing chain也是根据数据依赖关系自后向前建立，遇到<em>wide dependency</em>后形成 stage。computing chain从后到前建立，而实际计算出的数据从前到后流动，那么RDD内部是如何实现计算的呢。在每个stage中，每个RDD中的compute()调用parentRDD.iter()来将parent RDDs中的 records一个个fetch过来。</p>

<blockquote>
  <p>代码实现：每个 RDD 包含的 getDependency() 负责确立 RDD 的数据依赖，compute() 方法负责接收 parent RDDs 或者 data block 流入的 records，进行计算，然后输出 record。经常可以在 RDD 中看到这样的代码firstParent[T].iterator(split, context).map(f)。firstParent 表示该 RDD 依赖的第一个 parent RDD，iterator() 表示 parentRDD 中的 records 是一个一个流入该 RDD 的，map(f) 表示每流入一个 recod 就对其进行 f(record) 操作，输出 record。为了统一接口，这段 compute() 仍然返回一个 iterator，来迭代 map(f) 输出的 records。</p>
</blockquote>

<p>在实现中，每个task是基于数据的本地性进行分配的，任务是在该分片的节点上执行的。而对于<em>wide dependecy</em>，会在拥有父分片的节点上计算中间结果，这是为了减少fault recovery的时间。</p>

<h3 id="spark-architecture">Spark Architecture</h3>
<p>在上一节中我们确定了Spark stage和task的生成和执行方式，那么这些stage和task在整个spark application中又处于什么样的位置呢，上文中我们提到的每个job又是什么意思呢，为什么一个application中又会有好多个job，它们之间的关系又是什么样的呢？</p>

<p><img src="http://spark-internals.books.yourtion.com/markdown/PNGfigures/deploy.png" alt="image" /></p>

<p>从部署图中可以看到</p>

<ul>
  <li>整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。</li>
  <li>Master 节点上常驻 Master 守护进程，负责管理全部的 Worker 节点。</li>
  <li>Worker 节点上常驻 Worker 守护进程，负责与 Master 节点通信并管理 executors。</li>
  <li>Driver 官方解释是 “The process running the main() function of the application and creating the SparkContext”。Application 就是用户自己写的 Spark 程序（driver program），比如 WordCount.scala。</li>
  <li>每个 Worker 上存在一个或者多个 ExecutorBackend 进程。每个进程包含一个 Executor 对象，该对象持有一个线程池，每个线程可以执行一个 task。</li>
  <li>每个 application 包含一个 driver 和多个 executors，每个 executor 里面运行的 tasks 都属于同一个 application。</li>
</ul>

<p>在最开始的时候我们介绍了RDD的<code>action</code>操作，还提到每个<code>action</code>就是一个job，事实上上表中的<code>action</code>操作其实是论文中的，实际上还有更多的<code>action</code>。如下表：</p>

<table>
<thead>
<tr>
<th style="text-align:left">Action</th>
<th style="text-align:left">finalRDD(records) =&gt; result</th>
<th style="text-align:left">compute(results)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">reduce(func)</td>
<td style="text-align:left">(record1, record2) =&gt; result, (result, record i) =&gt; result</td>
<td style="text-align:left">(result1, result 2) =&gt; result, (result, result i) =&gt; result</td>
</tr>
<tr>
<td style="text-align:left">collect()</td>
<td style="text-align:left">Array[records] =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">count()</td>
<td style="text-align:left">count(records) =&gt; result</td>
<td style="text-align:left">sum(result)</td>
</tr>
<tr>
<td style="text-align:left">foreach(f)</td>
<td style="text-align:left">f(records) =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">take(n)</td>
<td style="text-align:left">record (i&lt;=n) =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">first()</td>
<td style="text-align:left">record 1 =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">takeSample()</td>
<td style="text-align:left">selected records =&gt; result</td>
<td style="text-align:left">Array[result]</td>
</tr>
<tr>
<td style="text-align:left">takeOrdered(n, [ordering])</td>
<td style="text-align:left">TopN(records) =&gt; result</td>
<td style="text-align:left">TopN(results)</td>
</tr>
<tr>
<td style="text-align:left">saveAsHadoopFile(path)</td>
<td style="text-align:left">records =&gt; write(records)</td>
<td style="text-align:left">null</td>
</tr>
<tr>
<td style="text-align:left">countByKey()</td>
<td style="text-align:left">(K, V) =&gt; Map(K, count(K))</td>
<td style="text-align:left">(Map, Map) =&gt; Map(K, count(K))</td>
</tr>
</tbody>
</table>

<p>用户的 driver 程序中一旦出现 action()，就会生成一个 job，比如 foreach() 会调用sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(f))，向 DAGScheduler 提交 job。如果 driver 程序后面还有 action()，那么其他 action() 也会生成 job 提交。所以，driver 有多少个 action()，就会生成多少个 job。这就是 Spark 称 driver 程序为 application（可能包含多个 job）而不是 job 的原因。</p>

<p>每一个 job 包含 n 个 stage，最后一个 stage 产生 result。。在提交 job 过程中，DAGScheduler 会首先划分 stage，然后先提交无 parent stage 的 stages，并在提交过程中确定该 stage 的 task 个数及类型，并提交具体的 task。无 parent stage 的 stage 提交完后，依赖该 stage 的 stage 才能够提交。从 stage 和 task 的执行角度来讲，一个 stage 的 parent stages 执行完后，该 stage 才能执行。</p>

<p>Spark就先介绍到这里，没有涉及到spark的具体内部实现，包括job的调度，shuffle的过程、文件的管理、cache和broadcast机制等。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Compression Format Introduction]]></title>
    <link href="http://billowkiller.github.io/blog/2015/09/18/compression-format-introduction/"/>
    <updated>2015-09-18T13:34:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/09/18/compression-format-introduction</id>
    <content type="html"><![CDATA[<h1 id="section">压缩格式</h1>
<p>最近在做一个日志的压缩策略，在此就记录下几种不同的压缩算法。本文不涉及每种算法的具体实现，只是用于介绍概念。</p>

<!--more-->

<h2 id="gzip">Gzip</h2>
<p>说到gzip，其实我是不想写的，满满的都是痛苦的回忆啊。在研究生时期曾经写过一个程序，在网关出对用户访问的网页进行hack，在网页的最后加上一个网页跳转的代码。由于网页传输的压缩编码就是gzip，具体做法是获取gzip的头文件后，构造压缩后的数据添加到原始的网页数据上。这里面很有意思的是，gzip是bit-oriented的，所以需要定位去除EOF marker后的数据最后的bit位置，再插入构造好的压缩数据。这里面涉及到的东西比较多，包括对gzip头文件的分析，找到每个单词对应的huffman编码；对压缩数据的反解找到对应的bit位；还有HTTP传输编码替换，所有的编码都改为CHUNKED编码方式。言归正传，那么什么是Gzip压缩编码？</p>

<p>Gzip是基于DEFLATE压缩算法的，它是由LZ77和Huffman编码的组成。LZ77其实很好理解，就是将重复的字符串用数字表示，标记为(length, distance)，相当于建立一个索引，后续出现的数据都可以通过这个索引找到原始的数据。举个简单的例子A A A A A A B A B A A A A A 经过LZ77编码后转化为A A A A A B &lt;2,2&gt; &lt;5,8&gt;, 可以看到A B和A A A A A是重复的，用长度和距离的组合来表示。Huffman编码是一种可变字长编码，依据字符出现概率来构造字符的二进制表示，用于保证了按字符出现概率分配码长，使平均码长最短。</p>

<p>通常我们说的gzip是指gzip文件格式，它的构成如下：</p>

<ul>
  <li>10字节的头，包括magic number，版本号和时间戳</li>
  <li>可选的附加字段，例如源文件名</li>
  <li>正文，也就是经过DEFLATED压缩后的数据</li>
  <li>8字节的EOF marker，包括CRC-32校验和和原始未压缩数据的长度。</li>
</ul>

<p>详细的文件格式如下：</p>

<p><img src="http://img-storage.qiniudn.com/15-9-18/30592957.jpg" alt="" /></p>

<h2 id="lzo">lzo</h2>

<p>lzo令人郁闷的一点是压缩算法是定义好的，但是传输和存储的格式并没有定义好。这就导致不同的厂商在lzo文件压缩格式上有不同的解决方案，在查找lzo资料的时候我就曾见过三种不同的传输或存储格式。那么为什么会有这种情况发生呢，原因就在于lzo lib包中只是定义了压缩的算法，且算法有好几十种。lib包关心的是如何将一串文本转成压缩好的字符串数据，而不关心客户端和服务端是否沟通协调好具体的算法，在传输或存储时候是否会有数据损坏的情况，所以在对lzo压缩进行产品化的时候我们需要考虑这些情况。</p>

<p>在hadoop中，lzo所采用的和<em>lzop</em>相同的压缩格式，lzop是神马呢，官网简介如下：
&gt;lzop is a file compressor which is very similar to gzip. lzop uses the LZO data compression library for compression services, and its main advantages over gzip are much higher compression and decompression speed (at the cost of some compression ratio).</p>

<p>lzop定的的头文件包含的信息如下：</p>

<pre><code>typedef struct {
    unsigned char magic[9]
    uint16_t version;
    uint16_t lib_version;
    uint16_t version_needed_to_extract;
    unsigned char method;
    unsigned char level;                // ignore
    uint32_t flags;                     
    uint32_t filter;                    // filter is not supported
    uint32_t mode;                      // ignore
    uint32_t mtime_low;                 // ignore
    uint32_t mtime_high;                // ignore
    unsigned char filename_len;
    uint32_t checksum;
} header_t;
</code></pre>

<p>在lzop的头文件格式中中还有定义一些extra fields，filename，这些在hadoop的解压算法中就直接给忽略了，所以这里也就没有添加上去，<code>header_t</code>中都是一些必要的占位字段，有些无意义，有些有意义。其中在<code>flags</code>中定义了一些是压缩格式实现的细节，例如压缩前数据的校验和算法，压缩后数据的校验和算法，是否有filter，是否是多文件合并等等。<code>method</code>字段定义了具体的压缩算法，使用的比较多的是M_LZO1X<em>1、M_LZO1X</em>1<em>15、M_LZO1X</em>999. 在官方的lib包中又对各种压缩算法的一个比较，通常情况下使用M_LZO1X_1就足够了。</p>

<p>lzo的压缩是面向块的，所以正文部分也就是由一块一块的压缩块构成，每块的压缩格式如下：</p>

<pre><code>/*
 * uncompressed block size
 * compressed block size
 * uncompressed block checksum
 * compressed block checksum
 * compressed block
 */
</code></pre>

<p>最后写一个EOF marker，也就是4bytes的0.</p>

<h2 id="snappy">snappy</h2>

<p>Google开发的一个 C++ 的用来压缩和解压缩的开发包，旨在提供高速压缩速度和合理的压缩率，Snappy 比 zlib 更快，但文件相对要大 20% 到 100%。Snappy特地为64位x86处理器做了优化，在单个Intel Core i7处理器内核上能够达到至少每秒250MB的压缩速率和每秒500MB的解压速率。Snappy的压缩是属于LZ77 体系，不同于gzip，它是byte-oriented，也就是说数据块之间是有严格的字节区分的；而gzip由于最后加入了Huffman编码，所以它是bit-oriented。byte-oriented的好处是处理起来比较简单，不需要记录字节中的位占用情况和加入数据的位迁移。</p>

<p>Snappy的压缩方式有一个frame的概念，这个帧并不是Snappy压缩所必须的，这个帧其实也就是Snappy的压缩文本加上一些必要的其他信息，例如checksum。Snappy中帧与帧之间是相互独立的，也就是说在压缩过程中你可以不关心其他帧的情况，只需要压缩好自己的数据然后发送或存储，解压的过程中会根据帧的粒度进行解压。这就意味着在分布式的环境中，你可以很容易的合并各个节点上的压缩数据，无需和其他节点进行同步，这个在其他算法中是无法办到的。</p>

<p>Snappy文件只由一个个连续的chunk构成，每个chunk的构成包括：1字节的chunk标识符，3字节的chunk长度，接着就是数据。chunk长度可以为0，这就表示该chunk的数据不存在。Snappy文件的第一个chunk是stream chunk，固定的6字节长度，其中数据部分为”sNaPpY”。这个chunk可以理解为Snappy的头文件，但是这个头文件可以出现多次，这也就是每个Snappy frame可以独立的原因之一；另外一个原因是Snappy并没有EOF marker。其他chunk的类型还包括：</p>

<ul>
  <li>Compressed data</li>
  <li>Uncompressed data</li>
  <li>padding</li>
  <li>Reserved unskippable chunks</li>
  <li>Reserved skippable chunks</li>
</ul>

<h2 id="section-1">对比</h2>
<p>以下是在论文中找到的一些资料。</p>

<p><img src="http://img-storage.qiniudn.com/15-9-17/82781797.jpg" alt="" /></p>

<p><img src="http://ww1.sinaimg.cn/large/74311666jw1ew5aeta9loj20w10hfdi2.jpg" alt="" /></p>

<h2 id="section-2">其他</h2>
<p><a href="https://github.com/billowkiller/Codec">以上三种压缩格式的c++方法</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mapreduce framework]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/mapreduce-framework/"/>
    <updated>2015-07-26T17:23:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/mapreduce-framework</id>
    <content type="html"><![CDATA[<h2 id="mapreduce">MapReduce框架</h2>

<p>mapReduce 的输入是hdfs上存储的一系列文件集。在hadoop中，这些文件被一种定义了如何分割一个文件成分片的input format来分割，一个分片是一个文件基于字节的可以被一个map任务加载的一个块。</p>

<ul>
  <li>每个map任务被分为以下阶段：<code>record reader</code>，<code>mapper</code>，<code>combiner</code>，<code>partitioner</code>。Map任务的输出叫中间数据，包括keys和values，发送到reduce端。</li>
  <li>Reduce任务分为以下阶段：<code>shuffle</code>，<code>sort</code>，<code>reduce</code>，<code>output format</code>。运行map任务的节点会尽量选择数据所在的节点。这种情况下，不会出现网络传输，在本地节点就可以完成计算。</li>
</ul>

<p>过程如图所示，接下来的章节会一一介绍。</p>

<p><img src="https://farm3.static.flickr.com/2275/3529146683_c8247ff6db_o.png" alt="" /></p>

<!--more-->

<h3 id="input-format">Input Format</h3>

<p>Record reader会把根据input fromat生成输入分片翻译成records。Record reader的目的是把数据解析成记录，而不是解析数据本身。它把数据以键值对的形式传递给mapper。通常情况下键是偏移量，值是这条记录的整个字节块。从Input file到map的中间过程如下图所示</p>

<p><img src="http://i12.tietuku.com/691b0fd1648b0497.png" width="450px" /></p>

<p>InputFormat其实做了三件事：</p>

<ul>
  <li>校验job的input configuration（比如，查看数据是否存在）。</li>
  <li>split输入的数据文件为逻辑上分片InputSplit，每个InputSplit给接下来的map task处理。</li>
  <li>创建RecordReader从InputSplit中解析出一个个键值对，这个键值对就是Record。</li>
</ul>

<p>如果要自定义InputFormat则最重要的是两个方法：</p>

<ul>
  <li><code>public List&lt;InputSplit&gt; getSplits(JobContext context)</code></li>
  <li><code>public RecordReader createRecordReader(InputSplit split, TaskAttemptContext context)</code></li>
</ul>

<p>通常在处理文本文件的时候，为了保证记录的完整性，RecorderReader会读取超过InputSplit边界的数据。</p>

<p><img src="http://i5.tietuku.com/392e9f9f99737b4d.jpg" alt="" /></p>

<p>在上图中共有三个InputSplit，在hadoop中默认的InputSplit大小为HDFS中每个Block的大小，所以共产生三个map task，它们读取数据的情况如下：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Start</th>
      <th>Actual Start</th>
      <th>End</th>
      <th>Line(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mapper1</td>
      <td>B1:0</td>
      <td>B1:0</td>
      <td>B2:150</td>
      <td>L1, L2, L3</td>
    </tr>
    <tr>
      <td>Mapper2</td>
      <td>B2:128</td>
      <td>B2:150</td>
      <td>B3:300</td>
      <td>L4, L5, L6</td>
    </tr>
    <tr>
      <td>Mapper3</td>
      <td>B3:256</td>
      <td>B3:300</td>
      <td>B3:300</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<p>现有的InputFormat包括：</p>

<ul>
  <li>TextInputFormat，Hadoop中默认的InputFormat，每行都是一个record，以偏移量为key</li>
  <li>KeyValueTextInputFormat， 可以指定key value分割符的TextInputFormat</li>
  <li>NLineInputFormat, mapper接受固定行数的记录</li>
  <li>SequenceFileInputFormat，二进制的KV
    <ul>
      <li>SequenceFileAsTextInputFormat，文本形式的KV</li>
      <li>SequenceFileAsBinaryInputFormat</li>
      <li>FixedLengthInputFormat</li>
    </ul>
  </li>
  <li>MultipleInputs</li>
  <li>DBInputFormat</li>
</ul>

<h3 id="map">Map</h3>

<p>Map阶段，会对每个从RecordReader读取的Record键值对执行用户代码，这些键值对又叫中间键值对。键和值的选择不是任意的，并且对MapReduce job的成功非常重要。键会用来分组，值是reducer端用来分析的数据。。</p>

<h3 id="combiner">Combiner</h3>
<p>Combiner 是一个map阶段分组数据，可选的，局部reducer。它根据用户提供的方法在一个mapper范围内根据中间键去聚合值。例如：数的总和是各个部分数量的和，你可以先计算中间的数目，最后再把所有中间数目加起来。很多情况下，这样能减少数据的网络传输量。发送（hello world，1）三次很显然要比发送（hello world，3）需要更多的网络传输字节量。</p>

<p><img src="http://i12.tietuku.com/009804b71667c5b9.png" width="500px" /></p>

<p>上图所示就是combiner发生的时机，它发生在map side写入磁盘的时候，可能会发生两次，一次是在Spill的时候，一次是在Merge的时候。在这两个阶段之前，是有一个sort的过程，这是为了最大化地提高combiner效率。其实从结构和作用来看，combiner函数和reducer函数的作用是相同的，很多情况下，Combiner也是直接用Reducer。</p>

<h3 id="partitioner">Partitioner</h3>

<p>Partitioner很简单，它决定哪个reducer接收该mapper数据记录，默认的是一个hash函数，对key进行hash之后取reducer个数的模。Partitioner会获取从mapper（或combiner）来的键值对，并分割成分片，每个reducer一个分片。默认用哈希值，典型使用md5sum。然后partitioner根据reduce的个数执行取余运算：key.hashCode() % (number of reducers)。这样能随即均匀的根据key分发数据到reduce，但仍然要保证不同mapper的相同key要到同一个reduce。Partitioner也可以自定义，使用更高级的样式，例如排序。然而，更改partitioner很少用。Partitioner的每个map的数据会写到本地磁盘，并等待对应的reducer检测，拿走数据。</p>

<h3 id="shuffle-and-sort">Shuffle and sort</h3>
<p>虽然说这个阶段主要是在Reduce端，但是Map端的一些行为也会影响到Reduce端。</p>

<p><img src="http://i5.tietuku.com/952140624345e77f.png" width="500px" /></p>

<p>上图是Map端的Shuffle过程，从整个过程来看，最重的部分在于Spill和Merge这两个I/O相关的操作，所有的数据需要从磁盘中读取后又重新写入到磁盘中，所以理想情况下是能够将所有mapper的output都装入到内存中。</p>

<p>Reduce任务开始于shuffle和sort阶段，Reduce将会开启多个fetcher从每个节点上的shuffle service中获取数据流。这一阶段获取partitioner的输出文件，并下载到reduce运行的本地机器。在下载的过程中，map的output首先会写入到内存中并进行排序，在数据达到一定量之后spill到磁盘，会有一个后台程序不断merge这些spilled file。最终，所有的output会合并成一个大的文件。这一阶段不能自定义，由框架自动处理。需要做的只是key的选择和可以自定义个用于分组的比较器。整个过程如下图所示。</p>

<p><img src="http://i5.tietuku.com/d11419d045f44d9a.png" width="500px" /></p>

<h3 id="reduce">Reduce</h3>
<p>Reduce 任务会把分组的数据作为输入并对每个key组执行reduce方法代码。方法会传递key和可以相关的所有值得迭代集合。很多的处理会在这个方法里执行，也就会有很多的模式。一旦reduce方法完成，会发送0或多个键值对到output format。跟map一样，不同的reduce依据不同的逻辑情形而不同。</p>

<h3 id="output-format">Output format</h3>
<p>Output Format会把reduce阶段的输出键值对根据record writer写到文件里。默认用tab分割键值对，用换行分割不同行。这里也可以自定义为更丰富的输出格式，最后，数据被写到hdfs。整个过程类似于InputFormat。</p>

<p><img src="http://i12.tietuku.com/cbdb549a3e19f898.png" width="500px" /></p>

<p>LazyOutputFormat 用来保证output (part-r-nnnnn) files有数据，不会存在空文件。</p>

<h3 id="output-commiter">Output Commiter</h3>

<p>Hadoop使用<code>OutputCommitter</code>来保证作业和任务的事务性。在旧的API中需要显示的使用<code>setOutputCommitter</code>或者设置<code>mapred.output.committer.class</code>。
而在新的API中，<code>OutputCommitter</code>是由<code>OutputFormat</code>通过<code>getOutputCommitter()</code>方法决定的。默认的是<code>FileOutputCommitter</code>，它适用于所有的基于文件的MapReduce。</p>

<p><code>OutputCommitter</code>API如下：</p>

<pre><code>public abstract class OutputCommitter {
	public abstract void setupJob(JobContext jobContext) throws IOException; 
	public void commitJob(JobContext jobContext) throws IOException { } 
	public void abortJob(JobContext jobContext, JobStatus.State state) throws IOException { }
	public abstract void setupTask(TaskAttemptContext taskContext) throws IOException;
	public abstract boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException;
	public abstract void commitTask(TaskAttemptContext taskContext) throws IOException;
	public abstract void abortTask(TaskAttemptContext taskContext) throws IOException;
}
</code></pre>

<p><code>setupJob</code>方法在job运行前就被调用，用来服务于作业的初始化，类似创建作业和task的临时目录。</p>

<p>job成功后会调用<code>commitJob()</code>方法，用于删除临时目录和创建<em>_SUCCESS</em>文件。如果job失败，则调用<code>abortJob()</code>方法
表示作业失败或者被kill，默认情况下会删除临时目录。</p>

<p>在task级别的操作类似。Hadoop框架会保证在一个task中的多个task attempt中，只有一个会commit，其他abort。有两种情况：</p>

<ul>
  <li>第一个attempt失败的时候会abort，第二个attempt如果成功则会commit。</li>
  <li>对于预测任务，只要有一个首先成功的话会commit，另外一个则abort。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PID File Analysis]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/pid-file-analysis/"/>
    <updated>2015-07-26T16:57:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/pid-file-analysis</id>
    <content type="html"><![CDATA[<h1 id="pid-">pid 文件浅析</h1>

<p>在Linux系统的目录/var/run下面一般我们都会看到很多的*.pid文件。而且往往新安装的程序在运行后也会在/var/run目录下面产生自己的pid文件。那么这些pid文件有什么作用呢？它的内容又是什么呢？</p>

<!--more-->

<h3 id="pid">pid文件的内容</h3>

<p>pid文件为文本文件，内容只有一行, 记录了该进程的ID。
用cat命令可以看到。</p>

<h3 id="pid-1">pid文件的作用</h3>

<p>防止进程启动多个副本。只有获得pid文件固定路径固定文件名)写入权限(F_WRLCK)的进程才能正常启动并把自身的PID写入该文件中。其它同一个程序的多余进程则自动退出。</p>

<h3 id="section">编程技巧</h3>

<p>调用fcntl设置pid文件的锁定F_SETLK状态，其中锁定的标志位F_WRLCK。
如果成功锁定，则写入进程当前PID，进程继续往下执行。
如果锁定不成功，说明已经有同样的进程在运行了，当前进程结束退出。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>pid usage</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="n">lock</span><span class="p">.</span><span class="n">l_type</span> <span class="o">=</span> <span class="n">F_WRLCK</span><span class="p">;</span>
</span><span class="line"><span class="n">lock</span><span class="p">.</span><span class="n">l_whence</span> <span class="o">=</span> <span class="n">SEEK_SET</span><span class="p">;</span>
</span><span class="line"><span class="k">if</span> <span class="p">(</span><span class="n">fcntl</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">F_SETLK</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">lock</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">){</span>
</span><span class="line">    <span class="c1">//锁定不成功, 退出......</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="n">sprintf</span> <span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="s">&quot;%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">pid</span><span class="p">);</span>
</span><span class="line"><span class="n">pidsize</span> <span class="o">=</span> <span class="n">strlen</span><span class="p">(</span><span class="n">buf</span><span class="p">);</span>
</span><span class="line"><span class="k">if</span> <span class="p">((</span><span class="n">tmp</span> <span class="o">=</span> <span class="n">write</span> <span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="n">pidsize</span><span class="p">))</span> <span class="o">!=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">pidsize</span><span class="p">){</span>
</span><span class="line">    <span class="c1">//写入不成功, 退出......</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">一些注意事项：</h3>

<ol>
  <li>如果进程退出，则该进程加的锁自动失效。</li>
  <li>如果进程关闭了该文件描述符fd， 则加的锁失效。(整个进程运行期间不能关闭此文件描述符)</li>
  <li>锁的状态不会被子进程继承。如果进程关闭则锁失效而不管子进程是否在运行。
 (Locks are associated with processes. A process can only have one kind of lock set for each byte of a given file. When any file descriptor for that file is closed by the process, all of the locks that process holds on that file are released, even if the locks were made using other descripors that remain open. Likewise, locks are released when a process exits, and are not inherited by child processes created using fork.)</li>
</ol>

<h3 id="section-2">其他</h3>

<p><strong>C的fcntl函数：</strong></p>

<pre><code>int fcntl(int fd, int cmd, struct flock *lock);
</code></pre>

<p>参数cmd代表欲垄断的号召</p>

<ul>
  <li>F_DUPFD 复制参数fd的文件描写符，厉行获胜则归来新复制的文件描写符，</li>
  <li>F_GETFD 获得close-on-exec符号，若些符号的FD_CLOEXEC位为0，代表在调用exec()相干函数时文件将不会关闭</li>
  <li>F_SETFD 设置close-on-exec符号，该符号以参数arg的 FD_CLOEXEC位定夺</li>
  <li>F_GETFL 获得open()设置的符号</li>
  <li>F_SETFL 改换open()设置的符号</li>
  <li>F_GETLK 获得文件锁定的事态，依据lock的描写，定夺是否上文件锁</li>
  <li>F_SETLK 设置文件锁定的事态，此刻flcok，构造的l_tpye值定然是F_RDLCK、F_WRLCK或F_UNLCK，
万一无法发生锁定，则归来-1</li>
  <li>F_SETLKW 是F_SETLK的阻塞版本，在无法获得锁时会进去睡眠事态，万一能够获得锁可能捉拿到信号则归来</li>
</ul>

<p>参数lock指针为flock构造指针定义如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>flock structure</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="k">struct</span> <span class="n">flock</span> <span class="p">{</span>
</span><span class="line">	<span class="p">...</span>
</span><span class="line">	<span class="kt">short</span> <span class="n">l_type</span><span class="p">;</span>
</span><span class="line">	<span class="kt">short</span> <span class="n">l_whence</span><span class="p">;</span>
</span><span class="line">	<span class="kt">off_t</span> <span class="n">l_start</span><span class="p">;</span> <span class="err">锁定区域的开关位置</span>
</span><span class="line">	<span class="kt">off_t</span> <span class="n">l_len</span><span class="p">;</span> <span class="err">锁定区域的大小</span>
</span><span class="line">	<span class="n">pid_t</span> <span class="n">l_pid</span><span class="p">;</span> <span class="err">锁定动作的历程</span>
</span><span class="line">	<span class="p">...</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>1_type有三种事态：</p>

<ul>
  <li>F_RDLCK读取锁（分享锁）</li>
  <li>F_WRLCK写入锁（排斥锁）</li>
  <li>F_UNLCK解锁</li>
</ul>

<p>l_whence也有三种措施</p>

<ul>
  <li>SEEK_SET以文件开始为锁定的起始位置</li>
  <li>SEEK_CUR以现在文件读写位置为锁定的起始位置</li>
  <li>SEEK_END以文件尾为锁定的起始位置</li>
</ul>

<p><strong>Python用法:</strong></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_fcntl.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">signal</span><span class="o">,</span> <span class="nn">fcntl</span><span class="o">,</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">time</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">signal_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
</span><span class="line">    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">pid_file</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_UN</span><span class="p">)</span>
</span><span class="line">    <span class="n">pid_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="n">pid_path</span><span class="p">)</span>
</span><span class="line">    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
</span><span class="line">    <span class="n">pid_path</span> <span class="o">=</span> <span class="s">&#39;test.pid&#39;</span>
</span><span class="line">    <span class="n">pid_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">pid_path</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>
</span><span class="line">    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">pid_file</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_EX</span> <span class="o">|</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_NB</span><span class="p">)</span>
</span><span class="line">    <span class="n">pid_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%d</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
</span><span class="line">    <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">signal_handler</span><span class="p">)</span>
</span><span class="line">    <span class="k">while</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;Hello World&quot;</span>
</span><span class="line">        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java Log4j]]></title>
    <link href="http://billowkiller.github.io/blog/2015/07/26/java-log4j/"/>
    <updated>2015-07-26T15:28:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2015/07/26/java-log4j</id>
    <content type="html"><![CDATA[<h2 id="java-log4j-">Java Log4j 使用记录</h2>

<p>Log4j即为Java的日志记录框架，除了Java语言外，它还支持其他的语言接口：C、C++、.Net和PL/SQL。说道日志框架，其他使用较多的日志框架还包括Logback、SLF4J Simple Logging、Java Util Logging，这些日志框架都是大同小异，目的都是用来记录程序运行的状态。它们的区别主要是在用法和性能上，由于日志通常涉及到IO读写磁盘（或者是阻塞或者是异步），这需要耗费时间，假设应用系统的日志比较庞大，对性能要求比较高，那么就需要好好斟酌下使用的框架。这里有个图可以直观感受下几个框架的区别：</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/log%20comparision_zpstrg3kc1m.png" alt="image" /></p>

<p>上图中的数字是每秒的日志写入行数，<a href="https://docs.google.com/spreadsheet/ccc?key=0Alceaf46X4GPdHBoLTdYQ29nRDh6V1dRY00zT1FwWWc&amp;usp=sharing">点击更详细的信息</a>。</p>

<!--more-->

<h2 id="hello-word-example">Hello Word Example</h2>

<p>示例是使用maven来组织的。maven用的越多，就越觉得它的方便，不用关心jar包的管理，开发、测试和发布都只需要一行命令，还能还IDE完美的融合。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>pom.xml</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="xml"><span class="line"><span class="nt">&lt;dependency&gt;</span>
</span><span class="line">    <span class="nt">&lt;groupId&gt;</span>log4j<span class="nt">&lt;/groupId&gt;</span>
</span><span class="line">    <span class="nt">&lt;artifactId&gt;</span>log4j<span class="nt">&lt;/artifactId&gt;</span>
</span><span class="line">    <span class="nt">&lt;version&gt;</span>1.2.17<span class="nt">&lt;/version&gt;</span>
</span><span class="line"><span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>创建log4j.properties文件，放在resources文件夹下，这个文件夹的路径是src/main/resources。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>log4j.properties</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="properties"><span class="line"><span class="c"># Root logger option</span>
</span><span class="line"><span class="na">log4j.rootLogger</span><span class="o">=</span><span class="s">DEBUG, stdout, file</span>
</span><span class="line">
</span><span class="line"><span class="c"># Redirect log messages to console</span>
</span><span class="line"><span class="na">log4j.appender.stdout</span><span class="o">=</span><span class="s">org.apache.log4j.ConsoleAppender</span>
</span><span class="line"><span class="na">log4j.appender.stdout.Target</span><span class="o">=</span><span class="s">System.out</span>
</span><span class="line"><span class="na">log4j.appender.stdout.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
</span><span class="line"><span class="na">log4j.appender.stdout.layout.ConversionPattern</span><span class="o">=</span><span class="s">%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n</span>
</span><span class="line">
</span><span class="line"><span class="c"># Redirect log messages to a log file, support file rolling.</span>
</span><span class="line"><span class="na">log4j.appender.file</span><span class="o">=</span><span class="s">org.apache.log4j.RollingFileAppender</span>
</span><span class="line"><span class="na">log4j.appender.file.File</span><span class="o">=</span><span class="s">C:\\log4j-application.log</span>
</span><span class="line"><span class="na">log4j.appender.file.MaxFileSize</span><span class="o">=</span><span class="s">5MB</span>
</span><span class="line"><span class="na">log4j.appender.file.MaxBackupIndex</span><span class="o">=</span><span class="s">10</span>
</span><span class="line"><span class="na">log4j.appender.file.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
</span><span class="line"><span class="na">log4j.appender.file.layout.ConversionPattern</span><span class="o">=</span><span class="s">%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>说明下ConversionPattern中的符号和参数：</p>

<ol>
  <li><code>%d{yyyy-MM-dd HH:mm:ss}</code> 日期和时间</li>
  <li><code>%-5p</code> 日志优先级, 输出DEBUG、ERROR等. <code>-5</code>是可选的, 格式化输出宽度，为了更好的输出效果.</li>
  <li><code>%c{1}</code> 是日志名称，通过getLogger()设置, 可以参考<a href="http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html">log4j PatternLayout guide</a>.</li>
  <li><code>%L</code> 打印日志的句子在源文件中的行数.</li>
  <li><code>%m%n</code> 打印的日志信息和换行符.</li>
</ol>

<p>上述设置的输出效果是：</p>

<pre><code>2014-07-02 20:52:39 DEBUG className:200 - This is debug message
2014-07-02 20:52:39 DEBUG className:201 - This is debug message2
</code></pre>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>HelloExample.java</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kn">import</span> <span class="nn">org.apache.log4j.Logger</span><span class="o">;</span>
</span><span class="line">
</span><span class="line"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">HelloExample</span><span class="o">{</span>
</span><span class="line">
</span><span class="line">    <span class="kd">final</span> <span class="kd">static</span> <span class="n">Logger</span> <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">HelloExample</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class="line">
</span><span class="line">    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">
</span><span class="line">        <span class="n">HelloExample</span> <span class="n">obj</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HelloExample</span><span class="o">();</span>
</span><span class="line">        <span class="n">obj</span><span class="o">.</span><span class="na">runMe</span><span class="o">(</span><span class="s">&quot;billowkiller&quot;</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="kd">private</span> <span class="kt">void</span> <span class="nf">runMe</span><span class="o">(</span><span class="n">String</span> <span class="n">parameter</span><span class="o">){</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span><span class="o">(</span><span class="n">logger</span><span class="o">.</span><span class="na">isDebugEnabled</span><span class="o">())</span> <span class="o">{</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">debug</span><span class="o">(</span><span class="s">&quot;This is debug : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="k">if</span><span class="o">(</span><span class="n">logger</span><span class="o">.</span><span class="na">isInfoEnabled</span><span class="o">())</span> <span class="o">{</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;This is info : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="s">&quot;This is warn : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;This is error : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">        <span class="n">logger</span><span class="o">.</span><span class="na">fatal</span><span class="o">(</span><span class="s">&quot;This is fatal : &quot;</span> <span class="o">+</span> <span class="n">parameter</span><span class="o">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">try</span><span class="o">{</span>
</span><span class="line">            <span class="mi">1</span><span class="o">/</span><span class="mi">0</span><span class="o">;</span>
</span><span class="line">        <span class="o">}</span><span class="k">catch</span><span class="o">(</span><span class="n">ArithmeticException</span> <span class="n">ex</span><span class="o">){</span>
</span><span class="line">            <span class="n">logger</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Sorry, something wrong!&quot;</span><span class="o">,</span> <span class="n">ex</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>可以调整配置文件中的<code>log4j.rootLogger</code>设置日志输出的级别和<code>appender</code>，级别从低到高有DEBUG、INFO、WARN、ERROR、FATAL，一般使用INFO和ERROR。</p>

<p>顺便说一句，类似<code>logger.isDebugEnabled()</code>这种句子在源码中实在是不太美观。如果debug中间的信息不影响程序的性能还是不要使用这种句子，因为log4j自己会检查需不需要输出debug信息。</p>

<h3 id="tips">Tips</h3>

<ul>
  <li>java -calsspath *.jar 加载多个jar包，而多个jar包中可能包含多个log4j.properties的时候，系统自动加载第一个，忽略后面的。也就是说将你想要使用的log4j.properties所包含的jar文件放在classpath的第一个。</li>
  <li>mvn的package和test可以分别对应两个不同的log4j.properties中，如下：src/main/resources/log4j.properties, src/test/resources/log4j.properties。</li>
  <li><strong>需要注意程序的输出级别，用户可以定义不同的配置文件来输出不同级别的信息</strong>。这一点尤其重要：
    <ul>
      <li>工具是为程序提供服务的，在开发和调试阶段，日志可以帮助我们更好更快地定位bug；在运行维护阶段，日志系统又可以帮我们记录大部分的异常信息，从而帮助我们更好的完善系统。</li>
      <li>定义好级别和输出的日志信息，配合使用<code>grep</code>等命令行工具，可以更好的帮助我们定位错误。</li>
      <li>在设计系统之前需要先确定好日志子系统，在什么情况下程序会发生错误，输出什么样的错误信息，在debug和正常运行的时候日志如何调整，error信息是否需要另外打印。</li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Static and Dynamic Library]]></title>
    <link href="http://billowkiller.github.io/blog/2014/08/29/static-and-dynamic-library/"/>
    <updated>2014-08-29T02:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/08/29/static-and-dynamic-library</id>
    <content type="html"><![CDATA[<p>库是写好的现有的，成熟的，可以复用的代码。现实中每个程序都要依赖很多基础的底层库。C默认使用的库就是<code>libc.so</code>。</p>

<p>本质上来说库是一种可执行代码的二进制形式，可以被操作系统载入内存执行。库有两种：静态库（.a、.lib）和动态库（.so、.dll）。</p>

<p>所谓静态、动态是指链接。</p>

<!--more-->

<h2 id="section">编译和链接</h2>

<p>将一个程序编译成可执行程序的步骤：</p>

<ol>
  <li>
    <p><strong>预处理：</strong>主要是处理源代码中以“#”开始的预编译指令，以及删除注释，添加行号和文件名标识。</p>

    <pre><code> gcc -E hello.c -o hello.i
</code></pre>
  </li>
  <li>
    <p><strong>编译：</strong> 把预处理完的文件进行一系列<strong>词法分析</strong>、<strong>语法分析</strong>、<strong>语义分析</strong>以及<strong>优化</strong>后<strong>生成相应的汇编代码文件</strong>。</p>

    <pre><code> gcc -S hello.i -o hello.s 
</code></pre>
  </li>
  <li>
    <p><strong>汇编：</strong>将汇编代码转变为机器代码。</p>

    <pre><code> as hello.s -o hello.o
 gcc -c hello.s -o hello.o
</code></pre>
  </li>
  <li>
    <p><strong>链接：</strong>链接过程主要包括了<strong>地址和空间分配</strong>、<strong>符号决议</strong>和<strong>重定位</strong>。</p>

    <p>链接所要做的工作也就是打补丁，将模块中引用其他模块变量或函数的地方由0修改成其他模块中的地址。这个修正的过程叫做重定位，每个要被修正的地方叫一个重定位入口。</p>

    <pre><code> ld a.o b.o -e main -o ab
</code></pre>
  </li>
</ol>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/gcc_zps9691e38f.png" alt="gcc过程分解" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/compile_zps2febeeff.gif" alt="" /></p>

<h2 id="section-1">静态链接</h2>

<p>链接器异步都采用一种叫两步链接的方法。也就是整个链接过程分两步。</p>

<ul>
  <li>
    <p>空间与地址分配</p>

    <p>链接器获得所有输入目标文件的段长度，并且将他们合并，计算出输出文件中各个段合并后的长度和位置，并建立映射关系。</p>

    <p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/compile2_zps2aaa528b.png" alt="" /></p>
  </li>
  <li>
    <p>符号解析与重定位</p>

    <p>通过空间与地址的分配可以得知，链接器在完成空间与地址的分配之后就可以确定所有符号的虚拟地址了，那么链接器就可以根据符号的地址对每个需要重定位的指令进行地址修正。</p>
  </li>
</ul>

<p>在ELF文件中，有个叫做<strong>重定位表</strong>的数据结构专门用来保存这些与重定位相关的信息，它在ELF文件中往往是一个或多个段。.text和.data段各有自己的重定位表，包含信息有<strong>重定位入口</strong>以及<strong>偏移</strong>。</p>

<p>重定位过程中，每个重定位的入口都是对一个符号的引用，当链接器需要对某个符号的引用进行重定位时，它就要确定这个符号的目标地址。这时候链接器就会去查找有所有输入目标文件的符号表组成的<strong>全局符号表</strong>，找到相应的符号后进行重定位。</p>

<p>在链接器扫描完所有的输入目标文件之后，所有这些未定义的符号都应该能够在全局符号表中找到，否则连接器就报<strong>符号未定义</strong>错误。</p>

<p><strong>静态库</strong>可以简单地看成一组目标文件的集合，即很多目标文件经过压缩打包后形成的一个文件。人们使用<code>ar</code>压缩程序将这些目标文件压缩到一起，并且对其进行编号和索引，以便于查找和检索。</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/compile3_zps21c35794.png" alt="" /></p>

<h2 id="section-2">可执行文件的装载</h2>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load1_zps072482ca.png" alt="" />
<img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load2_zpscd53f782.png" alt="" />
<img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load3_zpsf9324209.png" alt="" />
<img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load4_zpsc3f80dea.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load5_zps5d1477cf.png" alt="" /></p>

<p>ELF文件中，段的权限往往只有为数不多的几种组合，基本上是三种：</p>

<ul>
  <li>以代码段为代表的权限为可读可执行的段</li>
  <li>以数据段和.bss段为代表的可读可写段</li>
  <li>以制度数据段为代表的权限为只读的段</li>
</ul>

<p>操作系统通过给进程空间划分出一个个VMA来管理进程的虚拟空间，基本原则是将相同权限属性的、有相同映像文件的映射成一个VMA，这样可以减少页面内部碎片，从而节省了内存空间：</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load6_zpsde691d1c.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/load7_zps0008209b.png" alt="" /></p>

<p>操作系统堆的最大申请空间会受到操作系统版本、程序本身大小、用到的动态/共享库数量、大小、程序栈数量、大小的等的影响。甚至有可能每次运行的结果都会不同，因为有些操作系统使用了一种叫做随机地址空间分布的技术，使得进程的堆空间变小。</p>

<h3 id="section-3">程序的执行过程</h3>

<p>首先在用户层面，bash进程会调用<code>fork()</code>系统调用创建一个新的进程，新的进程调用<code>execve()</code>系统调用执行指定的ELF，原先的bash进程继续返回等待刚才启动的新进程结束，然后继续等待用户输入命令。</p>

<p>系统通过文件开头的魔数判断是ELF文件后调用<code>load_elf_binary()</code>;</p>

<ol>
  <li>检测ELF可执行文件格式的有效性</li>
  <li>寻找动态链接的“.interp”段，设置动态连接器路径</li>
  <li>根据ELF可执行文件的程序头表的描述，对ELF文件进行映射，比如代码、数据、只读数据。</li>
  <li>初始化ELF进程环境</li>
  <li>将系统调用的返回地址修改成ELF可执行文件的入口点，这个入口点取决于程序的连接方式，对于静态连接的ELF可执行文件，这个入口就是ELF文件的文件头中的<code>e_entry</code>所指向的地址；对于动态链接的ELF可执行文件，入口点就是动态链接器。</li>
</ol>

<p>指令寄存器设置成程序的入口点，启动程序。</p>

<p><strong>系统的动态链接器会将程序所需要的所有动态链接库装载到进程的地址空间，并且将程序中所有未决议的符号绑定到相应的动态链接库中，并进行重定位工作。</strong></p>

<p>我们在静态链接时提到过重定位，那时的重定位叫做<strong>链接时重定位</strong>（Link Time Relocation） ， 而现在这种情况经常被称为<strong>装载时重定位</strong>（Load Time Relocation）。</p>

<p>所以说装载时重定位一个很大的缺点就是指令部分无法在多个进程之间共享，这样就失去了动态链接节省内存的一大优势。 解决方法就是把指令中那些需要被修改的部分分离出来，跟数据部分放在一起，这样指令部分就可以保持不变， 而数据部分可以在进程中拥有一个副本。这种方案被称为 <strong>地址无关代码</strong>（PIC, Position-independent Code）技术。目的是希望程序模块中共享的指令部分在装载时不需要因为装载地址的改变而改变。</p>

<h2 id="section-4">动态链接</h2>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/dl1_zps2e9939ac.png" alt="" /></p>

<p>为什么需要动态库，其实也是静态库的特点导致。</p>

<ul>
  <li>空间浪费是静态库的一个问题。</li>
  <li>另一个问题是静态库对程序的更新、部署和发布页会带来麻烦。如果静态库liba.lib更新了，所以使用它的应用程序都需要重新编译、发布给用户（对于玩家来说，可能是一个很小的改动，却导致整个程序重新下载，全量更新）。</li>
</ul>

<p>动态库在程序编译时并不会被连接到目标代码中，而是等到程序要运行时才进行链接。<strong>不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例，规避了空间浪费问题</strong>。动态库在程序运行是才被载入，也解决了静态库对程序的更新、部署和发布页会带来麻烦。用户只需要更新动态库即可，<strong>增量更新</strong>。</p>

<p>动态库特点总结：</p>

<ul>
  <li>动态库把对一些库函数的链接载入推迟到程序运行的时期。</li>
  <li>动态链接的方式使得开发过程中各个模块更加独立、耦合度更小，便于不同的开发者和开发组织之间进行独立的开发和测试。</li>
  <li>程序在运行时可以动态的选择加载各种程序模块，使得插件成为可能。</li>
</ul>

<p>动态链接过程：</p>

<p><strong>链接时重定位（静态链接）；装载时重定位（动态链接）</strong></p>

<p><strong>地址无关代码</strong>：PIC，Position-independent Code，把指令中那些需要修改的部分分离出来，跟数据部分放在一起，这样指令部分就可以保持不变，而数据部分可以再每个进程中拥有个副本。</p>

<p>对于现代机器来说，产生地址无关的代码并不麻烦。我们首先把共享对象模块中的地址引用按照是否为跨模块分成两类： 模块内部引用和模块外部引用；按照不同的引用方式又可以分为指令引用和数据访问。 这样我们就得到了如下4种情况：</p>

<ol>
  <li>模块内部的函数调用、跳转等；</li>
  <li>模块内部的数据访问，比如模块中定义的全局变量、静态变量；</li>
  <li>模块外部的函数调用、跳转等；</li>
  <li>模块外部的数据访问，比如其它模块中定义的全局变量；</li>
</ol>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/dl2_zps9afd4901.png" alt="" /></p>

<p><strong>动态链接比静态链接慢</strong>的主要原因是动态链接下对于全局静态的数据访问要进行复杂的GOT定位,然后间接寻址.</p>

<p><strong>延迟绑定：</strong></p>

<p>由于很多函数在程序执行过程中不一定被用到（错误处理函数，特殊功能模块），ELF采用一种叫做 <strong>延迟绑定（Lazy Binding）</strong>的做法， 基本思想就是当函数第一次被用到时才进行绑定（符号查找、重定位等），如果没有用到则不进行绑定。可以大大加快程序的启动速度。</p>

<p>ELF使用PLT（Procedure Linkage Table）的方法来实现延迟绑定。PLT大致的工作原理如下：每个外部函数在PLT中都有一个相应的项，如果链接器在初始化阶段已经初始化该项，并且将函数地址填入该项，那么直接跳转到这个地址，实现函数的正确调用。否则调用_dl_runtime_resolve()函数，该函数利用重定位表完成符号解析和重定位工作。解析结束后将地址填入对应的项中。</p>

<p><strong>ELF中的字段：</strong></p>

<p>在动态链接的ELF可执行文件中，有一个专门的段叫做“.interp”，该段保存了需要的动态链接器需路径，一般是/lib/ld-linux.so.2。</p>

<p>动态链接ELF中最重要的结构应该是“.dynamic”段，这个段里面保存了动态连接器所需要的基本信息，比如依赖哪些共享对象、动态链接符号表的位置、动态链接重定位表的位置、共享对象初始化代码的地址等。</p>

<p>为了表示动态链接这些模块之间的符号导出与导入关系，ELF专门有一个叫做动态符号表的段用来保存这些信息。这个段叫做“.dynsym”。</p>

<p>动态链接的文件中，也有类似静态链接的重定位表，分别叫做“.rel.dyn”和“.rel.plt”，他们分别相当于“.rel.text”和“.rel.data”。</p>

<p><strong>动态链接的步骤和实现</strong>：</p>

<p>动态链接的步骤基本分为三步：先启动动态连接器本身，然后装载所有需要的共享对象，最后是重定位和初始化。</p>

<p>但是对于动态链接器本身来说，它的重定位工作是由谁来完成的？ 动态连接器本身通过自举（Bootstrap）来完成。完成基本自举之后，动态连接器将可执行文件和连接器本身的符号表都合并到一个符号表当中，我们可以称它为全局符号表（Global Symbol Table）。</p>

<p>动态连接器按照各个模块之间的依赖关系，当有两个不同的模块定义了同一个符号时怎么办？当一个符号需要被加入全局符号表时，如果相同的符号已经存在，则后加入的符号被忽略。</p>

<p><strong>显示运行时链接：</strong></p>

<p>动态库的装载则是通过一些列由动态连接器提供的API，具体是4个函数：打开动态库（<code>dlopen</code>）、查找符号（<code>dlsym</code>）、错误处理（<code>dlerror</code>）、以及关闭动态库（<code>dlclose</code>）。</p>

<p><code>dlopen()</code>函数用来打开动态库，并将其加载到进程的地址空间，完成初始化过程。</p>

<pre><code>void  *dlopen(const char *filename, int flag);
</code></pre>

<p><code>dlsym()</code>，我们通过该函数找到所需要的符号。</p>

<pre><code>dlsym(void  *handle,  char  *symbol);
</code></pre>

<h3 id="c">显式调用C++动态库注意点</h3>

<p>对C++来说，情况稍微复杂。显式加载一个C++动态库的困难一部分是因为C++的name mangling；另一部分是因为没有提供一个合适的API来装载类，在C++中，您可能要用到库中的一个类，而这需要创建该类的一个实例，这不容易做到。</p>

<p>name mangling可以通过<code>extern "C"</code>解决。C++有个特定的关键字用来声明采用C binding的函数：<code>extern "C"</code> 。用 <code>extern "C"</code>声明的函数将使用函数名作符号名，就像C函数一样。因此，只有非成员函数才能被声明为<code>extern "C"</code>，并且不能被重载。尽管限制多多，<code>extern "C"</code>函数还是非常有用，因为它们可以象C函数一样被<code>dlopen</code>动态加载。冠以<code>extern "C"</code>限定符后，并不意味着函数中无法使用C++代码了，相反，它仍然是一个完全的C++函数，可以使用任何C++特性和各种类型的参数。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Classical Sync Problem]]></title>
    <link href="http://billowkiller.github.io/blog/2014/08/23/classical-sync-problem/"/>
    <updated>2014-08-23T02:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/08/23/classical-sync-problem</id>
    <content type="html"><![CDATA[<p>简单的记录下，后期再整理吧。</p>

<hr />

<h2 id="section">生产者、消费者</h2>

<h3 id="section-1">多个生产者，单个消费者</h3>

<!--more-->

<p><strong>1. 使用互斥锁 &amp;&amp; 条件变量</strong></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc1_zps5e6026ee.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc2_zps96f163b4.png" alt="" /></p>

<p><strong>2. 使用信号量</strong></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc3_zps90f678c0.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc4_zpsff637264.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc5_zpsc1ce838d.png" alt="" /></p>

<h3 id="section-2">多个生产者，多个消费者</h3>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc6_zps5fd624fc.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc7_zps77aab9e4.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc8_zps7ec189ae.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/pc9_zps88ec073a.png" alt="" /></p>

<h2 id="section-3">多个缓冲区读写</h2>

<p>最简单的双缓冲区示意图</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/db1_zps098b0d0f.png" alt="" /></p>

<p><strong>多缓冲区生产者消费者代码</strong></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/db2_zps9fda343a.png" alt="" /></p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/db3_zps03dac974.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Design Pattern Abstraction]]></title>
    <link href="http://billowkiller.github.io/blog/2014/08/22/design-pattern/"/>
    <updated>2014-08-22T02:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/08/22/design-pattern</id>
    <content type="html"><![CDATA[<p>本文是对设计模式的一个备忘录，仅是做了些摘要，具体的设计模式学习还得看GOF。</p>

<p>这儿也有一个还不错的博客<a href="http://blog.csdn.net/zhengzhb/article/category/926691">http://blog.csdn.net/zhengzhb/article/category/926691</a>。</p>

<hr />

<p>面向对象设计（OOD）核心原则是<strong>高内聚低耦合</strong>。</p>

<ul>
  <li><em>高内聚</em>是指某个特定模块（程序，类型）都应完成一系列相关功能，描述了不同程序，类型中方法，方法中不同操作描述的逻辑之间的距离相近。高内聚意味可维护性，可重新性，因为模块对外部的依赖少（功能的完备性）。</li>
  <li><em>耦合</em>是描述模块之间的依赖程度。低耦合是我们的设计目的，但不是不存在耦合不存依赖，依赖是必须的，因为模块之间必须通信交互，不过应该设计依赖于不变或者不易变的接口，无需了解模块的具体实现（OO封装性）。</li>
</ul>

<p>使用<strong>设计模式</strong>是为了提高代码的工程化，提供更加高内聚、低耦合的代码。同时可重用代码、让代码更容易被他人理解、保证代码可靠性。套用一句话就是：</p>

<blockquote>
  <p>每个模式描述了一个在我们周围不断重复发生的问题，已经该问题的解决方案的核心。这样我们就能一次又一次地使用该方案而不必做重复劳动。</p>
</blockquote>

<p>设计模式分为三种类型，共23种。</p>

<ul>
  <li><strong>创建型模式：</strong>单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。</li>
  <li><strong>结构型模式：</strong>适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。</li>
  <li><strong>行为型模式：</strong>模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式、访问者模式。</li>
</ul>

<!--more-->

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/LinkSource/dp1_zps74894e4d.png" alt="" /></p>

<h2 id="section">创建型模式</h2>

<h3 id="factory">工厂模式（Factory）</h3>

<p>定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method 使一个类的实例化延迟到其子类。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>当一个类不知道它所必须创建的对象的类的时候。</li>
  <li>当一个类希望由它的子类来指定它所创建的对象的时候。</li>
  <li>当类将创建对象的职责委托给多个帮助子类中的某一个，并且你希望将哪一个帮助子类是代理者这一信息局部化的时候。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/factory_zpsc31117cb.gif" alt="" /></p>

<h3 id="abstract-factory">抽象工厂模式（Abstract Factory）</h3>

<p>提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>一个系统要独立于它的产品的创建、组合和表示时。</li>
  <li>一个系统要由多个产品系列中的一个来配置时。</li>
  <li>当你要强调一系列相关的产品对象的设计以便进行联合使用时。</li>
  <li>当你提供一个产品类库，而只想显示它们的接口而不是实现时。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/abstractfactory_zpsf3be4cdd.gif" alt="" /></p>

<h3 id="builder">建造者模式（Builder）</h3>

<p>将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>当创建复杂对象的算法应该独立于该对象的组成部分以及它们的装配方式时。</li>
  <li>当构造过程必须允许被构造的对象有不同的表示时。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/builder_zps7d31e2f5.gif" alt="" /></p>

<h3 id="prototype">原型模式（Prototype）</h3>

<p>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>当要实例化的类是在运行时刻指定时，例如，通过动态装载；或者</li>
  <li>为了避免创建一个与产品类层次平行的工厂类层次时；或者</li>
  <li>当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/prototype_zpsb4a5c2aa.gif" alt="" /></p>

<h2 id="section-1">结构型模式</h2>

<h3 id="adapter">适配器模式（Adapter）</h3>

<p>将一个类的接口转换成另外一个客户希望的接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>你想使用一个已经存在的类，而它的接口不符合你的需求。</li>
  <li>你想创建一个可以复用的类，该类可以与其他不相关的类或不可预见的类（即那些接口可能不一定兼容的类）协同工作。</li>
  <li>（仅适用于对象Adapter）你想使用一些已经存在的子类，但是不可能对每一个都进行子类化以匹配它们的接口。对象适配器可以适配它的父类接口。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/adapter_zps1800d0cc.gif" alt="" /></p>

<h3 id="decorator">装饰模式（Decorator）</h3>

<p>动态地给一个对象添加一些额外的职责。就增加功能来说，Decorator模式相比生成子类更为灵活。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。</li>
  <li>处理那些可以撤消的职责。</li>
  <li>当不能采用生成子类的方法进行扩充时。一种情况是，可能有大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长。另一种情况可能是因为类定义被隐藏，或类定义不能用于生成子类。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/Decorator_zpse3f223d4.gif" alt="" /></p>

<h2 id="section-2">行为型模式</h2>

<h3 id="observer">观察者模式（Observer）</h3>

<p>定义对象间的一种一对多的依赖关系,当一个对象的状态发生改变时, 所有依赖于它的对象都得到通知并被自动更新。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>当一个抽象模型有两个方面, 其中一个方面依赖于另一方面。将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。</li>
  <li>当对一个对象的改变需要同时改变其它对象, 而不知道具体有多少对象有待改变。</li>
  <li>当一个对象必须通知其它对象，而它又不能假定其它对象是谁。换言之，你不希望这些对象是紧密耦合的。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/observer_zps892df2a6.gif" alt="" /></p>

<h3 id="state">状态模式（State）</h3>

<p>允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>一个对象的行为取决于它的状态, 并且它必须在运行时刻根据状态改变它的行为。</li>
  <li>一个操作中含有庞大的多分支的条件语句，且这些分支依赖于该对象的状态。这个状态通常用一个或多个枚举常量表示。通常, 有多个操作包含这一相同的条件结构。State模式将每一个条件分支放入一个独立的类中。这使得你可以根据对象自身的情况将对象的状态作为一个对象，这一对象可以不依赖于其他对象而独立变化。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/state_zps349fba82.gif" alt="" /></p>

<h3 id="strategy">策略模式（Strategy）</h3>

<p>定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。</p>

<p><strong>适用性</strong></p>

<ul>
  <li>许多相关的类仅仅是行为有异。“策略”提供了一种用多个行为中的一个行为来配置一个类的方法。</li>
  <li>需要使用一个算法的不同变体。例如，你可能会定义一些反映不同的空间/时间权衡的算法。当这些变体实现为一个算法的类层次时，可以使用策略模式。</li>
  <li>算法使用客户不应该知道的数据。可使用策略模式以避免暴露复杂的、与算法相关的数据结构。</li>
  <li>一个类定义了多种行为, 并且这些行为在这个类的操作中以多个条件语句的形式出现。将相关的条件分支移入它们各自的Strategy类中以代替这些条件语句。</li>
</ul>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/strategy_zpsc3f3c44b.gif" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux Memory Management]]></title>
    <link href="http://billowkiller.github.io/blog/2014/08/14/linux-memory-management/"/>
    <updated>2014-08-14T02:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/08/14/linux-memory-management</id>
    <content type="html"><![CDATA[<p>reprint from <a href="http://www.kerneltravel.net/journal/v/mem.htm">http://www.kerneltravel.net/journal/v/mem.htm</a></p>

<hr />

<p>看到一篇介绍linux内存管理的好文，忍不住转载，做个记录。内容作了部分删改。</p>

<h2 id="section">摘要</h2>

<p>本章首先以应用程序开发者的角度审视Linux的进程内存管理，在此基础上逐步深入到内核中讨论系统物理内存管理和内核内存的使用方法。力求从外到内、水到渠成地引导网友分析Linux的内存管理与使用。在本章最后，我们给出一个内存映射的实例，帮助网友们理解内核内存管理与用户内存管理之间的关系，希望大家最终能驾驭Linux内存管理。</p>

<!--more-->

<h2 id="section-1">前言</h2>

<p>内存管理一向是所有操作系统书籍不惜笔墨重点讨论的内容，无论市面上或是网上都充斥着大量涉及内存管理的教材和资料。因此，我们这里所要写的Linux内存管理采取避重就轻的策略，从理论层面就不去班门弄斧，贻笑大方了。我们最想做的和可能做到的是从开发者的角度谈谈对内存管理的理解，最终目的是把我们在内核开发中使用内存的经验和对Linux内存管理的认识与大家共享。</p>

<p>当然，这其中我们也会涉及到一些诸如段页等内存管理的基本理论，但我们的目的不是为了强调理论，而是为了指导理解开发中的实践，所以仅仅点到为止，不做深究。</p>

<p>遵循“理论来源于实践”的“教条”，我们先不必一下子就钻入内核里去看系统内 存到底是如何管理，那样往往会让你陷入似懂非懂的窘境（我当年就犯了这个错误！）。所以最好的方式是先从外部（用户编程范畴）来观察进程如何使用内存，等 到大家对内存的使用有了较直观的认识后，再深入到内核中去学习内存如何被管理等理论知识。最后再通过一个实例编程将所讲内容融会贯通。</p>

<h2 id="section-2">进程与内存</h2>

<h3 id="section-3">进程如何使用内存？</h3>

<p>毫无疑问，所有进程（执行的程序）都必须占用一定数量的内存，它或是用来存放从磁盘载入的程序代码，或是存放取自用户输入的数据等等。不过进程对这些内存的管理方式因内存用途不一而不尽相同，有些内存是事先静态分配和统一回收的，而有些却是按需要动态分配和回收的。</p>

<p>对任何一个普通进程来讲，它都会涉及到5种不同的数据段。稍有编程知识的朋友都能想到这几个数据段中包含有“程序代码段”、“程序数据段”、“程序堆栈段”等。不错，这几种数据段都在其中，但除了以上几种数据段之外，进程还另外包含两种数据段。下面我们来简单归纳一下进程对应的内存空间中所包含的5种不同的数据区。</p>

<p><strong>代码段：</strong>代码段是用来存放可执行文件的操作指令，也就是说是它是可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以只准许读取操作，而不允许写入（修改）操作——它是不可写的。</p>

<p><strong>数据段：</strong>数据段用来存放可执行文件中已初始化全局变量，换句话说就是存放程序静态分配的变量和全局变量。</p>

<p><strong>BSS段：</strong>BSS段包含了程序中未初始化的全局变量，在内存中 bss段全部置零。</p>

<p><strong>堆（heap）：</strong>堆是用于存放进程运行中被动态分配的内存段，它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）</p>

<p><strong>栈：</strong>栈是用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。</p>

<p><img src="http://dl.iteye.com/upload/picture/pic/80993/96d2c983-bec7-34d6-805b-2a8e40506848.png" alt="" /></p>

<p>从用户向内核看，所使用的内存表象形式会依次经历“逻辑地址”——“线性地址”——“物理地址”几种形式（关于几种地址的解释在前面已经讲述了）。逻辑地址经段机制转化成线性地址；线性地址又经过页机制转化为物理地址。（但是我们要知道Linux系统虽然保留了段机制，但是将所有程序的段地址都定死为0-4G，所以虽然逻辑地址和线性地址是两种不同的地址空间，但在Linux中逻辑地址就等于线性地址，它们的值是一样的）。沿着这条线索，我们所研究的主要问题也就集中在下面几个问题。</p>

<ol>
  <li>进程空间地址如何管理？</li>
  <li>进程地址如何映射到物理内存？</li>
  <li>物理内存如何被管理？</li>
</ol>

<p>以及由上述问题引发的一些子问题。如系统虚拟地址分布；内存分配接口；连续内存分配与非连续内存分配等。</p>

<h2 id="section-4">进程内存空间</h2>

<p>Linux操作系统采用虚拟内存管理技术，使得每个进程都有各自互不干涉的进程地址空间。该空间是块大小为4G的线性虚拟空间，用户所看到和接触到的都是该虚拟地址，无法看到实际的物理内存地址。利用这种虚拟地址不但能起到保护操作系统的效果（用户不能直接访问物理内存），而且更重要的是，用户程序可使用比实际物理内存更大的地址空间（具体的原因请看硬件基础部分）。</p>

<p>在讨论进程空间细节前，这里先要澄清下面几个问题：</p>

<ol>
  <li>4G的进程地址空间被人为的分为两个部分——用户空间与内核空间。用户空间从0到3G（0xC0000000），内核空间占据3G到4G。用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间虚拟地址。只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。</li>
  <li>用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程改变，是固定的。内核空间地址有自己对应的页表（init_mm.pgd），用户进程各自有不同的页表。</li>
  <li>每个进程的用户空间都是完全独立、互不相干的。不信的话，你可以把上面的程序同时运行10次（当然为了同时运行，让它们在返回前一同睡眠100秒吧），你会看到10个进程占用的线性地址一模一样。</li>
</ol>

<h3 id="section-5">进程内存管理</h3>

<p>进程内存管理的对象是进程线性地址空间上的内存镜像，这些内存镜像其实就是进程使用的虚拟内存区域（memory region）。进程虚拟空间是个32或64位的“平坦”（独立的连续区间）地址空间（空间的具体大小取决于体系结构）。要统一管理这么大的平坦空间可绝非易事，为了方便管理，虚拟空间被划分为许多大小可变的(但必须是4096的倍数)内存区域，这些区域在进程线性地址中像停车位一样有序排列。这些区域的划分原则是“将访问属性一致的地址空间存放在一起”，所谓访问属性在这里无非指的是“可读、可写、可执行等”。</p>

<p>如果你要查看某个进程占用的内存区域，可以使用命令<code>cat /proc/&lt;pid&gt;/maps</code>获得。</p>

<p><strong>注意，你一定会发现进程空间只包含三个内存区域，似乎没有上面所提到的堆、bss等，其实并非如此，程序内存段和进程地址空间中的内存区域是种模糊对应，也就是说，堆、bss、数据段（初始化过的）都在进程空间中由数据段内存区域表示。</strong></p>

<p>在Linux内核中对应进程内存区域的数据结构是: vm_area_struct, 内核将每个内存区域作为一个单独的内存对象管理，相应的操作也都一致。采用面向对象方法使VMA结构体可以代表多种类型的内存区域－－比如内存映射文件或进程的用户空间栈等，对这些区域的操作也都不尽相同。</p>

<p>vm_area_strcut结构比较复杂，关于它的详细结构请参阅相关资料。我们这里只对它的组织方法做一点补充说明。vm_area_struct是描述进程地址空间的基本管理单元，对于一个进程来说往往需要多个内存区域来描述它的虚拟空间，如何关联这些不同的内存区域呢？大家可能都会想到使用链表，的确vm_area_struct结构确实是以链表形式链接，不过为了方便查找，内核又以红黑树（以前的内核使用平衡树）的形式组织内存区域，以便降低搜索耗时。并存的两种组织形式，并非冗 余：链表用于需要遍历全部节点的时候用，而红黑树适用于在地址空间中定位特定内存区域的时候。内核为了内存区域上的各种不同操作都能获得高性能，所以同时 使用了这两种数据结构。</p>

<p>下图反映了进程地址空间的管理模型：</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/tu1_zps5b8ee942.jpg" alt="" /></p>

<p><strong>进程的地址空间对应的描述结构是“内存描述符结构”,它表示进程的全部地址空间，——包含了和进程地址空间有关的全部信息，其中当然包含进程的内存区域。</strong></p>

<h3 id="section-6">进程内存的分配与回收</h3>

<p>创建进程<code>fork()</code>、程序载入<code>execve()</code>、映射文件<code>mmap()</code>、动态内存分配<code>malloc()/brk()</code>等进程相关操作都需要分配内存给进程。不过这时进程申请和获得的还不是实际内存，而是虚拟内存，准确的说是“内存区域”。进程对内存区域的分配最终都会归结到<code>do_mmap（）</code>函数上来（<code>brk</code>调用被单独以系统调用实现，不用<code>do_mmap()</code>），</p>

<p>内核使用<code>do_mmap()</code>函数创建一个新的线性地址区间。但是说该函数创建了一个新VMA并不非常准确，因为如果创建的地址区间和一个已经存在的地址区间相邻，并且它们具有相同的访问权限的话，那么两个区间将合并为一个。如果不能合并，那么就确实需要创建一个新的VMA了。但无论哪种情况， <code>do_mmap()</code>函数都会将一个地址区间加入到进程的地址空间中－－无论是扩展已存在的内存区域还是创建一个新的区域。</p>

<p>同样，释放一个内存区域应使用函数<code>do_ummap()</code>，它会销毁对应的内存区域。</p>

<h3 id="section-7">如何由虚变实！</h3>

<p>从上面已经看到进程所能直接操作的地址都为虚拟地址。当进程需要内存时，从内核获得的仅仅是虚拟的内存区域，而不是实际的物理地址，进程并没有获得物理内存（物理页面——页的概念请大家参考硬件基础一章），获得的仅仅是对一个新的线性地址区间的使用权。实际的物理内存只有当进程真的去访问新获取的虚拟地址时，才会由“请求页机制”产生“缺页”异常，从而进入分配实际页面的例程。</p>

<p>该异常是虚拟内存机制赖以存在的基本保证——它会告诉内核去真正为进程分配物理页，并建立对应的页表，这之后虚拟地址才实实在在地映射到了系统的物理内存上。（当然，如果页被换出到磁盘，也会产生缺页异常，不过这时不用再建立页表了）</p>

<p>这种请求页机制把页面的分配推迟到不能再推迟为止，并不急于把所有的事情都一次做完（这种思想有点像设计模式中的代理模式（proxy））。之所以能这么做是利用了内存访问的“局部性原理”，请求页带来的好处是节约了空闲内存，提高了系统的吞吐率。要想更清楚地了解请求页机制，可以看看《深入理解linux内核》一书。</p>

<p>这里我们需要说明在内存区域结构上的nopage操作。当访问的进程虚拟内存并未真正分配页面时，该操作便被调用来分配实际的物理页，并为该页建立页表项。在最后的例子中我们会演示如何使用该方法。</p>

<h2 id="section-8">系统物理内存管理</h2>

<p>虽然应用程序操作的对象是映射到物理内存之上的虚拟内存，但是处理器直接操作的却是物理内存。所以当应用程序访问一个虚拟地址时，首先必须将虚拟地址转化成 物理地址，然后处理器才能解析地址访问请求。地址的转换工作需要通过查询页表才能完成，概括地讲，地址转换需要将虚拟地址分段，使每段虚地址都作为一个索引指向页表，而页表项则指向下一级别的页表或者指向最终的物理页面。</p>

<p>每个进程都有自己的页表。进程描述符的pgd域指向的就是进程的页全局目录。下面我们借用《linux设备驱动程序》中的一幅图大致看看进程地址空间到物理页之间的转换关系。</p>

<p><img src="http://www.kerneltravel.net/journal/v/mem.files/image003.jpg" alt="" /></p>

<p>上面的过程说起来简单，做起来难呀。因为在虚拟地址映射到页之前必须先分配物理页——也就是说必须先从内核中获取空闲页，并建立页表。下面我们介绍一下内核管理物理内存的机制。</p>

<h3 id="section-9">物理内存管理（页管理）</h3>

<p>Linux内核管理物理内存是通过分页机制实现的，它将整个内存划分成无数个4k（在i386体系结构中）大小的页，从而分配和回收内存的基本单位便是内存页了。利用分页管理有助于灵活分配内存地址，因为分配时不必要求必须有大块的连续内存,系统可以东一页、西一页的凑出所需要的内存供进程使用。虽然如此，但是实际上系统使用内存时还是倾向于分配连续的内存块，因为分配连续内存时，页表不需要更改，因此能降低TLB的刷新率（频繁刷新会在很大程度上降低访问速度）。</p>

<p>鉴于上述需求，内核分配物理页面时为了尽量减少不连续情况，采用了“伙伴”关系来管理空闲页面。伙伴关系分配算法大家应该不陌生——几乎所有操作系统方面的书都会提到,我们不去详细说它了，如果不明白可以参看有关资料。这里只需要大家明白Linux中空闲页面的组织和管理利用了伙伴关系，因此空闲页面分配时也需要遵循伙伴关系，最小单位只能是2的幂倍页面大小。内核中分配空闲页面的基本函数是<code>get_free_page/get_free_pages</code>，它们或是分配单页或是分配指定的页面（2、4、8…512页）。</p>

<p><strong>注意：</strong> <code>get_free_page</code>是在内核中分配内存，不同于<code>malloc</code>在用户空间中分配，<code>malloc</code>利用堆动态分配，实际上是调用<code>brk()</code>系统调用，该调用的作用是扩大或缩小进程堆空间（它会修改进程的<code>brk</code>域）。如果现有的内存区域不够容纳堆空间，则会以页面大小的倍数为单位，扩张或收缩对应的内存区域，但<code>brk</code>值并非以页面大小为倍数修改，而是按实际请求修改。因此<code>malloc</code>在用户空间分配内存可以以字节为单位分配,但内核在内部仍然会是以页为单位分配的。</p>

<p>另外,需要提及的是，物理页在系统中由页结构<code>struct page</code>描述，系统中所有的页面都存储在数组<code>mem_map[]</code>中，可以通过该数组找到系统中的每一页（空闲或非空闲）。而其中的空闲页面则可由上述提到的以伙伴关系组织的空闲页链表（<code>free_area[MAX_ORDER]</code>）来索引。</p>

<p><img src="http://p.blog.csdn.net/images/p_blog_csdn_net/kanghua/systemcall.bmp" alt="" /></p>

<h3 id="section-10">内核内存使用</h3>

<p><strong>Slab</strong></p>

<p>所 谓尺有所长，寸有所短。以页为最小单位分配内存对于内核管理系统中的物理内存来说的确比较方便，但内核自身最常使用的内存却往往是很小（远远小于一页）的 内存块——比如存放文件描述符、进程描述符、虚拟内存区域描述符等行为所需的内存都不足一页。这些用来存放描述符的内存相比页面而言，就好比是面包屑与面 包。一个整页中可以聚集多个这些小块内存；而且这些小块内存块也和面包屑一样频繁地生成/销毁。</p>

<p>为了满足内核对这种小内存块的需要，Linux系统采用了一种被称为slab分配器的技术。Slab分配器的实现相当复杂，但原理不难，其核心思想就是“存储池”的运用。内存片段（小块内存）被看作对象，当被使用完后，并不直接释放而是被缓存到“存储池”里，留做下次使用，这无疑避免了频繁创建与销毁对象所带来的额外负载。</p>

<p>Slab技术不但避免了内存内部分片（下文将解释）带来的不便（引入Slab分配器的主要目的是为了减少对伙伴系统分配算法的调用次数——频繁分配和回收必然会导致内存碎片——难以找到大块连续的可用内存），而且可以很好地利用硬件缓存提高访问速度。</p>

<p><strong>Slab并非是脱离伙伴关系而独立存在的一种内存分配方式，slab仍然是建立在页面基础之上，换句话说，Slab将页面（来自于伙伴关系管理的空闲页面链表）撕碎成众多小内存块以供分配，slab中的对象分配和销毁使用<code>kmem_cache_alloc</code>与<code>kmem_cache_free</code>。</strong></p>

<p><strong>Kmalloc</strong></p>

<p>Slab分配器不仅仅只用来存放内核专用的结构体，它还被用来处理内核对小块内存的请求。当然鉴于Slab分配器的特点，一般来说内核程序中对小于一页的小块内存的请求才通过Slab分配器提供的接口<code>Kmalloc</code>来完成（虽然它可分配32 到131072字节的内存）。从内核内存分配的角度来讲，kmalloc可被看成是<code>get_free_page（s）</code>的一个有效补充，内存分配粒度更灵活了。</p>

<p>有兴趣的话，可以到<code>/proc/slabinfo</code>中找到内核执行现场使用的各种slab信息统计，其中你会看到系统中所有slab的使用信息。从信息中可以看到系统中除了专用结构体使用的slab外，还存在大量为<code>Kmalloc</code>而准备的Slab（其中有些为dma准备的）。</p>

<p><strong>内核非连续内存分配（Vmalloc）</strong></p>

<p>伙伴关系也好、slab技术也好，从内存管理理论角度而言目的基本是一致的，它们都是为了防止“分片”，不过分片又分为外部分片和内部分片之说，所谓内部分片是说系统为了满足一小 段内存区（连续）的需要，不得不分配了一大区域连续内存给它，从而造成了空间浪费；外部分片是指系统虽有足够的内存，但却是分散的碎片，无法满足对大块 “连续内存”的需求。无论何种分片都是系统有效利用内存的障碍。slab分 配器使得一个页面内包含的众多小块内存可独立被分配使用，避免了内部分片，节约了空闲内存。伙伴关系把内存块按大小分组管理，一定程度上减轻了外部分片的 危害，因为页框分配不在盲目，而是按照大小依次有序进行，不过伙伴关系只是减轻了外部分片，但并未彻底消除。你自己比划一下多次分配页面后，空闲内存的剩余情况吧。</p>

<p>所以避免外部分片的最终思路还是落到了如何利用不连续的内存块组合成“看起来很大的内存块”——这里的情况很类似于用户空间分配虚拟内存，内存逻辑上连续，其实映射到并不一定连续的物理内存上。Linux内核借用了这个技术，允许内核程序在内核地址空间中分配虚拟地址，同样也利用页表（内核页表）将虚拟地址映射到分散的内存页上。以此完美地解决了内核内存使用中的外部分片问题。内核提供<code>vmalloc</code>函数分配内核虚拟内存，该函数不同<code>于kmalloc</code>，它可以分配较<code>Kmalloc</code>大得多的内存空间（可远大于128K，但必须是页大小的倍数），但相比<code>Kmalloc</code>来说,<code>Vmalloc</code>需要对内核虚拟地址进行重映射，必须更新内核页表，因此分配效率上要低一些（用空间换时间）。</p>

<p>与用户进程相似,内核也有一个名为<code>init_mm</code>的<code>mm_strcut</code>结构来描述内核地址空间，其中页表项<code>pdg=swapper_pg_dir</code>包含了系统内核空间（3G-4G）的映射关系。因此<code>vmalloc</code>分配内核虚拟地址必须更新内核页表，而<code>kmalloc</code>或<code>get_free_page</code>由于分配的连续内存，所以不需要更新内核页表。</p>

<p><img src="http://p.blog.csdn.net/images/p_blog_csdn_net/kanghua/systemcall2.bmp" alt="" /></p>

<p><code>vmalloc</code>分配的内核虚拟内存与<code>kmalloc/get_free_page</code>分配的内核虚拟内存位于不同的区间，不会重叠。因为内核虚拟空间被分区管理，各司其职。进程空间地址分布从０到３G(其实是到<code>PAGE_OFFSET</code>, 在0x86中它等于0xC0000000)，从3G到<code>vmalloc_start</code>这段地址是物理内存映射区域（该区域中包含了内核镜像、物理页面表<code>mem_map</code>等等）比如我使用的系统内存是64M(可以用free看到)，那么(3G——3G+64M)这片内存就应该映射到物理内存，而<code>vmalloc_start</code>位置应在3G+64M附近（说”附近”因为是在物理内存映射区与<code>vmalloc_start</code>期间还会存在一个8M大小的gap来防止跃界）,<code>vmalloc_end</code>的位置接近4G(说”接近”是因为最后位置系统会保留一片128k大小的区域用于专用页面映射，还有可能会有高端内存映射区，这些都是细节，这里我们不做纠缠)。</p>

<p><img src="http://i1113.photobucket.com/albums/k512/billowkiller/BILLOWKILLER-PC/tu2_zpsfa7ec115.jpg" alt="" /></p>

<p>由<code>get_free_page</code>或<code>Kmalloc</code>函数所分配的连续内存都陷于物理映射区域，所以它们返回的内核虚拟地址和实际物理地址仅仅是相差一个偏移量（<code>PAGE_OFFSET</code>），你可以很方便的将其转化为物理内存地址，同时内核也提供了<code>virt_to_phys（）</code>函数将内核虚拟空间中的物理映射区地址转化为物理地址。要知道，物理内存映射区中的地址与内核页表是有序对应的，系统中的每个物理页面都可以找到它对应的内核虚拟地址（在物理内存映射区中的）。</p>

<p>而<code>vmalloc</code>分配的地址则限于<code>vmalloc_start</code>与<code>vmalloc_end</code>之间。每一块k分配的内核虚拟内存都对应一个<code>vm_struct</code>结构体（可别和<code>vm_area_struct</code>搞混，那可是进程虚拟内存区域的结构），不同的内核虚拟地址被4k大小的空闲区间隔，以防止越界——见下图）。与进程虚拟地址的特性一样，这些虚拟地址与物理内存没有简单的位移关系，必须通过内核页表才可转换为物理地址或物理页。它们有可能尚未被映射，在发生缺页时才真正分配物理页面。</p>

<p><img src="http://www.kerneltravel.net/journal/v/mem.files/image013.jpg" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[QuickSort and Derivatives]]></title>
    <link href="http://billowkiller.github.io/blog/2014/08/09/quicksort/"/>
    <updated>2014-08-09T03:18:00+08:00</updated>
    <id>http://billowkiller.github.io/blog/2014/08/09/quicksort</id>
    <content type="html"><![CDATA[<p>记录下快排相关的一些东西。包括普通快排，迭代快排和单链表快排。</p>

<hr />

<!--more-->

<h2 id="section">普通快排</h2>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class="c++"><span class="line"><span class="cm">/* A typical recursive implementation of quick sort */</span>
</span><span class="line">
</span><span class="line"><span class="cm">/* This function takes last element as pivot, places the pivot element at its correct position in sorted array, and places all smaller (smaller than pivot) to left of pivot and all greater elements to right of pivot */</span>
</span><span class="line"><span class="kt">int</span> <span class="n">partition</span> <span class="p">(</span><span class="kt">int</span> <span class="n">arr</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">l</span><span class="p">,</span> <span class="kt">int</span> <span class="n">h</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">    <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">h</span><span class="p">];</span>
</span><span class="line">    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">l</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">h</span><span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">            <span class="n">i</span><span class="o">++</span><span class="p">;</span>
</span><span class="line">            <span class="n">swap</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line">    <span class="n">swap</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">h</span><span class="p">]);</span>
</span><span class="line">    <span class="k">return</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/* A[] --&gt; Array to be sorted, l  --&gt; Starting index, h  --&gt; Ending index */</span>
</span><span class="line"><span class="kt">void</span> <span class="n">quickSort</span><span class="p">(</span><span class="kt">int</span> <span class="n">A</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">l</span><span class="p">,</span> <span class="kt">int</span> <span class="n">h</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">    <span class="k">if</span> <span class="p">(</span><span class="n">l</span> <span class="o">&lt;</span> <span class="n">h</span><span class="p">){</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">p</span> <span class="o">=</span> <span class="n">partition</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">h</span><span class="p">);</span>
</span><span class="line">        <span class="n">quickSort</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">        <span class="n">quickSort</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">);</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这种实现方式有很多可以值得改进的地方：</p>

<ol>
  <li>上面的实现使用最后一个元素作为pivot，这对于已经排好序的数组来说是个灾难。可以随机选取一个元素或者直接用中位数来替代。</li>
  <li>为了减少递归层次，可以先递归数组中个数少的一半。</li>
  <li>对于小数组来说，插入排序可能更快。可以综合下插入和快排。</li>
  <li>使用了递归和函数调用栈来存储中间值，并且还需要存储其他的信息。此外还包括存储函数调用的活动记录，恢复上层函数执行的费用。</li>
</ol>

<p>可以使用迭代来替代递归。</p>

<h3 id="section-1">迭代快排</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
</pre></td><td class="code"><pre><code class="c++"><span class="line"><span class="cm">/* This function is same in both iterative and recursive*/</span>
</span><span class="line"><span class="kt">int</span> <span class="n">partition</span> <span class="p">(</span><span class="kt">int</span> <span class="n">arr</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">l</span><span class="p">,</span> <span class="kt">int</span> <span class="n">h</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">    <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="err">，</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">l</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">h</span><span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">){</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">){</span>
</span><span class="line">            <span class="n">i</span><span class="o">++</span><span class="p">;</span>
</span><span class="line">            <span class="n">swap</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line">    <span class="n">swap</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">h</span><span class="p">]);</span>
</span><span class="line">    <span class="k">return</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="kt">void</span> <span class="n">quickSortIterative</span> <span class="p">(</span><span class="kt">int</span> <span class="n">arr</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">l</span><span class="p">,</span> <span class="kt">int</span> <span class="n">h</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">	<span class="c1">// initialize top of stack</span>
</span><span class="line">    <span class="kt">int</span> <span class="n">stack</span><span class="p">[</span> <span class="n">h</span> <span class="o">-</span> <span class="n">l</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">];</span> <span class="kt">int</span> <span class="n">top</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">    <span class="c1">// push initial values of l and h to stack</span>
</span><span class="line">    <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">l</span><span class="p">;</span>  <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">    <span class="c1">// Keep popping from stack while is not empty</span>
</span><span class="line">    <span class="k">while</span> <span class="p">(</span> <span class="n">top</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="p">){</span>
</span><span class="line">        <span class="c1">// Pop h and l</span>
</span><span class="line">        <span class="n">h</span> <span class="o">=</span> <span class="n">stack</span><span class="p">[</span> <span class="n">top</span><span class="o">--</span> <span class="p">];</span>  <span class="n">l</span> <span class="o">=</span> <span class="n">stack</span><span class="p">[</span> <span class="n">top</span><span class="o">--</span> <span class="p">];</span>
</span><span class="line">
</span><span class="line">        <span class="kt">int</span> <span class="n">p</span> <span class="o">=</span> <span class="n">partition</span><span class="p">(</span> <span class="n">arr</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">h</span> <span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span> <span class="n">p</span><span class="o">-</span><span class="mi">1</span> <span class="o">&gt;</span> <span class="n">l</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">            <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">l</span><span class="p">;</span> <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span> <span class="n">p</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">h</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">            <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stack</span><span class="p">[</span> <span class="o">++</span><span class="n">top</span> <span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">    <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="section-2">文艺快排</h2>

<p>思路和数据的快速排序一样，都需要找到一个pivot元素、或者节点。然后将数组或者单向链表划分为两个部分，然后递归分别快排。</p>

<p>针对数组进行快排的时候，交换交换不同位置的数值，在分而治之完成之后，数据就是排序好的。那么单向链表是什么样的情况呢？除了交换节点值之外，是否有其他更好的方法呢？可以修改指针，不进行数值交换。这可以获取更高的效率。</p>

<p>在修改指针的过程中，会产生新的头指针以及尾指针，要记录下来。在partition之后，要将小于pivot的的部分、pivot、以及大于pivot的部分重新串起来成为一个singly linked list。</p>

<p>在partition时，我们用最后的节点作为pivot。当我们扫描链表时，如果节点值大于pivot，将节点移到尾部之后；如果节点小于，保持不变。</p>

<p>在递归排序时，我们先调用partition将pivot放到正确的为止并返回pivot，然后，递归左边，递归右边，最后在合成一个单链表。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
</pre></td><td class="code"><pre><code class="c++"><span class="line"><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">partition</span><span class="p">(</span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">end</span><span class="p">,</span>
</span><span class="line">                      <span class="k">struct</span> <span class="n">node</span> <span class="o">**</span><span class="n">newHead</span><span class="p">,</span> <span class="k">struct</span> <span class="n">node</span> <span class="o">**</span><span class="n">newEnd</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">   <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">pivot</span> <span class="o">=</span> <span class="n">end</span><span class="p">;</span>
</span><span class="line">   <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">prev</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">,</span> <span class="o">*</span><span class="n">cur</span> <span class="o">=</span> <span class="n">head</span><span class="p">,</span> <span class="o">*</span><span class="n">tail</span> <span class="o">=</span> <span class="n">pivot</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">   <span class="k">while</span><span class="p">(</span><span class="n">cur</span> <span class="o">!=</span> <span class="n">pivot</span><span class="p">)</span>
</span><span class="line">   <span class="p">{</span>
</span><span class="line">       <span class="k">if</span><span class="p">(</span><span class="n">cur</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">&lt;</span> <span class="n">pivot</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">)</span>
</span><span class="line">       <span class="p">{</span>
</span><span class="line">          <span class="k">if</span><span class="p">((</span><span class="o">*</span><span class="n">newHead</span><span class="p">)</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">               <span class="p">(</span><span class="o">*</span><span class="n">newHead</span><span class="p">)</span> <span class="o">=</span> <span class="n">cur</span><span class="p">;</span>
</span><span class="line">           <span class="n">prev</span> <span class="o">=</span> <span class="n">cur</span><span class="p">;</span>
</span><span class="line">           <span class="n">cur</span> <span class="o">=</span> <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">       <span class="p">}</span>
</span><span class="line">       <span class="k">else</span>
</span><span class="line">       <span class="p">{</span>
</span><span class="line">           <span class="k">if</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
</span><span class="line">               <span class="n">prev</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">           <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">tmp</span> <span class="o">=</span> <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">           <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span><span class="line">           <span class="n">tail</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">cur</span><span class="p">;</span>
</span><span class="line">           <span class="n">tail</span> <span class="o">=</span> <span class="n">cur</span><span class="p">;</span>
</span><span class="line">           <span class="n">cur</span> <span class="o">=</span> <span class="n">tmp</span><span class="p">;</span>
</span><span class="line">       <span class="p">}</span>
</span><span class="line">   <span class="p">}</span>
</span><span class="line">   <span class="k">if</span><span class="p">((</span><span class="o">*</span><span class="n">newHead</span><span class="p">)</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">(</span><span class="o">*</span><span class="n">newHead</span><span class="p">)</span> <span class="o">=</span> <span class="n">pivot</span><span class="p">;</span>
</span><span class="line">   <span class="p">(</span><span class="o">*</span><span class="n">newEnd</span><span class="p">)</span> <span class="o">=</span> <span class="n">tail</span><span class="p">;</span>
</span><span class="line">   <span class="k">return</span> <span class="n">pivot</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">quickSortRecur</span><span class="p">(</span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span><span class="p">,</span> <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">end</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">   <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">head</span> <span class="o">||</span> <span class="n">head</span> <span class="o">==</span> <span class="n">end</span><span class="p">)</span>
</span><span class="line">       <span class="k">return</span> <span class="n">head</span><span class="p">;</span>
</span><span class="line">   <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">newHead</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">,</span> <span class="o">*</span><span class="n">newEnd</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span><span class="line">   <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">pivot</span> <span class="o">=</span> <span class="n">partition</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">newHead</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">newEnd</span><span class="p">);</span>
</span><span class="line">   <span class="k">if</span><span class="p">(</span><span class="n">newHead</span> <span class="o">!=</span> <span class="n">pivot</span><span class="p">)</span>
</span><span class="line">   <span class="p">{</span>
</span><span class="line">      <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">tmp</span> <span class="o">=</span> <span class="n">newHead</span><span class="p">;</span>
</span><span class="line">       <span class="k">while</span><span class="p">(</span><span class="n">tmp</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">!=</span> <span class="n">pivot</span><span class="p">)</span>
</span><span class="line">           <span class="n">tmp</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">       <span class="n">tmp</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span><span class="line">       <span class="n">newHead</span> <span class="o">=</span> <span class="n">quickSortRecur</span><span class="p">(</span><span class="n">newHead</span><span class="p">,</span> <span class="n">tmp</span><span class="p">);</span>
</span><span class="line">       <span class="n">tmp</span> <span class="o">=</span> <span class="n">getTail</span><span class="p">(</span><span class="n">newHead</span><span class="p">);</span>
</span><span class="line">       <span class="n">tmp</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span>  <span class="n">pivot</span><span class="p">;</span>
</span><span class="line">   <span class="p">}</span>
</span><span class="line">   <span class="n">pivot</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">quickSortRecur</span><span class="p">(</span><span class="n">pivot</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">,</span> <span class="n">newEnd</span><span class="p">);</span>
</span><span class="line">   <span class="n">returnn</span> <span class="n">ewHead</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="kt">void</span> <span class="n">quickSort</span><span class="p">(</span><span class="k">struct</span> <span class="n">node</span> <span class="o">**</span><span class="n">headRef</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">   <span class="p">(</span><span class="o">*</span><span class="n">headRef</span><span class="p">)</span> <span class="o">=</span> <span class="n">quickSortRecur</span><span class="p">(</span><span class="o">*</span><span class="n">headRef</span><span class="p">,</span> <span class="n">getTail</span><span class="p">(</span><span class="o">*</span><span class="n">headRef</span><span class="p">));</span>
</span><span class="line">   <span class="k">return</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
</feed>
