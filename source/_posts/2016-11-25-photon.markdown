---
layout: post
title: "Photon: fault-tolerant and scalable joining of continuous data streams"
date: 2016-11-19 11:23
comments: true
category: Paper Weekend
tags: [paper，streaming, google]
---

> R Ananthanarayanan et al. SIGMOD 2013

## Intro and Issues

Photon 是 Google 的数据流合并的系统，目的是 “perform continuous stream joining in real-time”，例如 query log 和 click log 合并。为什么需要对数据流合并呢？

先看下用户使用的场景：用户先 query 一个搜索词，这时候的日志可以直接通过 Google 的网关服务器直接落盘，这里的日志称为 query log，也就是展示广告，表示发给用户的广告信息；之后的一段时间，用户浏览网页，可能会点击到某一条广告，这时候再将用户的 click log 发到某个 log datacenter。

这两个日志所附带的内容区别很大，query log 可以附带很多的信息，类似于展示位、广告主出价等，而 click log 的内容是受限的：虽然可以把需要的信息附带到广告的 url 上，但是传输量变大增加延迟降低用户体验，另外 url 的长度也是受限的。因此很有必要进行日志的合并以用于后续的计费、报表、模型训练等等。

那么处理这个问题的难点在哪里？

<!--more-->

* Exactly-once semantics: 任意时刻 at-most-once，实时的 near-exact，最终 exactly-once。
* Automatic datacenter-level fault-tolerance
* High scalability
* Low latency
* unordered streams: 特别的，由于网络或者其他原因，query log 可能会落后于 click log。

## Big Ideas

从对系统的要求来看，有两个重要的概念：

* `Persistent State Consistency`
* `Exactly Once Eventually`

### 1. Persistent State Consistency

高可用，容错的系统的最简单方法就是冗余，Photon 也如此。导致的问题是在不同的数据中心可能会处理同一份数据。这时候需要对 Worker 进行协调，保证它们不对同一份数据处理。方案是对处理好的 click_id 存储下来，各个 Worker 处理的时候查询下，并在输出 joined log 之前先存储 click_id 以保证 at most once 语义。

Photon 使用基于 `Multi-Paxos` 的容错、强一致的 `IdRegisty`。Paxos 能够保证在大多数副本之间同步状态，实现一致性。如下图是一个 IdRegistry 的架构：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/63413408.jpg" width="450px"/>

不同的数据中心来回的传输时间可能高达100ms，导致 IdRegistry 的 TPS 最高只有 10，对于每秒需要处理上万日志的 photon 来说是不可以接受的。那么如何增加吞吐呢？有两个措施：`server-side batching`，`sharding`。这是两个经典的扩大吞吐量的方法，`批量`和`分治`。

**批量：**

> Server-side batching combine multiple event-level commits into one bigger commit. The registry thread dequeues multiple requests and batches them into a single PaxosDB transaction.

在上图中，Registry thread 会取多个队列中的 request，把request中的 event_id 打包成一个 paxos 请求。这里会提供一个简单的冲突检测，针对打包的 event_id。 对于批量的id注册，有点类似 Mesa 里的 `MVCC`，对多个操作原子提交。

**分治：**

> To take advantage of the event_id independence, we partition the event id space handled by the IdRegistry into disjoint shards such that event ids from separate shards are managed by separate IdRegistry servers. 

sharding 需要 resharding 是比较麻烦的，需要考虑向后兼容性。对于落后的 log 来说它必须在 resharding 之后还能找到原来的 shard，原文称为 “deterministic mapping”。Photon 通过增加时间窗口实现，在 timestamp 在 [0, now + S] 内使用之前的sharding。这个时间会通过 True Time API 校正，这个 API 是 Google 特有的，通过原子时钟校正，不具有普适性。这里不得不感叹 Google 的基础架构之强大。

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/39260007.jpg" width="500px"/>

### 2. Exactly Once Eventually

这个语义和架构有关，所以我们先看下 Photon 的架构，了解里面的组件如何实现 Exactly-once，这里说明下印象比较深的是在 Paper 中， Photon 使用两个非常普遍的技术：`异步 PRC`, `throttling`，在提高效率和稳定性方面带来很大的帮助：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/public/16-11-26/70920655.jpg" width="500px"/>

**Dispatcher**

Dispatcher 负责监控日志文件，并将它们及时的传送到系统。需要注意读取文件的状态元数据会保存在GFS上，用于 failover。发送前需要查看 event_id 是否在 IdRegistry 中，这里的 event_id 需要保持唯一，Photon 使用 <ServerIP, ProcessID, Timestamp> 作为 event_id。

Dispatcher 异步的发送日志到 joiner，等待 joiner 的 ack。如果失败，则会将 click log 存入 GFS，稍后使用指数回退算法重试。一旦重试时间超过阈值，则判定日志为 unjoinable。通过这种方式达到 Dispatcher 的 at least once。

另外至少会有两条 Photon 流在运行，即使一个数据中心挂了，另外一个也可以正常工作。并且在恢复之后会执行 backlog，追上 IdRegistry 中已有的信息，需要注意的是追赶时候的 throttling，防止 IdRegistry 压力过大。

**joiner**

joiner 接收 dispatcher 传来的数据，并且协调 EventStore 和 IdRegistry，执行日志合并的业务逻辑。joiner 解析 click log 的 click_id 和 query_id，异步获取 EventStore 的 query log 进行比较，一旦获取失败或者 dispatcher 发送过多数据，则会导致失败使得dispatcher 进入重试逻辑。这里的 throttling 是用于 joiner 能维持平滑的数据流。

joiner 使用 adaptor 库处理业务逻辑。一旦 join 成功，则异步地注册 click_id 到 IdRegistry，成功注册后才允许下发 joined log 给下游。这两个步骤在 Photon 中并不是事务地进行，所以一旦在下发前宕机或者 ack 丢失就会导致 joined log 丢失。

之所以不进行事务处理，原因应该是想让 joiner `无状态`，提高可扩展性。导致的风险可以用下面的方法解决。

提交 click_id 给 IdRegistry 时，joiner 会附带自己的 <ServerIP, ProcessID, Timestamp> 信息作为 `unique token` 一并提交。当 IdRegistry 收到 click_id 的提交信息时，会比较 token，
相同则说明是上一次没有收到 ack 的那个 joiner 发过来的。IdRegistry 此时返回成功，允许 joiner 下发合并日志。

但此种方法并没有解决宕机带来的不一致。Photon 给的方案是使用另外一个 `verification system` 作为补充。一旦检测到 IdRegistry 中的 event 并不在 output log 中，则删除 IdRegistry 的 click_id，并重新注入 click log。

以上其实是通过内部机制实现尽力而为的 exactly-once，最后通过类似的 lambda 架构实现最终 exactly-once。








