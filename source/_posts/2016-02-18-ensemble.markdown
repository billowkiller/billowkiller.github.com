---
layout: post
title: "Ensemble Methods"
date: 2016-02-18 14:00
comments: true
category: "Machine Learning"
tags: ensemble,ml
---

Ensemble`|ɒnˈsɒmbl|` Methods 称为集成方法，它还有其他类似的名字，meta-algorithm、aggregation model，这些都代表这同一个意思，就是不同弱分类器的组合成一个强分类器。这里的弱分类器要比随机猜测的结果好，错误率小于50%；弱分类器可以是决策树、逻辑回归、朴素贝叶斯等算法。Ensemble的形式有很多种：

* 不同算法的集成;
* 同一算法在不同设置下的集成;
* 数据集不同部分分配给不同分类器之后的集成。

那么这多个弱分类器又是如何组合的呢，下面给出一个big picture，后面的文章也是对其的阐述。

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-18/38802537.jpg" width="450px"/>

<!--more-->

弱分类的组合可以是linear或者stacking的，linear又可以是uniform或者non-uniform.
stacking或者stacked generalization的大意是non-linear combining. 通常stacking的组合算法有LR，GBM, KNN, NN, RF 和 ET，可以参考[http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/). 作者Wolpert是这样形容的：

>stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn’t duplicated in what the other generalizer’s have to say), the better the resultant stacked generalization. 

那么为什么要把这些分类器进行组合呢？组合之后是否能获得更好的效果？可以看下下面的例子。

假设我们有10个samples的测试集，正确的结果是`1111111111`。现在有三个分类器，它们只有70%的正确率，那么三个分类器进行majority vote，可以得到如下的正确率：

$$0.7 * 0.7 * 0.7 + \binom{3}{2}0.7 * 0.7 * 0.3 = 0.784$$

也就是由原来的70%提升到了78%。正确率会随着分类器的增加而增加。5个分类器的正确率大约为83%。这个在统计学上就是“Wisdom of Crowds”。但是这个结果提高的前提在于sample的diversity，也就是减少sample之间的correlation。例如：

    1111111100 = 80% accuracy
    1111111100 = 80% accuracy
    1011111100 = 70% accuracy
    
在这个例子中得到`1111111100`还是只有80%的正确率，而

    1111111100 = 80% accuracy
    0111011101 = 70% accuracy
    1000101111 = 60% accuracy
    
经过Ensemle就可以得到`1111111101`，90%的正确率。


那么如何证明多个组合会比单个的结果好呢，可以用Uniform Linear的组合进行下面的理论描述。

$$ Let\ G(x) = \frac{1}{T}\sum_{t=1}^T g_t(x)$$

$$   avg((g_t(x) - f(x))^2) = avg(g_t^2 - 2g_tf + f^2)$$

$$   =avg(g_t^2) - G^2 + (G-f)^2$$

$$   =avg((g_t-G)^2) + (G-f)^2$$

$$ avg(E(g_t)) = avg((g_t-G)^2) + E(G) \ge E(G) $$

更一般的，有如下的预测模型$\hat{F}(x)^T = [\hat{f}\_1(x), \hat{f}\_2(x)...\hat{f}\_M(x)]$, 用最小二乘法寻找线性最小值

$$ \hat{w} = argmin_w E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 $$

$$ \hat{w} = E[\hat{F}(x)\hat{F}(x)^T]^{-1}E[\hat{F}(x)Y] $$

$$ E[Y - \sum_{m=1}^M w_m\hat{f}_m(x)]^2 \le E[Y-\hat{f}_m(x)]^2 \forall m$$
   

## Bagging

Bagging或者bootstrap aggregation是上文提到的Uniform Linear aggregation。其中用到bootstrapping，这是是一种resample的方法，定义如下

> re-sample N examples from original sample **uniformly with replacement** -- can also use arbitrary N' instead of original N

有training set $Z$, 对每个bootstrap sample $Z^{*b}, b=1,2...B$，bagging定义如下：

$$ \hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x) $$

## AdaBoost

AdaBoost或者Adaptive Boosting中只要弱分类器的正确率优于随机选择，那么通过AdaBoost就会得到非常好的结果。它是一种Boosting方法，所谓Boosting就是改变训练数据的概率分布（权值分布）针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。对应于上面的non-uniform linear model。

AdaBoost强调对错误分类的反复学习，但是最后对错误率较高的分类器赋予低权重。每轮学习中都会重新对分类器赋予不同的权重，这是为了得到更多关于数据的不同假设。如何得到更多的不同假设呢？

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-19/91708853.jpg" width="300px"/>

那么我们希望在$t+1$迭代学习的时候，$t$轮的结果尽可能的随机，即有错误率

$$ \epsilon_t = \frac{\sum_{n=1}^N u_n^{(t+1)}I(y_n \ne g_t(x_n))}{\sum_{n=1}^N u_n^{(t+1)}} = \frac{1}{2} $$

于是可以 multiply incorrect $\propto (1 - \epsilon_t)$; multiply correct $\propto \epsilon_t$

定义scalling factor $\blacklozenge_t = \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}}$

$$ incorrect \gets incorrect \cdot \blacklozenge_t \\ correct \gets correct \div \blacklozenge_t $$

最后对scalling factor取自然对数作为权值 $\alpha_t$ 将弱分类器线性组合在一块，取自然对数的逻辑如下：

$$ \epsilon_t = 1/2 => \blacklozenge_t = 1 => \alpha_t = 0 \\
\epsilon_t = 0 => \blacklozenge_t = \infty => \alpha_t = \infty $$

最后伪代码为

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/40495628.jpg" width="450px"/>

比较常见的弱分类器是Decision Stump，和AdaBoost组成`AdaBoost-Stump`，具有efficient feature selection and efficiency的特点。

## Random Forest

Bagging是通过平均带有噪声但是近似无偏的模型来减少variance，而对于足够深的Decision Tree来说，它的bias可以非常少，但是variance非常高。所以自然的想将二者结合，综合他们的优点。Bagged Tree并不能减少bias，但是可以有效的减少variance。这个Boosting正好相反，Boosting通过自适应的变化树的样子来减少bias。Random Forest就是Bagging + Decision Tree(C&RT)。EST给出RF的本质：

>The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much.

我们了解到增加hypothesis diversity可以提高最终结果的表现，那么Random Forest就将这种Random性发挥到极致，得到多样的hypothesis。为了增加随机性，可以做了以下的措施：

* re-sample new feature subspace for each b(x) in C&RT, 记得bagging进行data randomness for diversity, 那么在RF中是feature randomness for diversity
* random low-dimensional projections for each b(x) in C&RT, 这就是对feature进行投影，进行feature combination，在特征空间中随机选择若干特征组投影到若干个方向上。

伪代码如下（只用了第一个Randomness）：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/2671550.jpg" width = "500px"/>

RF还有的特点是训练的时候可以自带**Model Selection和Feature Selection**。那么RF是如何做到的？

### Model Selection
在RF中使用bagging时，每个bootstrap sample都会有一定的概率没有选择原来sample中的一些数据，这些数据就是out-of-bag (OOB) Samples. 在一个RF中，某个Decision Tree训练没有用到数据 $(x_n, y_n)$的概率为：$(1 - \frac{1}{N}) ^ N$，当N无限大的时候，接近 $\frac{1}{e}$.

可以用OOB来validate G, $E\_{oob}G = \frac{1}{N} \sum\_{n=1}^N err(y\_n, G\_n^-(x\_n))$, $G_n^-$ 表示 $x_n$在OOB中的Decision Tree。这样可以用 $E\_{oob}$ 对bagging/RF进行self-validation。

这有什么用呢，当然是进行模型选择了，可以使用 $E\_{oob}$ 进行RF的参数选择，例如feature subspace。下图表示使用Validation中进行的模型选择，可以看到RF中少了re-training的步骤。

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/30059076.jpg" width="250px"/>

### Feature Selection

在模型训练的时候通常希望能够去掉多余的、无关的特征，这样能够得到的好处有：

* **efficiency**: simpler hypothesis and shorter prediction time
* **generalization**: feature noise removed
* **interpretability**

通常可以通过特征的重要性来进行特征的选择，对于线性模型来说就是 $w_i$ 的绝对值，也就是特征对于最终结果的影响程度。那么在非线性的RF模型中呢？

所用的方法就是random test，例如特征 $i$ 被选择，那么在特征 $i$ 的数据集中加入随机变量重新训练则会降低模型正确率，而对于不重要的特征，怎么改变数据集当然对模型没有什么影响。

RF中使用的random test就是一种常用的统计学工具permutation test，也就是将特征 $i$ 的数据做重新排列。数据表达也就是：

$$ importance(i) = performance(\mathcal{D}) - performance(\mathcal{D}^{(p)}) $$

$$ \mathcal{D}^{(p)}\ is\ \mathcal{D}\ with\ \{x_{n,i}\}\ replaced\ by\ permuted\ \{x_{n,i}\}_{n=1}^N $$

$performance(\mathcal{D}^{(p)})$需要重新训练和评估，那么有什么办法可以避免呢？我们可以重新定义 $importance(i) = E\_{oob}(G^-) - E\_{oob}^{(p)}(G^-)$，表达式后项就是一个permuted OOB value。

具体过程如下，当 $b$ 个树生成的时候，记录OOB评估的 $G^-$ 的正确率，然后OOB sample中的特征 $i$ 数据重新随机排列后再次评估的 $G^-$ 的正确率，二者相减得到特征 $i$的重要性。

RF的缺点是，如果随机过程表现的不稳定，则需要很多的Decision Tree来支持。所以需要重新检查 $G$ 的稳定性来确保有足够多的树。

## Gradient Boosted Decision Tree

回忆下假设AdaBoost的分类器输出是binary的，则权值迭代可以转化为

$$ u_n^{t+1} = \begin{cases}{u_n^t \cdot \blacklozenge_t\ if\ incorrect}\\{u_n^t \div \blacklozenge_t\ if\ correct}\end{cases} = u_n^t \cdot \blacklozenge_t^{-y_ng_t(x_n)} = u_n^t \cdot exp(-y_n\alpha_tg_t(x_n))$$

$$ u_n^{(T+1)} = u_n^{(1)} \cdot \prod_{t=1}^Texp(-y_n\alpha_tg_t(x_n)) = \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) $$

在AdaBoost中 $G(x) = sign(\sum_{t=1}^T\alpha_tg_t(x_n))$ 括号中的表达式也被成为voting score。现在我们想要 $y_n(voting\ score)$ 为正且越大越好，也就是让 $u_n^{(T+1)}$ 越小越好。

所以AdaBoost的过程也就是让 $\sum_{n=1}^N u_n^{(t)}$ 减小，也就是最小化

$$ \sum_{n=1}^Nu_n^{(T+1)} =  \frac{1}{N} \cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n)) $$

注意到在gradient descent中第 $t$ 次迭代有：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/31496263.jpg" width = "400px"/>

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/14307315.jpg" width = "400px"/>

所以找到一个好的 $h(function\ direction)$ 函数也就是最小化 $\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$。对于二元分类，$y_n, h(x_n) \in \{-1, +1\}$，有

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/93309458.jpg" width="400px"/>

也就是每次迭代都需要最小化其中的hypothesis $E\_{in}^{u^{(t)}}(h)$，那么谁最小化 $E\_{in}^{u^{(t)}}(h)$呢，当然是AdaBoost中的reweighted sample所对应的 $g\_t$ 了。

所以现在需要优化 $\eta\_t$ 得到梯度下降方向最佳步长，原来的 $\hat{E}\_{ADA}$ 变为 $(\sum\_{n=1}^N u\_n^{(t)}) \cdot ((1-\epsilon\_t)exp(-\eta) + \epsilon\_t exp(+\eta))$。微分后容易得到 $\eta\_t=ln\sqrt{\frac{1-\epsilon\_t}{\epsilon\_t}} = \alpha\_t$。这和我们上面得到的scaling Factor是一致的。

所以AdaBoost是steepest descent with approximate functional gradient. 我们总结下，AdaBoost的数学表达式：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/16682553.jpg" width="400px"/>

Gradient Boost就是把二元分类的假设推广到任意的假设并且损失函数也可以任意的。

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/85486720.jpg" width="400px"/>

当选择平方损失函数时，有如下的可以看到

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/61222916.jpg" width="460px"/>

现在需要对 $h$ 加一些限制，否则 $h(x_n) = -\infty \cdot (s_n - y_n)$

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/34172769.jpg" width="400px"/>

最优 $g_t = h$ 就是 $(x_n, y_n-s_n)$ 上的最小二乘回归函数，$y_n - s_n$就是残差。于是原来的Gradient Boost表达式变成：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/37566404.jpg" width = "400px"/>

最小化 $\eta$ 就是 $(g\_t\ transformed\ input,\ residual)$ 上的单变量线性回归。所以GradientBoost for regression 的 $\alpha_t = optimal\ \eta\ by\ g_t\ transformed\ linear\ regression$.

Gradient Boosted Decision Tree也就是使用回归算法为Decision Tree。伪代码如下：

<img src="http://7xqfqs.com1.z0.glb.clouddn.com/16-2-20/38067174.jpg" width="500px"/>

总结下Ensemble Methods的有点：

* cure underfitting, 通过feature transform加强$G(x)$的Bias。
* cure overfitting, 通过多样化假设的合并达到regularization的目的，减少$G(x)$的variance。

